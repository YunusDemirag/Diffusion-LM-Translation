{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Diffusion-Lm with normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "COVOST_PATH = \"./covost/dataset\"\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "#nlp_en.initialize()\n",
    "lemmatizer_en = nlp_en.get_pipe(\"lemmatizer\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "#nlp_de.initialize()\n",
    "lemmatizer_de = nlp_de.get_pipe(\"lemmatizer\")\n",
    "\n",
    "def replace_german_special_chars(string: str) -> str:\n",
    "\n",
    "    replacements = {\n",
    "        'ä':\"ae\",\n",
    "        'ü':\"ue\",\n",
    "        'ö':\"oe\",\n",
    "        'ß':\"ss\"\n",
    "    }\n",
    "\n",
    "    for char, replacement in replacements.items():\n",
    "        string = string.replace(char, replacement)\n",
    "        \n",
    "    return string\n",
    "\n",
    "# function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # define the pattern to keep\n",
    "    regex = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(regex, '', text)\n",
    "\n",
    "# function to remove accented characters\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "for split in ['dev', 'test', 'train']:\n",
    "    covost_tsv = f'{COVOST_PATH}/covost_v2.de_en.{split}.tsv'\n",
    "    training_data_txt = f'{COVOST_PATH}/covost_v2.de_en_normalized.{split}.txt'\n",
    "    normalized_original_de = f'{COVOST_PATH}/covost_v2.normalized_original_de.{split}.txt'\n",
    "    normalized_original_en = f'{COVOST_PATH}/covost_v2.normalized_original_en.{split}.txt'\n",
    "    with open(covost_tsv, 'r') as tsv, open(training_data_txt, 'w') as txt, open(normalized_original_de, 'w') as txt_de, open(normalized_original_en, 'w') as txt_en:\n",
    "        for line in tsv.readlines():\n",
    "            _path, sentence, translation, _client_id, *_ = line.split('\\t')\n",
    "            sentence_tokenized = nlp_de(sentence)\n",
    "            sentence_lemmas = [token.lemma_ for token in sentence_tokenized]\n",
    "            sentence_lemmatized = ' '.join(sentence_lemmas)\n",
    "            sentence_lemmatized_decapitalized = sentence_lemmatized.lower()\n",
    "            sentence_lemmatized_decapitalized_replaced_special_chars = replace_german_special_chars(sentence_lemmatized_decapitalized)\n",
    "            sentence_lemmatized_decapitalized_replaced_special_chars_no_accents = remove_accented_chars(sentence_lemmatized_decapitalized_replaced_special_chars)\n",
    "            sentence_normalized = remove_special_characters(sentence_lemmatized_decapitalized_replaced_special_chars_no_accents)\n",
    "            translation_tokenized = nlp_en(translation)\n",
    "            translation_lemmas = [token.lemma_ for token in translation_tokenized]\n",
    "            translation_lemmatized = ' '.join(translation_lemmas)\n",
    "            translation_lemmatized_decapitalized = translation_lemmatized.lower()\n",
    "            translation_lemmatized_decapitalized_no_accents = remove_accented_chars(translation_lemmatized_decapitalized)\n",
    "            translation_normalized = remove_special_characters(translation_lemmatized_decapitalized_no_accents)\n",
    "            txt.write(f'{sentence_normalized} [SEP] {translation_normalized}\\n')\n",
    "            txt_de.write(f'{sentence_normalized} [SEP] {sentence}\\n')\n",
    "            txt_en.write(f'{translation_normalized} [SEP] {translation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "COVOST_PATH = \"./covost/dataset\"\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=VOCAB_SIZE)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "files = [f'{COVOST_PATH}/covost_v2.de_en_normalized.{split}.txt' for split in [\"test\", \"train\", \"dev\"]]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "tokenizer.save(f\"{COVOST_PATH}/tokenizer_{VOCAB_SIZE}_normalized.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a surface realization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from tokenizers import Tokenizer, Encoding\n",
    "from datasets import Dataset\n",
    "\n",
    "COVOST_PATH = \"./covost/dataset\"\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert2bert : EncoderDecoderModel = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
    "\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "bert2bert.forward()\n",
    "\n",
    "def get_data_loader(split):\n",
    "\n",
    "    data = {\n",
    "        'input_ids':[],\n",
    "        'labels':[]\n",
    "    }\n",
    "\n",
    "    reader = open(f'{COVOST_PATH}/covost_v2.normalized_original_en.{split}.txt', 'r')\n",
    "    for line in reader:\n",
    "        normalized_sentence, sentence = line.split(\"[SEP]\")\n",
    "        tokenized_normalized_sentence : Encoding = tokenizer.encode(normalized_sentence, return_tensors=\"pt\")\n",
    "        tokenized_sentence : Encoding = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        data['input_ids'].append(tokenized_normalized_sentence.ids)\n",
    "        data['labels'].append(tokenized_sentence.ids)\n",
    "    reader.close()\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_data_loader('train')\n",
    "eval_dataset = get_data_loader('dev')\n",
    "\n",
    "training_arguments = Seq2SeqTrainingArguments(output_dir=COVOST_PATH, evaluation_strategy='epoch')\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Diffusion-LM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e\n",
      "{'seed': 102, 'data_dir': '', 'schedule_sampler': 'uniform', 'lr': 0.0001, 'weight_decay': 0.0, 'lr_anneal_steps': 50000, 'batch_size': 64, 'microbatch': -1, 'ema_rate': '0.9999', 'log_interval': 50, 'save_interval': 50000, 'resume_checkpoint': '', 'use_fp16': False, 'fp16_scale_growth': 0.001, 'gradient_clipping': -1.0, 'eval_interval': 2000, 'checkpoint_path': './improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e', 'image_size': 8, 'num_channels': 128, 'num_res_blocks': 2, 'num_heads': 4, 'num_heads_upsample': -1, 'attention_resolutions': '16,8', 'dropout': 0.1, 'learn_sigma': False, 'sigma_small': False, 'class_cond': False, 'diffusion_steps': 2000, 'noise_schedule': 'sqrt', 'timestep_respacing': '', 'use_kl': False, 'predict_xstart': True, 'rescale_timesteps': True, 'rescale_learned_sigmas': True, 'use_checkpoint': False, 'use_scale_shift_norm': True, 'model_arch': 'transformer', 'in_channel': 256, 'out_channel': 256, 'training_mode': 'e2e', 'vocab_size': 30000, 'config_name': 'bert-base-uncased', 'experiment_mode': 'lm', 'logits_mode': 1, 'modality': 'text', 'dataset_name': 'covost', 'experiment': 'translation', 'loss_type': 'Lsimple', 'bsz': 64, 'diff_steps': 4000, 'emb_scale_factor': 1.0, 'noise_level': 0.0, 'cache_mode': 'no', 'use_bert_tokenizer': 'no', 'padding_mode': 'pad', 'preprocessing_num_workers': 1, 'submit': 'no', 'notes': 'xstart_e2e'}\n",
      "creating model and diffusion...\n",
      "creating model, based on transformer\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "LossType.E2E_MSE False\n",
      "training mode is  e2e\n",
      "training mode is  e2e\n",
      "the parameter count is 95194672\n",
      "saving the hyperparameters to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e/training_args.json\n",
      "creating data loader...\n",
      "load data **************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function create_data_loaders.<locals>.pad_function at 0x7fa5bc47f3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished filtering the dataset lines: 4 out of 13509 were too long. (0.029609889703160856\\%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f17fcc6b214b7780d5eee1768c856b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='padding', max=14.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished filtering the dataset lines: 17 out of 127637 were too long. (0.013319021913708406\\%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3287af0d16b64e1c937130f00ea884ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='padding', max=128.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished filtering the dataset lines: 2 out of 13509 were too long. (0.014804944851580428\\%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bf21f9096d467ca9f777ad28a2fbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='padding', max=14.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding model: Embedding(30000, 256)\n",
      " Requires Grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myunus-demirag\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ydemirag/studium/Diffusion-LM/wandb/run-20221212_135258-2xlb5o98</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yunus-demirag/diffusion_lm/runs/2xlb5o98\" target=\"_blank\">./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e</a></strong> to <a href=\"https://wandb.ai/yunus-demirag/diffusion_lm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "8\n",
      "------------------------\n",
      "| grad_norm | 0.969    |\n",
      "| loss      | 0.959    |\n",
      "| loss_q0   | 0.952    |\n",
      "| loss_q1   | 0.943    |\n",
      "| loss_q2   | 0.962    |\n",
      "| loss_q3   | 0.981    |\n",
      "| mse       | 0.959    |\n",
      "| mse_q0    | 0.952    |\n",
      "| mse_q1    | 0.943    |\n",
      "| mse_q2    | 0.962    |\n",
      "| mse_q3    | 0.981    |\n",
      "| samples   | 64       |\n",
      "| step      | 0        |\n",
      "------------------------\n",
      "8\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.758    |\n",
      "| eval_loss_q0 | 0.778    |\n",
      "| eval_loss_q1 | 0.755    |\n",
      "| eval_loss_q2 | 0.721    |\n",
      "| eval_loss_q3 | 0.79     |\n",
      "| eval_mse     | 0.758    |\n",
      "| eval_mse_q0  | 0.778    |\n",
      "| eval_mse_q1  | 0.755    |\n",
      "| eval_mse_q2  | 0.721    |\n",
      "| eval_mse_q3  | 0.79     |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e/model000000.pt\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e/model000000.pt\n",
      "saving model 0.9999...\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e/ema_0.9999_000000.pt\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_hhidden_size_s2_d0.1_sd102_xstart_e2e/ema_0.9999_000000.pt\n",
      "------------------------\n",
      "| grad_norm | 0.299    |\n",
      "| loss      | 0.509    |\n",
      "| loss_q0   | 0.499    |\n",
      "| loss_q1   | 0.507    |\n",
      "| loss_q2   | 0.508    |\n",
      "| loss_q3   | 0.522    |\n",
      "| mse       | 0.509    |\n",
      "| mse_q0    | 0.499    |\n",
      "| mse_q1    | 0.507    |\n",
      "| mse_q2    | 0.508    |\n",
      "| mse_q3    | 0.522    |\n",
      "| samples   | 3.26e+03 |\n",
      "| step      | 50       |\n",
      "------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6338/4014491994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mschedule_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschedule_sampler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0;34m**\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m ).run_loop()\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/train_util.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         ):\n\u001b[1;32m    181\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumpkvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/train_util.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, batch, cond)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_fp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/train_util.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, batch, conditions)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlast_batch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_ddp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/respace.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     ):  # pylint: disable=signature-differs\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# print('called training_losses')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \"\"\"\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'e2e'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses_e2e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'e2e-simple'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses_e2e_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses_e2e\u001b[0;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[1;32m   1562\u001b[0m             target = {\n\u001b[1;32m   1563\u001b[0m                 ModelMeanType.PREVIOUS_X: self.q_posterior_mean_variance(\n\u001b[0;32m-> 1564\u001b[0;31m                     \u001b[0mx_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                 )[0],\n\u001b[1;32m   1566\u001b[0m                 \u001b[0mModelMeanType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mq_posterior_mean_variance\u001b[0;34m(self, x_start, x_t, t)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mx_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         posterior_mean = (\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0m_extract_into_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior_mean_coef1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0m_extract_into_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior_mean_coef2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         )\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36m_extract_into_tensor\u001b[0;34m(arr, timesteps, broadcast_shape)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mK\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \"\"\"\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from improved_diffusion import dist_util, logger\n",
    "from improved_diffusion.image_datasets import load_data\n",
    "from improved_diffusion.text_datasets import load_data_text\n",
    "from improved_diffusion.resample import create_named_schedule_sampler\n",
    "from improved_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    args_to_dict,\n",
    "    add_dict_to_argparser,\n",
    ")\n",
    "from improved_diffusion.train_util import TrainLoop\n",
    "from transformers import set_seed\n",
    "from functools import partial\n",
    "from improved_diffusion.test_util import get_weights, compute_logp\n",
    "from improved_diffusion.rounding import load_models, load_tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "import torch.distributed as dist\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class Parameters(dict):\n",
    "    '''Most of the code in the Diffusion-LM Paper just passes the args through all the functions\n",
    "    I wanted to use dicts for the params, but only noticed then that they don't support getting their\n",
    "    values like attributes.'''\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def __getattribute__(self, __name: 'str'):\n",
    "        if hasattr(super(), __name):\n",
    "            return super().__getattribute__(__name) \n",
    "        elif super().get(__name):\n",
    "            return super().get(__name)\n",
    "        else:\n",
    "            raise AttributeError(f\"'parameters' object has no attribute '{__name}'\")\n",
    "\n",
    "\n",
    "COVOST_PATH = \"./covost/dataset\"\n",
    "DIFFUSION_MODELS_PATH = \"./improved-diffusion/diffusion_models/\"\n",
    "TEXT_DEFAULTS: 'dict[str, str | int | float]' = dict(\n",
    "    modality='text',\n",
    "    dataset_name='covost',\n",
    "    experiment='translation',\n",
    "    noise_schedule='cosine',\n",
    "    loss_type='Lsimple',\n",
    "    dropout=0.1,\n",
    "    weight_decay=0.0,\n",
    "    image_size=8,\n",
    "    #hidden_size=128,\n",
    "    in_channel=16, ## Embedding Dimension\n",
    "    lr_anneal_steps=400000, ## Training steps\n",
    "    num_res_blocks=2, ## Not sure\n",
    "    lr=1e-04, ## Learning rate?\n",
    "    bsz=64, ## Batch Size\n",
    "    diff_steps=4000, ## Steps of diffusion\n",
    "    model_arch='conv-unet',\n",
    "    emb_scale_factor=1.0, \n",
    "    noise_level=0.0, \n",
    "    cache_mode='no', \n",
    "    use_bert_tokenizer='no',\n",
    "    padding_mode='block',\n",
    "    preprocessing_num_workers=1,\n",
    "    #config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml',\n",
    "    #model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None',\n",
    "    #experiment='gpt2_pre_compress',\n",
    "\n",
    ")\n",
    "'''These are the defaults from Diffusion-LMs run_train.py'''\n",
    "\n",
    "DIFFUSION_DEFAULTS: \"dict[str, str | float]\" = {\n",
    "    'seed': 101,\n",
    "    'data_dir': \"\",\n",
    "    'schedule_sampler': \"uniform\",\n",
    "    'lr':1e-4,\n",
    "    'weight_decay':0.0,\n",
    "    'lr_anneal_steps':0,\n",
    "    'batch_size':1,\n",
    "    'microbatch':-1, # -1 disables microbatches\n",
    "    'ema_rate':\"0.9999\", # comma-seperated list of EMA values\n",
    "    'log_interval':50,\n",
    "    'save_interval':50000,\n",
    "    'resume_checkpoint':\"\",\n",
    "    'use_fp16':False,\n",
    "    'fp16_scale_growth':1e-3,\n",
    "    'gradient_clipping':-1.0,\n",
    "    'eval_interval':2000,\n",
    "    'checkpoint_path':\"diff_models\"\n",
    "}\n",
    "'''These are the defaults from improved-diffusions train.py'''\n",
    "\n",
    "MODEL_AND_DIFFUSION_DEFAULTS = model_and_diffusion_defaults()\n",
    "ARGS: 'dict[str, str | int | float]' = {\n",
    "    'diffusion_steps': 2000,\n",
    "    'model_arch': 'transformer',\n",
    "    'lr': 0.0001,\n",
    "    'lr_anneal_steps': 50000,\n",
    "    'seed': 102,\n",
    "    'noise_schedule': 'sqrt',\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256, # Same as in_channel\n",
    "    'modality': 'text',\n",
    "    'experiment': 'translation',\n",
    "    'submit': 'no',\n",
    "    'padding_mode': 'pad',\n",
    "    'predict_xstart': True,\n",
    "    'training_mode': 'e2e',\n",
    "    'notes': 'xstart_e2e',\n",
    "    'batch_size': 64,\n",
    "    'vocab_size': 30000\n",
    "}\n",
    "\n",
    "'''Adjust these for your run.'''\n",
    "\n",
    "PARAMS = Parameters(**{\n",
    "    **DIFFUSION_DEFAULTS,\n",
    "    **MODEL_AND_DIFFUSION_DEFAULTS,\n",
    "    **TEXT_DEFAULTS,\n",
    "    **ARGS\n",
    "})\n",
    "'''This collects all the parameters, as in the Diffusion-Lm repo'''\n",
    "\n",
    "### From run_train.py\n",
    "if PARAMS['loss_type'] == 'Lsimple':\n",
    "    PARAMS.update(use_kl= False, learn_sigma= False)\n",
    "elif PARAMS['loss_type'] == 'Lhybrid':\n",
    "    PARAMS.update(use_kl= False, learn_sigma= True)\n",
    "elif PARAMS['loss_type'] == 'Lvlb':\n",
    "    PARAMS.update(use_kl= True, learn_sigma= True)\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "def model_path(\n",
    "    modality,\n",
    "    padding_mode,\n",
    "    experiment,\n",
    "    in_channel,\n",
    "    model_arch,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    diff_steps,\n",
    "    noise_schedule,\n",
    "    loss_type,\n",
    "    #hidden_size,\n",
    "    num_res_blocks,\n",
    "    dropout,\n",
    "    seed,\n",
    "    notes=None,\n",
    "    **_\n",
    "    ):\n",
    "    MODEL_NAME = f\"diff\" \\\n",
    "        f\"_{modality}\" \\\n",
    "        f\"_{padding_mode}\" \\\n",
    "        f\"_{experiment}{in_channel}_normalized\" \\\n",
    "        f\"_{model_arch}\" \\\n",
    "        f\"_lr{lr}\" \\\n",
    "        f\"_{weight_decay}\" \\\n",
    "        f\"_{diff_steps}\" \\\n",
    "        f\"_{noise_schedule}\" \\\n",
    "        f\"_{loss_type}\" \\\n",
    "        f\"_h{'hidden_size'}\" \\\n",
    "        f\"_s{num_res_blocks}\" \\\n",
    "        f\"_d{dropout}\" \\\n",
    "        f\"_sd{seed}\" \\\n",
    "        f\"{f'_{notes}' if notes else ''}\"\n",
    "\n",
    "    return os.path.join(DIFFUSION_MODELS_PATH, MODEL_NAME)\n",
    "\n",
    "MODEL_PATH = model_path(**PARAMS)\n",
    "\n",
    "PARAMS['checkpoint_path'] = MODEL_PATH\n",
    "\n",
    "### Environment variables for the training script\n",
    "os.environ['OPENAI_LOGDIR']=MODEL_PATH\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "set_seed(PARAMS['seed']) \n",
    "dist_util.setup_dist() # DEBUG **\n",
    "logger.configure()\n",
    "\n",
    "print(PARAMS)\n",
    "\n",
    "logger.log(\"creating model and diffusion...\")\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "    **{\n",
    "        key: PARAMS[key] for key in MODEL_AND_DIFFUSION_DEFAULTS.keys()\n",
    "    }\n",
    ")\n",
    "model.to(dist_util.dev()) #  DEBUG **\n",
    "# model.cuda() #  DEBUG **\n",
    "\n",
    "pytorch_total_params = sum(parameter.numel() for parameter in model.parameters())\n",
    "logger.log(f'the parameter count is {pytorch_total_params}')\n",
    "\n",
    "schedule_sampler = create_named_schedule_sampler(PARAMS['schedule_sampler'], diffusion)\n",
    "\n",
    "logger.log(f'saving the hyperparameters to {PARAMS[\"checkpoint_path\"]}/training_args.json')\n",
    "with open(f'{PARAMS[\"checkpoint_path\"]}/training_args.json', 'w') as hyperparams_file:\n",
    "    json.dump(PARAMS, hyperparams_file, indent=2)\n",
    "\n",
    "logger.log(\"creating data loader...\")\n",
    "print('load data', '*'*50)\n",
    "\n",
    "tokenizer: 'Tokenizer' = Tokenizer.from_file(f'{COVOST_PATH}/tokenizer_{PARAMS.vocab_size}_normalized.json')\n",
    "\n",
    "embedding_model = torch.nn.Embedding(tokenizer.get_vocab_size(), PARAMS['in_channel'])\n",
    "\n",
    "torch.save(embedding_model.weight, f'{MODEL_PATH}/embedding_weights_initial.pt')\n",
    "\n",
    "def create_data_loaders():\n",
    "    '''Creating a function for this so that unnecessary data can be freed'''\n",
    "    from improved_diffusion.text_datasets import TextDataset_NoCache\n",
    "    from torch.utils.data import DataLoader\n",
    "    import itertools\n",
    "    from diffusion_translation.datasets import TextDataset_FileBacked\n",
    "\n",
    "    split_renamings = {\n",
    "        'train':'data',\n",
    "        'dev': 'eval_data'\n",
    "    }\n",
    "\n",
    "    # return {\n",
    "    #     split_renamings[split]: (\n",
    "    #         databatch for databatch in DataLoader(\n",
    "    #             TextDataset_FileBacked(\n",
    "    #                 tokenizer= tokenizer,\n",
    "    #                 file= f'{COVOST_PATH}/covost_v2.de_en.{split}.txt',\n",
    "    #                 embedding_model = embedding_model,\n",
    "    #                 **PARAMS\n",
    "    #             ),\n",
    "    #             batch_size=PARAMS['batch_size'],  # 64,\n",
    "    #             drop_last=True,\n",
    "    #             shuffle=False,\n",
    "    #             num_workers=1,\n",
    "    #         )\n",
    "    #     ) for split in ['train', 'dev']\n",
    "    # }\n",
    "\n",
    "    # As in Diffusion-LM\n",
    "    max_seq_len = PARAMS.image_size ** 2\n",
    "\n",
    "    from datasets import DatasetDict, Dataset\n",
    "    from improved_diffusion.text_datasets import _collate_batch_helper\n",
    "\n",
    "    data = DatasetDict()\n",
    "\n",
    "    for split in [\"test\", \"train\", \"dev\"]:\n",
    "        pad_token_id = int(tokenizer.token_to_id('[PAD]'))\n",
    "        reader = open(f'{COVOST_PATH}/covost_v2.de_en_normalized.{split}.txt')\n",
    "\n",
    "        ## Skip Header\n",
    "        next(reader)\n",
    "\n",
    "        encoded = (tokenizer.encode(line).ids for line in reader)\n",
    "\n",
    "        ## Some lines might be too long. Can't split them up for translation.\n",
    "        def filtered():\n",
    "            num_dataset_valid_lines = 0\n",
    "            num_dataset_filtered_out_lines = 0\n",
    "            for encoding in encoded:\n",
    "                if len(encoding) < max_seq_len:\n",
    "                    num_dataset_valid_lines+=1\n",
    "                    yield encoding\n",
    "                else:\n",
    "                    num_dataset_filtered_out_lines+=1\n",
    "            num_lines = num_dataset_filtered_out_lines + num_dataset_valid_lines\n",
    "            print(f'Finished filtering the dataset lines: {num_dataset_filtered_out_lines} out of {num_lines} were too long. ({num_dataset_filtered_out_lines/num_lines*100}\\%)')\n",
    "\n",
    "        encoded_dataset = Dataset.from_dict({\n",
    "            'input_ids':[ encoding  for encoding in filtered()]\n",
    "        })\n",
    "\n",
    "        def pad_function(group_lst):\n",
    "            group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], pad_token_id, max_seq_len)\n",
    "            return group_lst\n",
    "\n",
    "        padded_dataset = encoded_dataset.map(\n",
    "            pad_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            desc=f'padding',\n",
    "        )\n",
    "\n",
    "        data[split]=padded_dataset\n",
    "\n",
    "        reader.close()\n",
    "\n",
    "    def data_generator(split, data):\n",
    "        dataset = TextDataset_NoCache(\n",
    "            data,\n",
    "            PARAMS.image_size,\n",
    "            PARAMS,\n",
    "            model_arch=PARAMS['model_arch'],\n",
    "            model_emb=embedding_model.cpu(),\n",
    "            split=split\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=PARAMS['batch_size'],  # 64,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            yield from dataloader\n",
    "\n",
    "    return {\n",
    "        # split_renamings[split]: (\n",
    "        #     databatch for databatch in DataLoader(\n",
    "        #         TextDataset_NoCache(\n",
    "        #             data,\n",
    "        #             PARAMS.image_size,\n",
    "        #             PARAMS,\n",
    "        #             model_arch=PARAMS['model_arch'],\n",
    "        #             model_emb=embedding_model,\n",
    "        #             split=split\n",
    "        #         ),\n",
    "        #         batch_size=PARAMS['batch_size'],  # 20,\n",
    "        #         drop_last=True,\n",
    "        #         shuffle=False,\n",
    "        #         num_workers=1,\n",
    "        #     )\n",
    "        #) for split in ['train', 'dev']\n",
    "        split_renamings[split]: data_generator(split, data) for split in ['train', 'dev']\n",
    "    }\n",
    "\n",
    "data_loaders = create_data_loaders()\n",
    "\n",
    "embedding_model_cuda = embedding_model.cuda()\n",
    "\n",
    "def set_mapping_func(args, diffusion):\n",
    "    print(f'Embedding model: {embedding_model}\\n Requires Grad: {embedding_model.weight.requires_grad}')\n",
    "    mapping_func = partial(compute_logp, args, embedding_model_cuda)\n",
    "    diffusion.mapping_func = mapping_func\n",
    "\n",
    "set_mapping_func(PARAMS, diffusion)\n",
    "\n",
    "def training_params(\n",
    "    batch_size, \n",
    "    microbatch, \n",
    "    lr, \n",
    "    ema_rate,\n",
    "    log_interval,\n",
    "    save_interval,\n",
    "    resume_checkpoint,\n",
    "    use_fp16,\n",
    "    fp16_scale_growth,\n",
    "    weight_decay,\n",
    "    lr_anneal_steps,\n",
    "    checkpoint_path,\n",
    "    gradient_clipping,\n",
    "    eval_interval, **_):\n",
    "    '''Extracts just the training parameters'''\n",
    "    return dict(\n",
    "        batch_size=batch_size, \n",
    "        microbatch=microbatch, \n",
    "        lr=lr, \n",
    "        ema_rate=ema_rate,\n",
    "        log_interval=log_interval,\n",
    "        save_interval=save_interval,\n",
    "        resume_checkpoint=resume_checkpoint,\n",
    "        use_fp16=use_fp16,\n",
    "        fp16_scale_growth=fp16_scale_growth,\n",
    "        weight_decay=weight_decay,\n",
    "        lr_anneal_steps=lr_anneal_steps,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        eval_interval=eval_interval\n",
    "    )\n",
    "\n",
    "wandb.init(\n",
    "    project=os.getenv(\"WANDB_PROJECT\", \"diffusion_lm\"),\n",
    "    name=PARAMS['checkpoint_path'],\n",
    ")\n",
    "wandb.config.update(PARAMS, allow_val_change=True)\n",
    "\n",
    "logger.log(\"training...\")\n",
    "TrainLoop(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    schedule_sampler=schedule_sampler,\n",
    "    **training_params(**PARAMS),\n",
    "    **data_loaders\n",
    ").run_loop()\n",
    "\n",
    "## Saving the Embedding Model\n",
    "torch.save(embedding_model.weight, f'{MODEL_PATH}/embedding_weights.pt')\n",
    "torch.save(embedding_model_cuda.weight, f'{MODEL_PATH}/embedding_weights_cuda.pt')\n",
    "\n",
    "\n",
    "# for data_loader in data_loaders:\n",
    "#     data_loaders[data_loader].close()\n",
    "\n",
    "wandb.finish(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_loss_q0</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_loss_q1</td><td>█▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_loss_q2</td><td>█▂▂▁▁▁▂▁▁▁▁</td></tr><tr><td>eval_loss_q3</td><td>█▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval_mse</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_mse_q0</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_mse_q1</td><td>█▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_mse_q2</td><td>█▂▂▁▁▁▂▁▁▁▁</td></tr><tr><td>eval_mse_q3</td><td>█▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>grad_norm</td><td>▂█▅▄▄▃▃▃▂▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▁▂▂▂▂▂▂▂▁▁▂▁▂</td></tr><tr><td>loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q0</td><td>█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q1</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q2</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q3</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q0</td><td>█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q1</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q2</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q3</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>samples</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>0.10886</td></tr><tr><td>eval_loss_q0</td><td>0.04198</td></tr><tr><td>eval_loss_q1</td><td>0.06238</td></tr><tr><td>eval_loss_q2</td><td>0.07157</td></tr><tr><td>eval_loss_q3</td><td>0.19137</td></tr><tr><td>eval_mse</td><td>0.10886</td></tr><tr><td>eval_mse_q0</td><td>0.04198</td></tr><tr><td>eval_mse_q1</td><td>0.06238</td></tr><tr><td>eval_mse_q2</td><td>0.07157</td></tr><tr><td>eval_mse_q3</td><td>0.19137</td></tr><tr><td>grad_norm</td><td>0.05217</td></tr><tr><td>loss</td><td>0.08851</td></tr><tr><td>loss_q0</td><td>0.04406</td></tr><tr><td>loss_q1</td><td>0.06013</td></tr><tr><td>loss_q2</td><td>0.08181</td></tr><tr><td>loss_q3</td><td>0.16809</td></tr><tr><td>mse</td><td>0.08851</td></tr><tr><td>mse_q0</td><td>0.04406</td></tr><tr><td>mse_q1</td><td>0.06013</td></tr><tr><td>mse_q2</td><td>0.08181</td></tr><tr><td>mse_q3</td><td>0.16809</td></tr><tr><td>samples</td><td>1372864</td></tr><tr><td>step</td><td>21450</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">./improved-diffusion/diffusion_models/diff_text_pad_translation256_normalized_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e</strong>: <a href=\"https://wandb.ai/yunus-demirag/diffusion_lm/runs/3q5qk57u\" target=\"_blank\">https://wandb.ai/yunus-demirag/diffusion_lm/runs/3q5qk57u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221209_135936-3q5qk57u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('Diffusion-LM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 18 2022, 18:57:03) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb02dc56016c5c7b1e2062c7b40f50aeaad4c023c9f15df42684d8182a57eebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
