{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset formatting\n",
    "\n",
    "I'll bring the dataset into a line-by-line format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"./datasets/wit3_de_en/train.tags.de-en\"):\n",
    "  os.remove(\"./datasets/wit3_de_en/train.tags.de-en\")\n",
    "\n",
    "with open(\"./datasets/wit3_de_en/de-en/train.tags.de-en.de\", 'r') as reader:\n",
    "    xml_train_de = BeautifulSoup(reader.read())\n",
    "\n",
    "transcripts_de = xml_train_de.find_all(\"transcript\")\n",
    "\n",
    "with open(\"./datasets/wit3_de_en/de-en/train.tags.de-en.en\", 'r') as reader:\n",
    "    xml_train_en = BeautifulSoup(reader.read())\n",
    "\n",
    "transcripts_en = xml_train_en.find_all(\"transcript\")\n",
    "\n",
    "transcripts_de_en = zip(transcripts_de, transcripts_en)\n",
    "\n",
    "formatted_train_text = []\n",
    "\n",
    "for (transcript_de, transcript_en) in transcripts_de_en:\n",
    "    transcript_de_lines = transcript_de.text.splitlines()\n",
    "    transcript_en_lines = transcript_en.text.splitlines()\n",
    "    formatted_transcript = map(lambda line_de, line_en: line_de + ' ' + line_en + '\\n', transcript_de_lines, transcript_en_lines)\n",
    "    formatted_train_text.extend(formatted_transcript)\n",
    "\n",
    "with open(\"./datasets/wit3_de_en/train.tags.de-en\", \"x\") as writer:\n",
    "    writer.writelines(formatted_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./datasets/wit3_de_en/valid.tags.de-en\"):\n",
    "  os.remove(\"./datasets/wit3_de_en/valid.tags.de-en\")\n",
    "\n",
    "with open(\"./datasets/wit3_de_en/de-en/valid.tags.de-en.de\", 'r') as reader:\n",
    "    xml_train_de = BeautifulSoup(reader.read())\n",
    "\n",
    "transcripts_de = xml_train_de.find_all(\"transcript\")\n",
    "\n",
    "with open(\"./datasets/wit3_de_en/de-en/valid.tags.de-en.en\", 'r') as reader:\n",
    "    xml_train_en = BeautifulSoup(reader.read())\n",
    "\n",
    "transcripts_en = xml_train_en.find_all(\"transcript\")\n",
    "\n",
    "transcripts_de_en = zip(transcripts_de, transcripts_en)\n",
    "\n",
    "formatted_train_text = []\n",
    "\n",
    "for (transcript_de, transcript_en) in transcripts_de_en:\n",
    "    transcript_de_lines = transcript_de.text.splitlines()\n",
    "    transcript_en_lines = transcript_en.text.splitlines()\n",
    "    formatted_transcript = map(lambda line_de, line_en: line_de + ' ' + line_en + '\\n', transcript_de_lines, transcript_en_lines)\n",
    "    formatted_train_text.extend(formatted_transcript)\n",
    "\n",
    "with open(\"./datasets/wit3_de_en/valid.tags.de-en\", \"x\") as writer:\n",
    "    writer.writelines(formatted_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python transformers/examples/pytorch/language-modeling/train_translation.py         --output_dir=classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_         --model_name_or_path=bert-base-uncased         --tokenizer_name=bert-base-uncased         --per_device_train_batch_size 10         --per_device_eval_batch_size 10         --save_steps 50000         --num_train_epochs 6         --do_train --eval_steps 10000 --evaluation_strategy steps         --do_eval --dataloader_num_workers 4         --save_total_limit 1         --overwrite_output_dir          --logging_dir classifier_models/runs/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_         --block_size 100          --disable_tqdm True --model_type gpt2         --padding_mode none         --gradient_accumulation_steps 1 --experiment roc --seed 101 --train_file=./datasets/wit3_de_en/train.tags.de-en.txt --validation_file ./datasets/wit3_de_en/valid.tags.de-en.txt  --task translation --init_emb ./improved-diffusion/diffusion_models/diff_roc_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e --n_embd 16 --learned_emb yes\n",
      "11/16/2022 12:57:50 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/16/2022 12:57:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=True,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=10000,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=classifier_models/runs/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=6.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=10,\n",
      "per_device_train_batch_size=10,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_,\n",
      "save_on_each_node=False,\n",
      "save_steps=50000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=1,\n",
      "seed=101,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "Lodaing from Wit3 Dataset\n",
      "[[' \\n'], ['Mein', 'Vortrag', 'heißt', '\"', 'Flatternde', 'Vögel', 'und', 'Weltraumteleskope', '\"', '.', 'My', 'talk', 'is', '\"', 'Flapping', 'Birds', 'and', 'Space', 'Telescopes', '.', '\"', '\\n']]\n",
      "[[' \\n'], ['Eine', 'Möglichkeit', ',', 'unsere', 'Gene', 'zu', 'verändern', ',', 'ist', ',', 'neue', 'zu', 'erschaffen', ',', 'wie', 'Craig', 'Venter', 'so', 'elegant', 'gezeigt', 'hat', '.', 'One', 'way', 'to', 'change', 'our', 'genes', 'is', 'to', 'make', 'new', 'ones', ',', 'as', 'Craig', 'Venter', 'has', 'so', 'elegantly', 'shown', '.', '\\n']]\n",
      "132350 132353\n",
      "132353 derived vocabs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:652] 2022-11-16 12:57:59,085 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/ydemirag/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:688] 2022-11-16 12:57:59,086 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[WARNING|modeling_bert.py:1145] 2022-11-16 12:57:59,095 >> If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "initializing the tokenizer with small vocab\n",
      "****************************************************************************************************\n",
      "loading from dataset-specific vocab\n",
      "132353\n",
      "\n",
      " Initializing the model from scratch \n",
      "****************************************************************************************************\n",
      "11/16/2022 12:58:01 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7faf1c85d5f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 65/65 [00:01<00:00, 50.36ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/67 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/16/2022 12:58:02 - INFO - datasets.arrow_writer - Done writing 64909 examples in 10989634 bytes .\n",
      "11/16/2022 12:58:02 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7faf1c85d5f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 67/67 [00:01<00:00, 47.11ba/s]\n",
      "padding:   0%|          | 0/65 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/16/2022 12:58:04 - INFO - datasets.arrow_writer - Done writing 66513 examples in 11335561 bytes .\n",
      "11/16/2022 12:58:04 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.pad_function at 0x7faf1c37c830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "padding: 100%|██████████| 65/65 [00:00<00:00, 73.56ba/s]\n",
      "padding:   0%|          | 0/67 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/16/2022 12:58:04 - INFO - datasets.arrow_writer - Done writing 64909 examples in 50369904 bytes .\n",
      "11/16/2022 12:58:04 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.pad_function at 0x7faf1c37c830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "padding: 100%|██████████| 67/67 [00:00<00:00, 70.70ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/16/2022 12:58:05 - INFO - datasets.arrow_writer - Done writing 66513 examples in 51614624 bytes .\n",
      "11/16/2022 12:58:06 - INFO - datasets.load - Checking /home/ydemirag/.cache/huggingface/datasets/downloads/b839e05c74a5dac7459de9abd13555710e1df86d301bd838bc0b3c5c009c5fb0.c90b7a47035aca98431814fb3c916f50b10dd20d672f7073136109a2e0b43b46.py for additional imports.\n",
      "11/16/2022 12:58:06 - INFO - datasets.utils.filelock - Lock 140390123430800 acquired on /home/ydemirag/.cache/huggingface/datasets/downloads/b839e05c74a5dac7459de9abd13555710e1df86d301bd838bc0b3c5c009c5fb0.c90b7a47035aca98431814fb3c916f50b10dd20d672f7073136109a2e0b43b46.py.lock\n",
      "11/16/2022 12:58:06 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/accuracy/accuracy.py at /home/ydemirag/.cache/huggingface/modules/datasets_modules/metrics/accuracy\n",
      "11/16/2022 12:58:06 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/accuracy/accuracy.py at /home/ydemirag/.cache/huggingface/modules/datasets_modules/metrics/accuracy/d60e08bd37e7c5a7bcc3620dd0d2788d25d233238ee0bdb3cfabde6c43d60574\n",
      "11/16/2022 12:58:06 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/accuracy/accuracy.py to /home/ydemirag/.cache/huggingface/modules/datasets_modules/metrics/accuracy/d60e08bd37e7c5a7bcc3620dd0d2788d25d233238ee0bdb3cfabde6c43d60574/accuracy.py\n",
      "11/16/2022 12:58:06 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/accuracy/dataset_infos.json\n",
      "11/16/2022 12:58:06 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/accuracy/accuracy.py at /home/ydemirag/.cache/huggingface/modules/datasets_modules/metrics/accuracy/d60e08bd37e7c5a7bcc3620dd0d2788d25d233238ee0bdb3cfabde6c43d60574/accuracy.json\n",
      "11/16/2022 12:58:06 - INFO - datasets.utils.filelock - Lock 140390123430800 released on /home/ydemirag/.cache/huggingface/datasets/downloads/b839e05c74a5dac7459de9abd13555710e1df86d301bd838bc0b3c5c009c5fb0.c90b7a47035aca98431814fb3c916f50b10dd20d672f7073136109a2e0b43b46.py.lock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydemirag/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1277] 2022-11-16 12:58:08,347 >> ***** Running training *****\n",
      "[INFO|trainer.py:1278] 2022-11-16 12:58:08,347 >>   Num examples = 64909\n",
      "[INFO|trainer.py:1279] 2022-11-16 12:58:08,347 >>   Num Epochs = 6\n",
      "[INFO|trainer.py:1280] 2022-11-16 12:58:08,347 >>   Instantaneous batch size per device = 10\n",
      "[INFO|trainer.py:1281] 2022-11-16 12:58:08,347 >>   Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "[INFO|trainer.py:1282] 2022-11-16 12:58:08,347 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1283] 2022-11-16 12:58:08,347 >>   Total optimization steps = 38946\n",
      "[INFO|integrations.py:576] 2022-11-16 12:58:08,348 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: Currently logged in as: yunus-demirag. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.13.5\n",
      "wandb: Run data is saved locally in /home/ydemirag/studium/Diffusion-LM/wandb/run-20221116_125809-3tk41xru\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_\n",
      "wandb: ⭐️ View project at https://wandb.ai/yunus-demirag/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/yunus-demirag/huggingface/runs/3tk41xru\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0488, 'learning_rate': 4.935808555435732e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0614, 'learning_rate': 4.871617110871463e-05, 'epoch': 0.15}\n",
      "{'loss': 1.1636, 'learning_rate': 4.807425666307195e-05, 'epoch': 0.23}\n",
      "{'loss': 0.7762, 'learning_rate': 4.743234221742926e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5427, 'learning_rate': 4.679042777178658e-05, 'epoch': 0.39}\n",
      "{'loss': 0.406, 'learning_rate': 4.614851332614389e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3334, 'learning_rate': 4.550659888050121e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2787, 'learning_rate': 4.486468443485852e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2319, 'learning_rate': 4.422276998921584e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2036, 'learning_rate': 4.358085554357316e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1861, 'learning_rate': 4.293894109793047e-05, 'epoch': 0.85}\n",
      "{'loss': 0.1682, 'learning_rate': 4.2297026652287783e-05, 'epoch': 0.92}\n",
      "{'loss': 0.1517, 'learning_rate': 4.1655112206645105e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0751, 'learning_rate': 4.1013197761002414e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0649, 'learning_rate': 4.037128331535973e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0609, 'learning_rate': 3.972936886971705e-05, 'epoch': 1.23}\n",
      "{'loss': 0.054, 'learning_rate': 3.908745442407436e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0482, 'learning_rate': 3.8445539978431674e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0441, 'learning_rate': 3.7803625532788996e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0387, 'learning_rate': 3.7161711087146304e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2385] 2022-11-16 13:22:40,716 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2387] 2022-11-16 13:22:40,717 >>   Num examples = 66513\n",
      "[INFO|trainer.py:2390] 2022-11-16 13:22:40,717 >>   Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10834651440382004, 'eval_runtime': 193.9224, 'eval_samples_per_second': 342.988, 'eval_steps_per_second': 34.302, 'epoch': 1.54}\n",
      "{'loss': 0.0353, 'learning_rate': 3.6519796641503626e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0341, 'learning_rate': 3.587788219586094e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0311, 'learning_rate': 3.523596775021825e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0295, 'learning_rate': 3.459405330457557e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0252, 'learning_rate': 3.395213885893288e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0253, 'learning_rate': 3.3310224413290195e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0117, 'learning_rate': 3.2668309967647517e-05, 'epoch': 2.08}\n",
      "{'loss': 0.01, 'learning_rate': 3.2026395522004825e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0094, 'learning_rate': 3.138448107636214e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0082, 'learning_rate': 3.074256663071946e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0074, 'learning_rate': 3.0100652185076774e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0077, 'learning_rate': 2.945873773943409e-05, 'epoch': 2.46}\n",
      "{'loss': 0.0071, 'learning_rate': 2.8816823293791407e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0066, 'learning_rate': 2.817490884814872e-05, 'epoch': 2.62}\n",
      "{'loss': 0.006, 'learning_rate': 2.7532994402506034e-05, 'epoch': 2.7}\n",
      "{'loss': 0.0054, 'learning_rate': 2.6891079956863352e-05, 'epoch': 2.77}\n",
      "{'loss': 0.0048, 'learning_rate': 2.6249165511220664e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0049, 'learning_rate': 2.560725106557798e-05, 'epoch': 2.93}\n",
      "{'loss': 0.0043, 'learning_rate': 2.4965336619935294e-05, 'epoch': 3.0}\n",
      "{'loss': 0.002, 'learning_rate': 2.4323422174292613e-05, 'epoch': 3.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2385] 2022-11-16 13:50:11,622 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2387] 2022-11-16 13:50:11,622 >>   Num examples = 66513\n",
      "[INFO|trainer.py:2390] 2022-11-16 13:50:11,622 >>   Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09247037023305893, 'eval_runtime': 194.0155, 'eval_samples_per_second': 342.823, 'eval_steps_per_second': 34.286, 'epoch': 3.08}\n",
      "{'loss': 0.0021, 'learning_rate': 2.3681507728649928e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0018, 'learning_rate': 2.303959328300724e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0017, 'learning_rate': 2.2397678837364558e-05, 'epoch': 3.31}\n",
      "{'loss': 0.0015, 'learning_rate': 2.1755764391721873e-05, 'epoch': 3.39}\n",
      "{'loss': 0.0012, 'learning_rate': 2.1113849946079188e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0016, 'learning_rate': 2.0471935500436503e-05, 'epoch': 3.54}\n",
      "{'loss': 0.0014, 'learning_rate': 1.983002105479382e-05, 'epoch': 3.62}\n",
      "{'loss': 0.0014, 'learning_rate': 1.9188106609151133e-05, 'epoch': 3.7}\n",
      "{'loss': 0.0012, 'learning_rate': 1.854619216350845e-05, 'epoch': 3.77}\n",
      "{'loss': 0.001, 'learning_rate': 1.7904277717865764e-05, 'epoch': 3.85}\n",
      "{'loss': 0.0011, 'learning_rate': 1.726236327222308e-05, 'epoch': 3.93}\n",
      "{'loss': 0.0011, 'learning_rate': 1.6620448826580394e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0007, 'learning_rate': 1.597853438093771e-05, 'epoch': 4.08}\n",
      "{'loss': 0.0005, 'learning_rate': 1.5336619935295024e-05, 'epoch': 4.16}\n",
      "{'loss': 0.0006, 'learning_rate': 1.469470548965234e-05, 'epoch': 4.24}\n",
      "{'loss': 0.0004, 'learning_rate': 1.4052791044009656e-05, 'epoch': 4.31}\n",
      "{'loss': 0.0005, 'learning_rate': 1.341087659836697e-05, 'epoch': 4.39}\n",
      "{'loss': 0.0005, 'learning_rate': 1.2768962152724284e-05, 'epoch': 4.47}\n",
      "{'loss': 0.0004, 'learning_rate': 1.2127047707081601e-05, 'epoch': 4.54}\n",
      "{'loss': 0.0005, 'learning_rate': 1.1485133261438916e-05, 'epoch': 4.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2385] 2022-11-16 14:17:46,281 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2387] 2022-11-16 14:17:46,281 >>   Num examples = 66513\n",
      "[INFO|trainer.py:2390] 2022-11-16 14:17:46,281 >>   Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08524111658334732, 'eval_runtime': 193.8138, 'eval_samples_per_second': 343.18, 'eval_steps_per_second': 34.322, 'epoch': 4.62}\n",
      "{'loss': 0.0004, 'learning_rate': 1.0843218815796231e-05, 'epoch': 4.7}\n",
      "{'loss': 0.0003, 'learning_rate': 1.0201304370153546e-05, 'epoch': 4.78}\n",
      "{'loss': 0.0003, 'learning_rate': 9.559389924510861e-06, 'epoch': 4.85}\n",
      "{'loss': 0.0003, 'learning_rate': 8.917475478868177e-06, 'epoch': 4.93}\n",
      "{'loss': 0.0003, 'learning_rate': 8.275561033225492e-06, 'epoch': 5.01}\n",
      "{'loss': 0.0002, 'learning_rate': 7.633646587582807e-06, 'epoch': 5.08}\n",
      "{'loss': 0.0002, 'learning_rate': 6.991732141940123e-06, 'epoch': 5.16}\n",
      "{'loss': 0.0002, 'learning_rate': 6.349817696297438e-06, 'epoch': 5.24}\n",
      "{'loss': 0.0002, 'learning_rate': 5.707903250654753e-06, 'epoch': 5.32}\n",
      "{'loss': 0.0002, 'learning_rate': 5.065988805012068e-06, 'epoch': 5.39}\n",
      "{'loss': 0.0002, 'learning_rate': 4.424074359369383e-06, 'epoch': 5.47}\n",
      "{'loss': 0.0002, 'learning_rate': 3.782159913726699e-06, 'epoch': 5.55}\n",
      "{'loss': 0.0001, 'learning_rate': 3.140245468084014e-06, 'epoch': 5.62}\n",
      "{'loss': 0.0002, 'learning_rate': 2.498331022441329e-06, 'epoch': 5.7}\n",
      "{'loss': 0.0002, 'learning_rate': 1.8564165767986444e-06, 'epoch': 5.78}\n",
      "{'loss': 0.0001, 'learning_rate': 1.2145021311559595e-06, 'epoch': 5.85}\n",
      "{'loss': 0.0001, 'learning_rate': 5.725876855132748e-07, 'epoch': 5.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1506] 2022-11-16 14:43:03,109 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2135] 2022-11-16 14:43:03,110 >> Saving model checkpoint to classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_\n",
      "[INFO|configuration_utils.py:438] 2022-11-16 14:43:03,111 >> Configuration saved in classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6294.7622, 'train_samples_per_second': 61.87, 'train_steps_per_second': 6.187, 'train_loss': 0.14429008824810946, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1081] 2022-11-16 14:43:03,775 >> Model weights saved in classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_/pytorch_model.bin\n",
      "[INFO|trainer.py:2385] 2022-11-16 14:43:03,780 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2387] 2022-11-16 14:43:03,780 >>   Num examples = 66513\n",
      "[INFO|trainer.py:2390] 2022-11-16 14:43:03,780 >>   Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        6.0\n",
      "  train_loss               =     0.1443\n",
      "  train_runtime            = 1:44:54.76\n",
      "  train_samples            =      64909\n",
      "  train_samples_per_second =      61.87\n",
      "  train_steps_per_second   =      6.187\n",
      "11/16/2022 14:43:03 - INFO - __main__ - *** Evaluate ***\n",
      "{'eval_loss': 0.0784742459654808, 'eval_runtime': 196.7309, 'eval_samples_per_second': 338.091, 'eval_steps_per_second': 33.813, 'epoch': 6.0}\n",
      "***** eval metrics *****\n",
      "  epoch                   =        6.0\n",
      "  eval_loss               =     0.0785\n",
      "  eval_runtime            = 0:03:16.73\n",
      "  eval_samples            =      66513\n",
      "  eval_samples_per_second =    338.091\n",
      "  eval_steps_per_second   =     33.813\n",
      "  perplexity              =     1.0816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:460] 2022-11-16 14:46:20,982 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: | 0.485 MB of 0.485 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run history:\n",
      "wandb:                      eval/loss █▄▃▁\n",
      "wandb:                   eval/runtime ▁▁▁█\n",
      "wandb:        eval/samples_per_second ███▁\n",
      "wandb:          eval/steps_per_second ███▁\n",
      "wandb:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "wandb:            train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "wandb:                     train/loss █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:               train/total_flos ▁\n",
      "wandb:               train/train_loss ▁\n",
      "wandb:            train/train_runtime ▁\n",
      "wandb: train/train_samples_per_second ▁\n",
      "wandb:   train/train_steps_per_second ▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                      eval/loss 0.07847\n",
      "wandb:                   eval/runtime 196.7309\n",
      "wandb:        eval/samples_per_second 338.091\n",
      "wandb:          eval/steps_per_second 33.813\n",
      "wandb:                    train/epoch 6.0\n",
      "wandb:              train/global_step 38946\n",
      "wandb:            train/learning_rate 0.0\n",
      "wandb:                     train/loss 0.0001\n",
      "wandb:               train/total_flos 1.2828499755791616e+16\n",
      "wandb:               train/train_loss 0.14429\n",
      "wandb:            train/train_runtime 6294.7622\n",
      "wandb: train/train_samples_per_second 61.87\n",
      "wandb:   train/train_steps_per_second 6.187\n",
      "wandb: \n",
      "wandb: Synced classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_: https://wandb.ai/yunus-demirag/huggingface/runs/3tk41xru\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20221116_125809-3tk41xru/logs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = \"./improved-diffusion/diffusion_models/diff_roc_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e\"\n",
    "DATASET_TRAIN_PATH = \"./datasets/wit3_de_en/train.tags.de-en.txt\"\n",
    "DATASET_VALID_PATH = \"./datasets/wit3_de_en/valid.tags.de-en.txt\"\n",
    "EPOCH = 6\n",
    "BATCH_SIZE = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "PRETRAINED_MODEL ='bert-base-uncased'\n",
    "EXPERIMENT = 'roc'\n",
    "SEED = 101\n",
    "TASK = 'translation'\n",
    "NOTES = ''\n",
    "APP = f\"--init_emb {MODEL_PATH} --n_embd {16} --learned_emb yes\"\n",
    "BLOCK_SIZE = 100\n",
    "MODEL_TYPE = 'gpt2'\n",
    "\n",
    "folder_name = \"classifier_models\"\n",
    "\n",
    "\n",
    "if not os.path.isdir(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "\n",
    "Model_FILE = EXPERIMENT + \\\n",
    "    '_e={}_b={}_m={}_{}_{}_{}'.format(\n",
    "        EPOCH, BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "        PRETRAINED_MODEL, os.path.basename(DATASET_TRAIN_PATH), SEED, TASK)\n",
    "Model_FILE = Model_FILE + f'_{NOTES}'\n",
    "logging_dir = os.path.join(folder_name, 'runs', Model_FILE)\n",
    "Model_FILE = os.path.join(folder_name, Model_FILE)\n",
    "app = f\" --train_file={DATASET_TRAIN_PATH} --validation_file {DATASET_VALID_PATH} \" \\\n",
    "        f\" --task {TASK}\"\n",
    "app += \" \" + APP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COMMANDLINE = f\"python transformers/examples/pytorch/language-modeling/train_translation.py \\\n",
    "        --output_dir={Model_FILE} \\\n",
    "        --model_name_or_path={PRETRAINED_MODEL} \\\n",
    "        --tokenizer_name={PRETRAINED_MODEL} \\\n",
    "        --per_device_train_batch_size {BATCH_SIZE} \\\n",
    "        --per_device_eval_batch_size {BATCH_SIZE} \\\n",
    "        --save_steps 50000 \\\n",
    "        --num_train_epochs {EPOCH} \\\n",
    "        --do_train --eval_steps 10000 --evaluation_strategy steps \\\n",
    "        --do_eval --dataloader_num_workers 4 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --overwrite_output_dir  \\\n",
    "        --logging_dir {logging_dir} \\\n",
    "        --block_size {BLOCK_SIZE}  \\\n",
    "        --disable_tqdm True --model_type {MODEL_TYPE} \\\n",
    "        --padding_mode none \\\n",
    "        --gradient_accumulation_steps {GRADIENT_ACCUMULATION_STEPS} \" \\\n",
    "                f\"--experiment {EXPERIMENT} --seed {SEED}\"\n",
    "\n",
    "\n",
    "COMMANDLINE += app\n",
    "\n",
    "with open(Model_FILE + '.sh', 'w') as f:\n",
    "    print(COMMANDLINE, file=f)\n",
    "\n",
    "print(COMMANDLINE)\n",
    "os.system(COMMANDLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from ./improved-diffusion/diffusion_models/diff_roc_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e/vocab.json\n",
      "11043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os, json, sys\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from transformers import set_seed\n",
    "import torch.distributed as dist\n",
    "from improved_diffusion.rounding import rounding_func, load_models, load_tokenizer\n",
    "from improved_diffusion.test_util import get_weights, denoised_fn_round\n",
    "from functools import partial\n",
    "from improved_diffusion import dist_util, logger\n",
    "from improved_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    ")\n",
    "from transformers import AutoModelForCausalLM\n",
    "sys.path.insert(0, './improved-diffusion/scripts')\n",
    "from infill_util import langevin_fn3, get_score, langevin_fn3_compose, langevin_fn1, langevin_fn4, langevin_fn_tree, langevin_fn_length\n",
    "from spacy.lang.en import English\n",
    "\n",
    "MODEL_PATH = \"./improved-diffusion/diffusion_models/diff_roc_pad_rand128_transformer_lr0.0001_0.0_2000_sqrt_Lsimple_h128_s2_d0.1_sd101_xstart_e2e\"\n",
    "\n",
    "partial_seq = ['A kid friendly venue named Alimentum is located on the riverside .',\n",
    "                       'Alimentum , situated by the river , is quite child friendly .']\n",
    "\n",
    "emb_model, tokenizer = load_models(modality='roc', mode='random', model_name_or_path='', emb_dim=128, file=MODEL_PATH, )\n",
    "model3 = get_weights(emb_model, {'emb_scale_factor': 1.0})\n",
    "model3 = model3.cuda()\n",
    "\n",
    "\n",
    "##model_control = Classifier_GPT2.from_pretrained('./classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_').cuda()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('./classifier_models/roc_e=6_b=10_m=bert-base-uncased_train.tags.de-en.txt_101_translation_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11282/1931047794.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minput_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m         )\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m         )\n\u001b[1;32m    997\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m         return F.embedding(\n\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "partial_seq = ['A kid friendly venue named Alimentum is located on the riverside .',\n",
    "                       'Alimentum , situated by the river , is quite child friendly .']\n",
    "\n",
    "tokens2id = {v:k for k, v in tokenizer.items()}\n",
    "todo_pad_token = -1\n",
    "pad_token = tokens2id['PAD']\n",
    "encoded_partial_seq = [th.LongTensor([tokens2id.get(x, tokens2id['UNK']) for x in seq.split()]).cuda() for seq in partial_seq]\n",
    "\n",
    "\n",
    "\n",
    "embedding = model3(encoded_partial_seq[0])\n",
    "embedding\n",
    "input_embs = th.nn.Parameter(embedding)\n",
    "model_out = model(input_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('Diffusion-LM': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb02dc56016c5c7b1e2062c7b40f50aeaad4c023c9f15df42684d8182a57eebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
