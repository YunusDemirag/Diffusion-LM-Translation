{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation with Diffusion-LM and CoVost\n",
    "\n",
    "## Formatting the Covost Dataset\n",
    "\n",
    "For simplicity we want the dataset in a line-by-line format to apply a tokenizer. The dataset structure is as following: <br>\n",
    "path (audio filename), sentence (transcript), translation, client_id (speaker ID) all seperated by tab chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVOST_PATH = \"./covost/dataset\"\n",
    "\n",
    "for split in ['dev', 'test', 'train']:\n",
    "    with open(f'{COVOST_PATH}/covost_v2.de_en.{split}.tsv', 'r') as tsv, open(f'{COVOST_PATH}/covost_v2.de_en.{split}.txt', 'w') as txt:\n",
    "        for line in tsv.readlines():\n",
    "            _path, sentence, translation, _client_id, *_ = line.split('\\t')\n",
    "            txt.write(f'{sentence} {translation}\\n')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "COVOST_PATH = \"./covost/dataset\"\n",
    "\n",
    "def replace_german_special_chars(string: str) -> str:\n",
    "\n",
    "    replacements = {\n",
    "        'ä':\"ae\",\n",
    "        'ü':\"ue\",\n",
    "        'ö':\"oe\",\n",
    "        'ß':\"ss\"\n",
    "    }\n",
    "\n",
    "    for char, replacement in replacements.items():\n",
    "        string = string.replace(char, replacement)\n",
    "        \n",
    "    return string\n",
    "\n",
    "# function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # define the pattern to keep\n",
    "    regex = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\-]' \n",
    "    return re.sub(regex, '', text)\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "for split in ['dev', 'test', 'train']:\n",
    "    with open(f'{COVOST_PATH}/covost_v2.de_en.{split}.tsv', 'r') as tsv, open(f'{COVOST_PATH}/covost_v2.de_en.{split}.txt', 'w') as txt:\n",
    "        for line in tsv.readlines():\n",
    "            _path, sentence, translation, _client_id, *_ = line.split('\\t')\n",
    "            sentence_no_german_chars = replace_german_special_chars(sentence)\n",
    "            sentence_no_special_chars = remove_special_characters(sentence_no_german_chars)\n",
    "            sentence_no_special_chars = remove_accented_chars(sentence_no_special_chars)\n",
    "            translation_no_special_chars = remove_special_characters(translation)\n",
    "            translation_no_special_chars = remove_accented_chars(translation_no_special_chars)\n",
    "            txt.write(f'{sentence_no_special_chars} [SEP] {translation_no_special_chars}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Diffusion-LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "COVOST_PATH = \"./covost/dataset\"\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=VOCAB_SIZE)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "files = [f'{COVOST_PATH}/covost_v2.de_en.{split}.txt' for split in [\"test\", \"train\", \"dev\"]]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "tokenizer.save(f\"{COVOST_PATH}/tokenizer_{VOCAB_SIZE}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e\n",
      "{'seed': 102, 'data_dir': '', 'schedule_sampler': 'uniform', 'lr': 0.0001, 'weight_decay': 0.0, 'lr_anneal_steps': 50000, 'batch_size': 64, 'microbatch': -1, 'ema_rate': '0.9999', 'log_interval': 50, 'save_interval': 50000, 'resume_checkpoint': '', 'use_fp16': False, 'fp16_scale_growth': 0.001, 'gradient_clipping': -1.0, 'eval_interval': 2000, 'checkpoint_path': './improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e', 'image_size': 8, 'num_channels': 128, 'num_res_blocks': 2, 'num_heads': 4, 'num_heads_upsample': -1, 'attention_resolutions': '16,8', 'dropout': 0.1, 'learn_sigma': False, 'sigma_small': False, 'class_cond': False, 'diffusion_steps': 2000, 'noise_schedule': 'sqrt', 'timestep_respacing': '', 'use_kl': False, 'predict_xstart': True, 'rescale_timesteps': True, 'rescale_learned_sigmas': True, 'use_checkpoint': False, 'use_scale_shift_norm': True, 'model_arch': 'transformer', 'in_channel': 256, 'out_channel': 256, 'training_mode': 'e2e', 'vocab_size': 30000, 'config_name': 'bert-base-uncased', 'experiment_mode': 'lm', 'logits_mode': 1, 'modality': 'text', 'dataset_name': 'covost', 'experiment': 'translation', 'loss_type': 'Lsimple', 'hidden_size': 128, 'bsz': 64, 'diff_steps': 4000, 'emb_scale_factor': 1.0, 'noise_level': 0.0, 'cache_mode': 'no', 'use_bert_tokenizer': 'no', 'padding_mode': 'pad', 'preprocessing_num_workers': 1, 'submit': 'no', 'notes': 'xstart_e2e'}\n",
      "creating model and diffusion...\n",
      "creating model, based on transformer\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "LossType.E2E_MSE False\n",
      "training mode is  e2e\n",
      "training mode is  e2e\n",
      "the parameter count is 95194672\n",
      "saving the hyperparameters to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e/training_args.json\n",
      "creating data loader...\n",
      "load data **************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function create_data_loaders.<locals>.pad_function at 0x7fb2e5f395f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished filtering the dataset lines: 9 out of 13509 were too long. (0.06662225183211193\\%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ed5660015449babafc8f9deb4d62fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='padding', max=14.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished filtering the dataset lines: 45 out of 127637 were too long. (0.03525623447746343\\%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d662b80662074422a8863d419100db35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='padding', max=128.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished filtering the dataset lines: 4 out of 13509 were too long. (0.029609889703160856\\%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8308a57ca06b4730a28915e7002309e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='padding', max=14.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding model: Embedding(30000, 256)\n",
      " Requires Grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myunus-demirag\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ydemirag/studium/Diffusion-LM/wandb/run-20221208_154710-3os32mzj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yunus-demirag/diffusion_lm/runs/3os32mzj\" target=\"_blank\">./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e</a></strong> to <a href=\"https://wandb.ai/yunus-demirag/diffusion_lm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "8\n",
      "------------------------\n",
      "| grad_norm | 1.01     |\n",
      "| loss      | 0.953    |\n",
      "| loss_q0   | 0.939    |\n",
      "| loss_q1   | 0.95     |\n",
      "| loss_q2   | 0.958    |\n",
      "| loss_q3   | 0.961    |\n",
      "| mse       | 0.953    |\n",
      "| mse_q0    | 0.939    |\n",
      "| mse_q1    | 0.95     |\n",
      "| mse_q2    | 0.958    |\n",
      "| mse_q3    | 0.961    |\n",
      "| samples   | 64       |\n",
      "| step      | 0        |\n",
      "------------------------\n",
      "8\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.74     |\n",
      "| eval_loss_q0 | 0.729    |\n",
      "| eval_loss_q1 | 0.735    |\n",
      "| eval_loss_q2 | 0.763    |\n",
      "| eval_loss_q3 | 0.732    |\n",
      "| eval_mse     | 0.74     |\n",
      "| eval_mse_q0  | 0.729    |\n",
      "| eval_mse_q1  | 0.735    |\n",
      "| eval_mse_q2  | 0.763    |\n",
      "| eval_mse_q3  | 0.732    |\n",
      "---------------------------\n",
      "saving model 0...\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e/model000000.pt\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e/model000000.pt\n",
      "saving model 0.9999...\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e/ema_0.9999_000000.pt\n",
      "writing to ./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e/ema_0.9999_000000.pt\n",
      "------------------------\n",
      "| grad_norm | 0.58     |\n",
      "| loss      | 0.548    |\n",
      "| loss_q0   | 0.527    |\n",
      "| loss_q1   | 0.543    |\n",
      "| loss_q2   | 0.546    |\n",
      "| loss_q3   | 0.573    |\n",
      "| mse       | 0.548    |\n",
      "| mse_q0    | 0.527    |\n",
      "| mse_q1    | 0.543    |\n",
      "| mse_q2    | 0.546    |\n",
      "| mse_q3    | 0.573    |\n",
      "| samples   | 3.26e+03 |\n",
      "| step      | 50       |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0738   |\n",
      "| loss      | 0.411    |\n",
      "| loss_q0   | 0.406    |\n",
      "| loss_q1   | 0.404    |\n",
      "| loss_q2   | 0.408    |\n",
      "| loss_q3   | 0.424    |\n",
      "| mse       | 0.411    |\n",
      "| mse_q0    | 0.406    |\n",
      "| mse_q1    | 0.404    |\n",
      "| mse_q2    | 0.408    |\n",
      "| mse_q3    | 0.424    |\n",
      "| samples   | 6.46e+03 |\n",
      "| step      | 100      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.396    |\n",
      "| loss_q0   | 0.388    |\n",
      "| loss_q1   | 0.384    |\n",
      "| loss_q2   | 0.401    |\n",
      "| loss_q3   | 0.408    |\n",
      "| mse       | 0.396    |\n",
      "| mse_q0    | 0.388    |\n",
      "| mse_q1    | 0.384    |\n",
      "| mse_q2    | 0.401    |\n",
      "| mse_q3    | 0.408    |\n",
      "| samples   | 9.66e+03 |\n",
      "| step      | 150      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0587   |\n",
      "| loss      | 0.397    |\n",
      "| loss_q0   | 0.395    |\n",
      "| loss_q1   | 0.397    |\n",
      "| loss_q2   | 0.389    |\n",
      "| loss_q3   | 0.404    |\n",
      "| mse       | 0.397    |\n",
      "| mse_q0    | 0.395    |\n",
      "| mse_q1    | 0.397    |\n",
      "| mse_q2    | 0.389    |\n",
      "| mse_q3    | 0.404    |\n",
      "| samples   | 1.29e+04 |\n",
      "| step      | 200      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0488   |\n",
      "| loss      | 0.387    |\n",
      "| loss_q0   | 0.383    |\n",
      "| loss_q1   | 0.389    |\n",
      "| loss_q2   | 0.377    |\n",
      "| loss_q3   | 0.4      |\n",
      "| mse       | 0.387    |\n",
      "| mse_q0    | 0.383    |\n",
      "| mse_q1    | 0.389    |\n",
      "| mse_q2    | 0.377    |\n",
      "| mse_q3    | 0.4      |\n",
      "| samples   | 1.61e+04 |\n",
      "| step      | 250      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.399    |\n",
      "| loss_q0   | 0.394    |\n",
      "| loss_q1   | 0.399    |\n",
      "| loss_q2   | 0.398    |\n",
      "| loss_q3   | 0.403    |\n",
      "| mse       | 0.399    |\n",
      "| mse_q0    | 0.394    |\n",
      "| mse_q1    | 0.399    |\n",
      "| mse_q2    | 0.398    |\n",
      "| mse_q3    | 0.403    |\n",
      "| samples   | 1.93e+04 |\n",
      "| step      | 300      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0545   |\n",
      "| loss      | 0.389    |\n",
      "| loss_q0   | 0.383    |\n",
      "| loss_q1   | 0.386    |\n",
      "| loss_q2   | 0.393    |\n",
      "| loss_q3   | 0.394    |\n",
      "| mse       | 0.389    |\n",
      "| mse_q0    | 0.383    |\n",
      "| mse_q1    | 0.386    |\n",
      "| mse_q2    | 0.393    |\n",
      "| mse_q3    | 0.394    |\n",
      "| samples   | 2.25e+04 |\n",
      "| step      | 350      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0788   |\n",
      "| loss      | 0.375    |\n",
      "| loss_q0   | 0.363    |\n",
      "| loss_q1   | 0.368    |\n",
      "| loss_q2   | 0.375    |\n",
      "| loss_q3   | 0.394    |\n",
      "| mse       | 0.375    |\n",
      "| mse_q0    | 0.363    |\n",
      "| mse_q1    | 0.368    |\n",
      "| mse_q2    | 0.375    |\n",
      "| mse_q3    | 0.394    |\n",
      "| samples   | 2.57e+04 |\n",
      "| step      | 400      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0621   |\n",
      "| loss      | 0.366    |\n",
      "| loss_q0   | 0.366    |\n",
      "| loss_q1   | 0.359    |\n",
      "| loss_q2   | 0.359    |\n",
      "| loss_q3   | 0.38     |\n",
      "| mse       | 0.366    |\n",
      "| mse_q0    | 0.366    |\n",
      "| mse_q1    | 0.359    |\n",
      "| mse_q2    | 0.359    |\n",
      "| mse_q3    | 0.38     |\n",
      "| samples   | 2.89e+04 |\n",
      "| step      | 450      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0592   |\n",
      "| loss      | 0.36     |\n",
      "| loss_q0   | 0.353    |\n",
      "| loss_q1   | 0.357    |\n",
      "| loss_q2   | 0.356    |\n",
      "| loss_q3   | 0.373    |\n",
      "| mse       | 0.36     |\n",
      "| mse_q0    | 0.353    |\n",
      "| mse_q1    | 0.357    |\n",
      "| mse_q2    | 0.356    |\n",
      "| mse_q3    | 0.373    |\n",
      "| samples   | 3.21e+04 |\n",
      "| step      | 500      |\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to serialize metric: The operating system has blocked the request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "| grad_norm | 0.0701   |\n",
      "| loss      | 0.355    |\n",
      "| loss_q0   | 0.346    |\n",
      "| loss_q1   | 0.351    |\n",
      "| loss_q2   | 0.354    |\n",
      "| loss_q3   | 0.371    |\n",
      "| mse       | 0.355    |\n",
      "| mse_q0    | 0.346    |\n",
      "| mse_q1    | 0.351    |\n",
      "| mse_q2    | 0.354    |\n",
      "| mse_q3    | 0.371    |\n",
      "| samples   | 3.53e+04 |\n",
      "| step      | 550      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.081    |\n",
      "| loss      | 0.345    |\n",
      "| loss_q0   | 0.334    |\n",
      "| loss_q1   | 0.337    |\n",
      "| loss_q2   | 0.343    |\n",
      "| loss_q3   | 0.366    |\n",
      "| mse       | 0.345    |\n",
      "| mse_q0    | 0.334    |\n",
      "| mse_q1    | 0.337    |\n",
      "| mse_q2    | 0.343    |\n",
      "| mse_q3    | 0.366    |\n",
      "| samples   | 3.85e+04 |\n",
      "| step      | 600      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0831   |\n",
      "| loss      | 0.332    |\n",
      "| loss_q0   | 0.318    |\n",
      "| loss_q1   | 0.314    |\n",
      "| loss_q2   | 0.334    |\n",
      "| loss_q3   | 0.359    |\n",
      "| mse       | 0.332    |\n",
      "| mse_q0    | 0.318    |\n",
      "| mse_q1    | 0.314    |\n",
      "| mse_q2    | 0.334    |\n",
      "| mse_q3    | 0.359    |\n",
      "| samples   | 4.17e+04 |\n",
      "| step      | 650      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0807   |\n",
      "| loss      | 0.318    |\n",
      "| loss_q0   | 0.297    |\n",
      "| loss_q1   | 0.307    |\n",
      "| loss_q2   | 0.319    |\n",
      "| loss_q3   | 0.35     |\n",
      "| mse       | 0.318    |\n",
      "| mse_q0    | 0.297    |\n",
      "| mse_q1    | 0.307    |\n",
      "| mse_q2    | 0.319    |\n",
      "| mse_q3    | 0.35     |\n",
      "| samples   | 4.49e+04 |\n",
      "| step      | 700      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0796   |\n",
      "| loss      | 0.304    |\n",
      "| loss_q0   | 0.274    |\n",
      "| loss_q1   | 0.287    |\n",
      "| loss_q2   | 0.302    |\n",
      "| loss_q3   | 0.351    |\n",
      "| mse       | 0.304    |\n",
      "| mse_q0    | 0.274    |\n",
      "| mse_q1    | 0.287    |\n",
      "| mse_q2    | 0.302    |\n",
      "| mse_q3    | 0.351    |\n",
      "| samples   | 4.81e+04 |\n",
      "| step      | 750      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0813   |\n",
      "| loss      | 0.289    |\n",
      "| loss_q0   | 0.253    |\n",
      "| loss_q1   | 0.265    |\n",
      "| loss_q2   | 0.301    |\n",
      "| loss_q3   | 0.339    |\n",
      "| mse       | 0.289    |\n",
      "| mse_q0    | 0.253    |\n",
      "| mse_q1    | 0.265    |\n",
      "| mse_q2    | 0.301    |\n",
      "| mse_q3    | 0.339    |\n",
      "| samples   | 5.13e+04 |\n",
      "| step      | 800      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0739   |\n",
      "| loss      | 0.28     |\n",
      "| loss_q0   | 0.242    |\n",
      "| loss_q1   | 0.26     |\n",
      "| loss_q2   | 0.283    |\n",
      "| loss_q3   | 0.34     |\n",
      "| mse       | 0.28     |\n",
      "| mse_q0    | 0.242    |\n",
      "| mse_q1    | 0.26     |\n",
      "| mse_q2    | 0.283    |\n",
      "| mse_q3    | 0.34     |\n",
      "| samples   | 5.45e+04 |\n",
      "| step      | 850      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0822   |\n",
      "| loss      | 0.274    |\n",
      "| loss_q0   | 0.235    |\n",
      "| loss_q1   | 0.25     |\n",
      "| loss_q2   | 0.28     |\n",
      "| loss_q3   | 0.332    |\n",
      "| mse       | 0.274    |\n",
      "| mse_q0    | 0.235    |\n",
      "| mse_q1    | 0.25     |\n",
      "| mse_q2    | 0.28     |\n",
      "| mse_q3    | 0.332    |\n",
      "| samples   | 5.77e+04 |\n",
      "| step      | 900      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0713   |\n",
      "| loss      | 0.263    |\n",
      "| loss_q0   | 0.217    |\n",
      "| loss_q1   | 0.241    |\n",
      "| loss_q2   | 0.27     |\n",
      "| loss_q3   | 0.326    |\n",
      "| mse       | 0.263    |\n",
      "| mse_q0    | 0.217    |\n",
      "| mse_q1    | 0.241    |\n",
      "| mse_q2    | 0.27     |\n",
      "| mse_q3    | 0.326    |\n",
      "| samples   | 6.09e+04 |\n",
      "| step      | 950      |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0777   |\n",
      "| loss      | 0.259    |\n",
      "| loss_q0   | 0.211    |\n",
      "| loss_q1   | 0.235    |\n",
      "| loss_q2   | 0.266    |\n",
      "| loss_q3   | 0.323    |\n",
      "| mse       | 0.259    |\n",
      "| mse_q0    | 0.211    |\n",
      "| mse_q1    | 0.235    |\n",
      "| mse_q2    | 0.266    |\n",
      "| mse_q3    | 0.323    |\n",
      "| samples   | 6.41e+04 |\n",
      "| step      | 1e+03    |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0736   |\n",
      "| loss      | 0.252    |\n",
      "| loss_q0   | 0.199    |\n",
      "| loss_q1   | 0.231    |\n",
      "| loss_q2   | 0.263    |\n",
      "| loss_q3   | 0.313    |\n",
      "| mse       | 0.252    |\n",
      "| mse_q0    | 0.199    |\n",
      "| mse_q1    | 0.231    |\n",
      "| mse_q2    | 0.263    |\n",
      "| mse_q3    | 0.313    |\n",
      "| samples   | 6.73e+04 |\n",
      "| step      | 1.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0684   |\n",
      "| loss      | 0.247    |\n",
      "| loss_q0   | 0.193    |\n",
      "| loss_q1   | 0.222    |\n",
      "| loss_q2   | 0.255    |\n",
      "| loss_q3   | 0.318    |\n",
      "| mse       | 0.247    |\n",
      "| mse_q0    | 0.193    |\n",
      "| mse_q1    | 0.222    |\n",
      "| mse_q2    | 0.255    |\n",
      "| mse_q3    | 0.318    |\n",
      "| samples   | 7.05e+04 |\n",
      "| step      | 1.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0718   |\n",
      "| loss      | 0.242    |\n",
      "| loss_q0   | 0.185    |\n",
      "| loss_q1   | 0.216    |\n",
      "| loss_q2   | 0.246    |\n",
      "| loss_q3   | 0.324    |\n",
      "| mse       | 0.242    |\n",
      "| mse_q0    | 0.185    |\n",
      "| mse_q1    | 0.216    |\n",
      "| mse_q2    | 0.246    |\n",
      "| mse_q3    | 0.324    |\n",
      "| samples   | 7.37e+04 |\n",
      "| step      | 1.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0718   |\n",
      "| loss      | 0.238    |\n",
      "| loss_q0   | 0.185    |\n",
      "| loss_q1   | 0.212    |\n",
      "| loss_q2   | 0.24     |\n",
      "| loss_q3   | 0.316    |\n",
      "| mse       | 0.238    |\n",
      "| mse_q0    | 0.185    |\n",
      "| mse_q1    | 0.212    |\n",
      "| mse_q2    | 0.24     |\n",
      "| mse_q3    | 0.316    |\n",
      "| samples   | 7.69e+04 |\n",
      "| step      | 1.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0737   |\n",
      "| loss      | 0.238    |\n",
      "| loss_q0   | 0.176    |\n",
      "| loss_q1   | 0.209    |\n",
      "| loss_q2   | 0.247    |\n",
      "| loss_q3   | 0.319    |\n",
      "| mse       | 0.238    |\n",
      "| mse_q0    | 0.176    |\n",
      "| mse_q1    | 0.209    |\n",
      "| mse_q2    | 0.247    |\n",
      "| mse_q3    | 0.319    |\n",
      "| samples   | 8.01e+04 |\n",
      "| step      | 1.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0718   |\n",
      "| loss      | 0.233    |\n",
      "| loss_q0   | 0.175    |\n",
      "| loss_q1   | 0.204    |\n",
      "| loss_q2   | 0.242    |\n",
      "| loss_q3   | 0.308    |\n",
      "| mse       | 0.233    |\n",
      "| mse_q0    | 0.175    |\n",
      "| mse_q1    | 0.204    |\n",
      "| mse_q2    | 0.242    |\n",
      "| mse_q3    | 0.308    |\n",
      "| samples   | 8.33e+04 |\n",
      "| step      | 1.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0695   |\n",
      "| loss      | 0.227    |\n",
      "| loss_q0   | 0.168    |\n",
      "| loss_q1   | 0.207    |\n",
      "| loss_q2   | 0.236    |\n",
      "| loss_q3   | 0.299    |\n",
      "| mse       | 0.227    |\n",
      "| mse_q0    | 0.168    |\n",
      "| mse_q1    | 0.207    |\n",
      "| mse_q2    | 0.236    |\n",
      "| mse_q3    | 0.299    |\n",
      "| samples   | 8.65e+04 |\n",
      "| step      | 1.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0717   |\n",
      "| loss      | 0.227    |\n",
      "| loss_q0   | 0.167    |\n",
      "| loss_q1   | 0.199    |\n",
      "| loss_q2   | 0.237    |\n",
      "| loss_q3   | 0.306    |\n",
      "| mse       | 0.227    |\n",
      "| mse_q0    | 0.167    |\n",
      "| mse_q1    | 0.199    |\n",
      "| mse_q2    | 0.237    |\n",
      "| mse_q3    | 0.306    |\n",
      "| samples   | 8.97e+04 |\n",
      "| step      | 1.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0692   |\n",
      "| loss      | 0.222    |\n",
      "| loss_q0   | 0.161    |\n",
      "| loss_q1   | 0.195    |\n",
      "| loss_q2   | 0.227    |\n",
      "| loss_q3   | 0.302    |\n",
      "| mse       | 0.222    |\n",
      "| mse_q0    | 0.161    |\n",
      "| mse_q1    | 0.195    |\n",
      "| mse_q2    | 0.227    |\n",
      "| mse_q3    | 0.302    |\n",
      "| samples   | 9.29e+04 |\n",
      "| step      | 1.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0716   |\n",
      "| loss      | 0.22     |\n",
      "| loss_q0   | 0.16     |\n",
      "| loss_q1   | 0.197    |\n",
      "| loss_q2   | 0.222    |\n",
      "| loss_q3   | 0.301    |\n",
      "| mse       | 0.22     |\n",
      "| mse_q0    | 0.16     |\n",
      "| mse_q1    | 0.197    |\n",
      "| mse_q2    | 0.222    |\n",
      "| mse_q3    | 0.301    |\n",
      "| samples   | 9.61e+04 |\n",
      "| step      | 1.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0697   |\n",
      "| loss      | 0.216    |\n",
      "| loss_q0   | 0.156    |\n",
      "| loss_q1   | 0.191    |\n",
      "| loss_q2   | 0.224    |\n",
      "| loss_q3   | 0.301    |\n",
      "| mse       | 0.216    |\n",
      "| mse_q0    | 0.156    |\n",
      "| mse_q1    | 0.191    |\n",
      "| mse_q2    | 0.224    |\n",
      "| mse_q3    | 0.301    |\n",
      "| samples   | 9.93e+04 |\n",
      "| step      | 1.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0666   |\n",
      "| loss      | 0.214    |\n",
      "| loss_q0   | 0.152    |\n",
      "| loss_q1   | 0.188    |\n",
      "| loss_q2   | 0.226    |\n",
      "| loss_q3   | 0.296    |\n",
      "| mse       | 0.214    |\n",
      "| mse_q0    | 0.152    |\n",
      "| mse_q1    | 0.188    |\n",
      "| mse_q2    | 0.226    |\n",
      "| mse_q3    | 0.296    |\n",
      "| samples   | 1.02e+05 |\n",
      "| step      | 1.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0706   |\n",
      "| loss      | 0.214    |\n",
      "| loss_q0   | 0.154    |\n",
      "| loss_q1   | 0.186    |\n",
      "| loss_q2   | 0.223    |\n",
      "| loss_q3   | 0.298    |\n",
      "| mse       | 0.214    |\n",
      "| mse_q0    | 0.154    |\n",
      "| mse_q1    | 0.186    |\n",
      "| mse_q2    | 0.223    |\n",
      "| mse_q3    | 0.298    |\n",
      "| samples   | 1.06e+05 |\n",
      "| step      | 1.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.07     |\n",
      "| loss      | 0.212    |\n",
      "| loss_q0   | 0.145    |\n",
      "| loss_q1   | 0.186    |\n",
      "| loss_q2   | 0.221    |\n",
      "| loss_q3   | 0.297    |\n",
      "| mse       | 0.212    |\n",
      "| mse_q0    | 0.145    |\n",
      "| mse_q1    | 0.186    |\n",
      "| mse_q2    | 0.221    |\n",
      "| mse_q3    | 0.297    |\n",
      "| samples   | 1.09e+05 |\n",
      "| step      | 1.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0702   |\n",
      "| loss      | 0.211    |\n",
      "| loss_q0   | 0.144    |\n",
      "| loss_q1   | 0.181    |\n",
      "| loss_q2   | 0.21     |\n",
      "| loss_q3   | 0.304    |\n",
      "| mse       | 0.211    |\n",
      "| mse_q0    | 0.144    |\n",
      "| mse_q1    | 0.181    |\n",
      "| mse_q2    | 0.21     |\n",
      "| mse_q3    | 0.304    |\n",
      "| samples   | 1.12e+05 |\n",
      "| step      | 1.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0693   |\n",
      "| loss      | 0.206    |\n",
      "| loss_q0   | 0.142    |\n",
      "| loss_q1   | 0.177    |\n",
      "| loss_q2   | 0.216    |\n",
      "| loss_q3   | 0.29     |\n",
      "| mse       | 0.206    |\n",
      "| mse_q0    | 0.142    |\n",
      "| mse_q1    | 0.177    |\n",
      "| mse_q2    | 0.216    |\n",
      "| mse_q3    | 0.29     |\n",
      "| samples   | 1.15e+05 |\n",
      "| step      | 1.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0671   |\n",
      "| loss      | 0.205    |\n",
      "| loss_q0   | 0.142    |\n",
      "| loss_q1   | 0.178    |\n",
      "| loss_q2   | 0.212    |\n",
      "| loss_q3   | 0.286    |\n",
      "| mse       | 0.205    |\n",
      "| mse_q0    | 0.142    |\n",
      "| mse_q1    | 0.178    |\n",
      "| mse_q2    | 0.212    |\n",
      "| mse_q3    | 0.286    |\n",
      "| samples   | 1.18e+05 |\n",
      "| step      | 1.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0714   |\n",
      "| loss      | 0.206    |\n",
      "| loss_q0   | 0.139    |\n",
      "| loss_q1   | 0.179    |\n",
      "| loss_q2   | 0.212    |\n",
      "| loss_q3   | 0.295    |\n",
      "| mse       | 0.206    |\n",
      "| mse_q0    | 0.139    |\n",
      "| mse_q1    | 0.179    |\n",
      "| mse_q2    | 0.212    |\n",
      "| mse_q3    | 0.295    |\n",
      "| samples   | 1.22e+05 |\n",
      "| step      | 1.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0658   |\n",
      "| loss      | 0.201    |\n",
      "| loss_q0   | 0.134    |\n",
      "| loss_q1   | 0.174    |\n",
      "| loss_q2   | 0.207    |\n",
      "| loss_q3   | 0.284    |\n",
      "| mse       | 0.201    |\n",
      "| mse_q0    | 0.134    |\n",
      "| mse_q1    | 0.174    |\n",
      "| mse_q2    | 0.207    |\n",
      "| mse_q3    | 0.284    |\n",
      "| samples   | 1.25e+05 |\n",
      "| step      | 1.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0648   |\n",
      "| loss      | 0.199    |\n",
      "| loss_q0   | 0.133    |\n",
      "| loss_q1   | 0.176    |\n",
      "| loss_q2   | 0.205    |\n",
      "| loss_q3   | 0.289    |\n",
      "| mse       | 0.199    |\n",
      "| mse_q0    | 0.133    |\n",
      "| mse_q1    | 0.176    |\n",
      "| mse_q2    | 0.205    |\n",
      "| mse_q3    | 0.289    |\n",
      "| samples   | 1.28e+05 |\n",
      "| step      | 2e+03    |\n",
      "------------------------\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.205    |\n",
      "| eval_loss_q0 | 0.154    |\n",
      "| eval_loss_q1 | 0.17     |\n",
      "| eval_loss_q2 | 0.179    |\n",
      "| eval_loss_q3 | 0.287    |\n",
      "| eval_mse     | 0.205    |\n",
      "| eval_mse_q0  | 0.154    |\n",
      "| eval_mse_q1  | 0.17     |\n",
      "| eval_mse_q2  | 0.179    |\n",
      "| eval_mse_q3  | 0.287    |\n",
      "---------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.065    |\n",
      "| loss      | 0.199    |\n",
      "| loss_q0   | 0.133    |\n",
      "| loss_q1   | 0.171    |\n",
      "| loss_q2   | 0.206    |\n",
      "| loss_q3   | 0.289    |\n",
      "| mse       | 0.199    |\n",
      "| mse_q0    | 0.133    |\n",
      "| mse_q1    | 0.171    |\n",
      "| mse_q2    | 0.206    |\n",
      "| mse_q3    | 0.289    |\n",
      "| samples   | 1.31e+05 |\n",
      "| step      | 2.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0665   |\n",
      "| loss      | 0.194    |\n",
      "| loss_q0   | 0.131    |\n",
      "| loss_q1   | 0.171    |\n",
      "| loss_q2   | 0.206    |\n",
      "| loss_q3   | 0.281    |\n",
      "| mse       | 0.194    |\n",
      "| mse_q0    | 0.131    |\n",
      "| mse_q1    | 0.171    |\n",
      "| mse_q2    | 0.206    |\n",
      "| mse_q3    | 0.281    |\n",
      "| samples   | 1.34e+05 |\n",
      "| step      | 2.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0661   |\n",
      "| loss      | 0.194    |\n",
      "| loss_q0   | 0.127    |\n",
      "| loss_q1   | 0.166    |\n",
      "| loss_q2   | 0.202    |\n",
      "| loss_q3   | 0.283    |\n",
      "| mse       | 0.194    |\n",
      "| mse_q0    | 0.127    |\n",
      "| mse_q1    | 0.166    |\n",
      "| mse_q2    | 0.202    |\n",
      "| mse_q3    | 0.283    |\n",
      "| samples   | 1.38e+05 |\n",
      "| step      | 2.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0677   |\n",
      "| loss      | 0.196    |\n",
      "| loss_q0   | 0.129    |\n",
      "| loss_q1   | 0.162    |\n",
      "| loss_q2   | 0.204    |\n",
      "| loss_q3   | 0.288    |\n",
      "| mse       | 0.196    |\n",
      "| mse_q0    | 0.129    |\n",
      "| mse_q1    | 0.162    |\n",
      "| mse_q2    | 0.204    |\n",
      "| mse_q3    | 0.288    |\n",
      "| samples   | 1.41e+05 |\n",
      "| step      | 2.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0647   |\n",
      "| loss      | 0.195    |\n",
      "| loss_q0   | 0.126    |\n",
      "| loss_q1   | 0.161    |\n",
      "| loss_q2   | 0.199    |\n",
      "| loss_q3   | 0.289    |\n",
      "| mse       | 0.195    |\n",
      "| mse_q0    | 0.126    |\n",
      "| mse_q1    | 0.161    |\n",
      "| mse_q2    | 0.199    |\n",
      "| mse_q3    | 0.289    |\n",
      "| samples   | 1.44e+05 |\n",
      "| step      | 2.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0657   |\n",
      "| loss      | 0.191    |\n",
      "| loss_q0   | 0.124    |\n",
      "| loss_q1   | 0.165    |\n",
      "| loss_q2   | 0.198    |\n",
      "| loss_q3   | 0.277    |\n",
      "| mse       | 0.191    |\n",
      "| mse_q0    | 0.124    |\n",
      "| mse_q1    | 0.165    |\n",
      "| mse_q2    | 0.198    |\n",
      "| mse_q3    | 0.277    |\n",
      "| samples   | 1.47e+05 |\n",
      "| step      | 2.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0658   |\n",
      "| loss      | 0.195    |\n",
      "| loss_q0   | 0.124    |\n",
      "| loss_q1   | 0.163    |\n",
      "| loss_q2   | 0.201    |\n",
      "| loss_q3   | 0.289    |\n",
      "| mse       | 0.195    |\n",
      "| mse_q0    | 0.124    |\n",
      "| mse_q1    | 0.163    |\n",
      "| mse_q2    | 0.201    |\n",
      "| mse_q3    | 0.289    |\n",
      "| samples   | 1.5e+05  |\n",
      "| step      | 2.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0657   |\n",
      "| loss      | 0.191    |\n",
      "| loss_q0   | 0.121    |\n",
      "| loss_q1   | 0.162    |\n",
      "| loss_q2   | 0.195    |\n",
      "| loss_q3   | 0.283    |\n",
      "| mse       | 0.191    |\n",
      "| mse_q0    | 0.121    |\n",
      "| mse_q1    | 0.162    |\n",
      "| mse_q2    | 0.195    |\n",
      "| mse_q3    | 0.283    |\n",
      "| samples   | 1.54e+05 |\n",
      "| step      | 2.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0689   |\n",
      "| loss      | 0.189    |\n",
      "| loss_q0   | 0.121    |\n",
      "| loss_q1   | 0.159    |\n",
      "| loss_q2   | 0.197    |\n",
      "| loss_q3   | 0.28     |\n",
      "| mse       | 0.189    |\n",
      "| mse_q0    | 0.121    |\n",
      "| mse_q1    | 0.159    |\n",
      "| mse_q2    | 0.197    |\n",
      "| mse_q3    | 0.28     |\n",
      "| samples   | 1.57e+05 |\n",
      "| step      | 2.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0667   |\n",
      "| loss      | 0.186    |\n",
      "| loss_q0   | 0.119    |\n",
      "| loss_q1   | 0.158    |\n",
      "| loss_q2   | 0.199    |\n",
      "| loss_q3   | 0.279    |\n",
      "| mse       | 0.186    |\n",
      "| mse_q0    | 0.119    |\n",
      "| mse_q1    | 0.158    |\n",
      "| mse_q2    | 0.199    |\n",
      "| mse_q3    | 0.279    |\n",
      "| samples   | 1.6e+05  |\n",
      "| step      | 2.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0642   |\n",
      "| loss      | 0.188    |\n",
      "| loss_q0   | 0.119    |\n",
      "| loss_q1   | 0.159    |\n",
      "| loss_q2   | 0.196    |\n",
      "| loss_q3   | 0.275    |\n",
      "| mse       | 0.188    |\n",
      "| mse_q0    | 0.119    |\n",
      "| mse_q1    | 0.159    |\n",
      "| mse_q2    | 0.196    |\n",
      "| mse_q3    | 0.275    |\n",
      "| samples   | 1.63e+05 |\n",
      "| step      | 2.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0657   |\n",
      "| loss      | 0.188    |\n",
      "| loss_q0   | 0.114    |\n",
      "| loss_q1   | 0.156    |\n",
      "| loss_q2   | 0.191    |\n",
      "| loss_q3   | 0.291    |\n",
      "| mse       | 0.188    |\n",
      "| mse_q0    | 0.114    |\n",
      "| mse_q1    | 0.156    |\n",
      "| mse_q2    | 0.191    |\n",
      "| mse_q3    | 0.291    |\n",
      "| samples   | 1.66e+05 |\n",
      "| step      | 2.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0668   |\n",
      "| loss      | 0.185    |\n",
      "| loss_q0   | 0.114    |\n",
      "| loss_q1   | 0.155    |\n",
      "| loss_q2   | 0.196    |\n",
      "| loss_q3   | 0.275    |\n",
      "| mse       | 0.185    |\n",
      "| mse_q0    | 0.114    |\n",
      "| mse_q1    | 0.155    |\n",
      "| mse_q2    | 0.196    |\n",
      "| mse_q3    | 0.275    |\n",
      "| samples   | 1.7e+05  |\n",
      "| step      | 2.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0634   |\n",
      "| loss      | 0.184    |\n",
      "| loss_q0   | 0.112    |\n",
      "| loss_q1   | 0.155    |\n",
      "| loss_q2   | 0.193    |\n",
      "| loss_q3   | 0.273    |\n",
      "| mse       | 0.184    |\n",
      "| mse_q0    | 0.112    |\n",
      "| mse_q1    | 0.155    |\n",
      "| mse_q2    | 0.193    |\n",
      "| mse_q3    | 0.273    |\n",
      "| samples   | 1.73e+05 |\n",
      "| step      | 2.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0646   |\n",
      "| loss      | 0.183    |\n",
      "| loss_q0   | 0.113    |\n",
      "| loss_q1   | 0.154    |\n",
      "| loss_q2   | 0.188    |\n",
      "| loss_q3   | 0.275    |\n",
      "| mse       | 0.183    |\n",
      "| mse_q0    | 0.113    |\n",
      "| mse_q1    | 0.154    |\n",
      "| mse_q2    | 0.188    |\n",
      "| mse_q3    | 0.275    |\n",
      "| samples   | 1.76e+05 |\n",
      "| step      | 2.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.063    |\n",
      "| loss      | 0.18     |\n",
      "| loss_q0   | 0.11     |\n",
      "| loss_q1   | 0.15     |\n",
      "| loss_q2   | 0.184    |\n",
      "| loss_q3   | 0.28     |\n",
      "| mse       | 0.18     |\n",
      "| mse_q0    | 0.11     |\n",
      "| mse_q1    | 0.15     |\n",
      "| mse_q2    | 0.184    |\n",
      "| mse_q3    | 0.28     |\n",
      "| samples   | 1.79e+05 |\n",
      "| step      | 2.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0649   |\n",
      "| loss      | 0.183    |\n",
      "| loss_q0   | 0.111    |\n",
      "| loss_q1   | 0.153    |\n",
      "| loss_q2   | 0.19     |\n",
      "| loss_q3   | 0.279    |\n",
      "| mse       | 0.183    |\n",
      "| mse_q0    | 0.111    |\n",
      "| mse_q1    | 0.153    |\n",
      "| mse_q2    | 0.19     |\n",
      "| mse_q3    | 0.279    |\n",
      "| samples   | 1.82e+05 |\n",
      "| step      | 2.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0638   |\n",
      "| loss      | 0.183    |\n",
      "| loss_q0   | 0.11     |\n",
      "| loss_q1   | 0.152    |\n",
      "| loss_q2   | 0.192    |\n",
      "| loss_q3   | 0.276    |\n",
      "| mse       | 0.183    |\n",
      "| mse_q0    | 0.11     |\n",
      "| mse_q1    | 0.152    |\n",
      "| mse_q2    | 0.192    |\n",
      "| mse_q3    | 0.276    |\n",
      "| samples   | 1.86e+05 |\n",
      "| step      | 2.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0635   |\n",
      "| loss      | 0.181    |\n",
      "| loss_q0   | 0.108    |\n",
      "| loss_q1   | 0.15     |\n",
      "| loss_q2   | 0.191    |\n",
      "| loss_q3   | 0.274    |\n",
      "| mse       | 0.181    |\n",
      "| mse_q0    | 0.108    |\n",
      "| mse_q1    | 0.15     |\n",
      "| mse_q2    | 0.191    |\n",
      "| mse_q3    | 0.274    |\n",
      "| samples   | 1.89e+05 |\n",
      "| step      | 2.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0625   |\n",
      "| loss      | 0.176    |\n",
      "| loss_q0   | 0.108    |\n",
      "| loss_q1   | 0.149    |\n",
      "| loss_q2   | 0.184    |\n",
      "| loss_q3   | 0.271    |\n",
      "| mse       | 0.176    |\n",
      "| mse_q0    | 0.108    |\n",
      "| mse_q1    | 0.149    |\n",
      "| mse_q2    | 0.184    |\n",
      "| mse_q3    | 0.271    |\n",
      "| samples   | 1.92e+05 |\n",
      "| step      | 3e+03    |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0622   |\n",
      "| loss      | 0.178    |\n",
      "| loss_q0   | 0.11     |\n",
      "| loss_q1   | 0.151    |\n",
      "| loss_q2   | 0.184    |\n",
      "| loss_q3   | 0.267    |\n",
      "| mse       | 0.178    |\n",
      "| mse_q0    | 0.11     |\n",
      "| mse_q1    | 0.151    |\n",
      "| mse_q2    | 0.184    |\n",
      "| mse_q3    | 0.267    |\n",
      "| samples   | 1.95e+05 |\n",
      "| step      | 3.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0633   |\n",
      "| loss      | 0.178    |\n",
      "| loss_q0   | 0.108    |\n",
      "| loss_q1   | 0.149    |\n",
      "| loss_q2   | 0.184    |\n",
      "| loss_q3   | 0.267    |\n",
      "| mse       | 0.178    |\n",
      "| mse_q0    | 0.108    |\n",
      "| mse_q1    | 0.149    |\n",
      "| mse_q2    | 0.184    |\n",
      "| mse_q3    | 0.267    |\n",
      "| samples   | 1.98e+05 |\n",
      "| step      | 3.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0633   |\n",
      "| loss      | 0.175    |\n",
      "| loss_q0   | 0.106    |\n",
      "| loss_q1   | 0.148    |\n",
      "| loss_q2   | 0.183    |\n",
      "| loss_q3   | 0.269    |\n",
      "| mse       | 0.175    |\n",
      "| mse_q0    | 0.106    |\n",
      "| mse_q1    | 0.148    |\n",
      "| mse_q2    | 0.183    |\n",
      "| mse_q3    | 0.269    |\n",
      "| samples   | 2.02e+05 |\n",
      "| step      | 3.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0623   |\n",
      "| loss      | 0.175    |\n",
      "| loss_q0   | 0.104    |\n",
      "| loss_q1   | 0.145    |\n",
      "| loss_q2   | 0.185    |\n",
      "| loss_q3   | 0.267    |\n",
      "| mse       | 0.175    |\n",
      "| mse_q0    | 0.104    |\n",
      "| mse_q1    | 0.145    |\n",
      "| mse_q2    | 0.185    |\n",
      "| mse_q3    | 0.267    |\n",
      "| samples   | 2.05e+05 |\n",
      "| step      | 3.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0599   |\n",
      "| loss      | 0.174    |\n",
      "| loss_q0   | 0.104    |\n",
      "| loss_q1   | 0.144    |\n",
      "| loss_q2   | 0.179    |\n",
      "| loss_q3   | 0.27     |\n",
      "| mse       | 0.174    |\n",
      "| mse_q0    | 0.104    |\n",
      "| mse_q1    | 0.144    |\n",
      "| mse_q2    | 0.179    |\n",
      "| mse_q3    | 0.27     |\n",
      "| samples   | 2.08e+05 |\n",
      "| step      | 3.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0612   |\n",
      "| loss      | 0.172    |\n",
      "| loss_q0   | 0.101    |\n",
      "| loss_q1   | 0.141    |\n",
      "| loss_q2   | 0.182    |\n",
      "| loss_q3   | 0.264    |\n",
      "| mse       | 0.172    |\n",
      "| mse_q0    | 0.101    |\n",
      "| mse_q1    | 0.141    |\n",
      "| mse_q2    | 0.182    |\n",
      "| mse_q3    | 0.264    |\n",
      "| samples   | 2.11e+05 |\n",
      "| step      | 3.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0634   |\n",
      "| loss      | 0.176    |\n",
      "| loss_q0   | 0.105    |\n",
      "| loss_q1   | 0.14     |\n",
      "| loss_q2   | 0.181    |\n",
      "| loss_q3   | 0.271    |\n",
      "| mse       | 0.176    |\n",
      "| mse_q0    | 0.105    |\n",
      "| mse_q1    | 0.14     |\n",
      "| mse_q2    | 0.181    |\n",
      "| mse_q3    | 0.271    |\n",
      "| samples   | 2.14e+05 |\n",
      "| step      | 3.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0619   |\n",
      "| loss      | 0.172    |\n",
      "| loss_q0   | 0.102    |\n",
      "| loss_q1   | 0.145    |\n",
      "| loss_q2   | 0.176    |\n",
      "| loss_q3   | 0.266    |\n",
      "| mse       | 0.172    |\n",
      "| mse_q0    | 0.102    |\n",
      "| mse_q1    | 0.145    |\n",
      "| mse_q2    | 0.176    |\n",
      "| mse_q3    | 0.266    |\n",
      "| samples   | 2.18e+05 |\n",
      "| step      | 3.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0637   |\n",
      "| loss      | 0.171    |\n",
      "| loss_q0   | 0.101    |\n",
      "| loss_q1   | 0.143    |\n",
      "| loss_q2   | 0.178    |\n",
      "| loss_q3   | 0.264    |\n",
      "| mse       | 0.171    |\n",
      "| mse_q0    | 0.101    |\n",
      "| mse_q1    | 0.143    |\n",
      "| mse_q2    | 0.178    |\n",
      "| mse_q3    | 0.264    |\n",
      "| samples   | 2.21e+05 |\n",
      "| step      | 3.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0606   |\n",
      "| loss      | 0.169    |\n",
      "| loss_q0   | 0.101    |\n",
      "| loss_q1   | 0.14     |\n",
      "| loss_q2   | 0.178    |\n",
      "| loss_q3   | 0.265    |\n",
      "| mse       | 0.169    |\n",
      "| mse_q0    | 0.101    |\n",
      "| mse_q1    | 0.14     |\n",
      "| mse_q2    | 0.178    |\n",
      "| mse_q3    | 0.265    |\n",
      "| samples   | 2.24e+05 |\n",
      "| step      | 3.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0618   |\n",
      "| loss      | 0.171    |\n",
      "| loss_q0   | 0.101    |\n",
      "| loss_q1   | 0.14     |\n",
      "| loss_q2   | 0.18     |\n",
      "| loss_q3   | 0.265    |\n",
      "| mse       | 0.171    |\n",
      "| mse_q0    | 0.101    |\n",
      "| mse_q1    | 0.14     |\n",
      "| mse_q2    | 0.18     |\n",
      "| mse_q3    | 0.265    |\n",
      "| samples   | 2.27e+05 |\n",
      "| step      | 3.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0617   |\n",
      "| loss      | 0.17     |\n",
      "| loss_q0   | 0.101    |\n",
      "| loss_q1   | 0.139    |\n",
      "| loss_q2   | 0.182    |\n",
      "| loss_q3   | 0.265    |\n",
      "| mse       | 0.17     |\n",
      "| mse_q0    | 0.101    |\n",
      "| mse_q1    | 0.139    |\n",
      "| mse_q2    | 0.182    |\n",
      "| mse_q3    | 0.265    |\n",
      "| samples   | 2.3e+05  |\n",
      "| step      | 3.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0637   |\n",
      "| loss      | 0.169    |\n",
      "| loss_q0   | 0.0998   |\n",
      "| loss_q1   | 0.138    |\n",
      "| loss_q2   | 0.174    |\n",
      "| loss_q3   | 0.261    |\n",
      "| mse       | 0.169    |\n",
      "| mse_q0    | 0.0998   |\n",
      "| mse_q1    | 0.138    |\n",
      "| mse_q2    | 0.174    |\n",
      "| mse_q3    | 0.261    |\n",
      "| samples   | 2.34e+05 |\n",
      "| step      | 3.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0636   |\n",
      "| loss      | 0.167    |\n",
      "| loss_q0   | 0.0968   |\n",
      "| loss_q1   | 0.137    |\n",
      "| loss_q2   | 0.176    |\n",
      "| loss_q3   | 0.259    |\n",
      "| mse       | 0.167    |\n",
      "| mse_q0    | 0.0968   |\n",
      "| mse_q1    | 0.137    |\n",
      "| mse_q2    | 0.176    |\n",
      "| mse_q3    | 0.259    |\n",
      "| samples   | 2.37e+05 |\n",
      "| step      | 3.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0632   |\n",
      "| loss      | 0.172    |\n",
      "| loss_q0   | 0.0998   |\n",
      "| loss_q1   | 0.139    |\n",
      "| loss_q2   | 0.178    |\n",
      "| loss_q3   | 0.265    |\n",
      "| mse       | 0.172    |\n",
      "| mse_q0    | 0.0998   |\n",
      "| mse_q1    | 0.139    |\n",
      "| mse_q2    | 0.178    |\n",
      "| mse_q3    | 0.265    |\n",
      "| samples   | 2.4e+05  |\n",
      "| step      | 3.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0639   |\n",
      "| loss      | 0.168    |\n",
      "| loss_q0   | 0.0959   |\n",
      "| loss_q1   | 0.137    |\n",
      "| loss_q2   | 0.177    |\n",
      "| loss_q3   | 0.263    |\n",
      "| mse       | 0.168    |\n",
      "| mse_q0    | 0.0959   |\n",
      "| mse_q1    | 0.137    |\n",
      "| mse_q2    | 0.177    |\n",
      "| mse_q3    | 0.263    |\n",
      "| samples   | 2.43e+05 |\n",
      "| step      | 3.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0612   |\n",
      "| loss      | 0.166    |\n",
      "| loss_q0   | 0.0929   |\n",
      "| loss_q1   | 0.135    |\n",
      "| loss_q2   | 0.176    |\n",
      "| loss_q3   | 0.26     |\n",
      "| mse       | 0.166    |\n",
      "| mse_q0    | 0.0929   |\n",
      "| mse_q1    | 0.135    |\n",
      "| mse_q2    | 0.176    |\n",
      "| mse_q3    | 0.26     |\n",
      "| samples   | 2.46e+05 |\n",
      "| step      | 3.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0613   |\n",
      "| loss      | 0.169    |\n",
      "| loss_q0   | 0.0982   |\n",
      "| loss_q1   | 0.134    |\n",
      "| loss_q2   | 0.171    |\n",
      "| loss_q3   | 0.272    |\n",
      "| mse       | 0.169    |\n",
      "| mse_q0    | 0.0982   |\n",
      "| mse_q1    | 0.134    |\n",
      "| mse_q2    | 0.171    |\n",
      "| mse_q3    | 0.272    |\n",
      "| samples   | 2.5e+05  |\n",
      "| step      | 3.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0612   |\n",
      "| loss      | 0.163    |\n",
      "| loss_q0   | 0.0953   |\n",
      "| loss_q1   | 0.134    |\n",
      "| loss_q2   | 0.17     |\n",
      "| loss_q3   | 0.255    |\n",
      "| mse       | 0.163    |\n",
      "| mse_q0    | 0.0953   |\n",
      "| mse_q1    | 0.134    |\n",
      "| mse_q2    | 0.17     |\n",
      "| mse_q3    | 0.255    |\n",
      "| samples   | 2.53e+05 |\n",
      "| step      | 3.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0598   |\n",
      "| loss      | 0.166    |\n",
      "| loss_q0   | 0.0941   |\n",
      "| loss_q1   | 0.136    |\n",
      "| loss_q2   | 0.169    |\n",
      "| loss_q3   | 0.26     |\n",
      "| mse       | 0.166    |\n",
      "| mse_q0    | 0.0941   |\n",
      "| mse_q1    | 0.136    |\n",
      "| mse_q2    | 0.169    |\n",
      "| mse_q3    | 0.26     |\n",
      "| samples   | 2.56e+05 |\n",
      "| step      | 4e+03    |\n",
      "------------------------\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.144    |\n",
      "| eval_loss_q0 | 0.0886   |\n",
      "| eval_loss_q1 | 0.143    |\n",
      "| eval_loss_q2 | 0.17     |\n",
      "| eval_loss_q3 | 0.201    |\n",
      "| eval_mse     | 0.144    |\n",
      "| eval_mse_q0  | 0.0886   |\n",
      "| eval_mse_q1  | 0.143    |\n",
      "| eval_mse_q2  | 0.17     |\n",
      "| eval_mse_q3  | 0.201    |\n",
      "---------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0599   |\n",
      "| loss      | 0.166    |\n",
      "| loss_q0   | 0.0922   |\n",
      "| loss_q1   | 0.138    |\n",
      "| loss_q2   | 0.174    |\n",
      "| loss_q3   | 0.262    |\n",
      "| mse       | 0.166    |\n",
      "| mse_q0    | 0.0922   |\n",
      "| mse_q1    | 0.138    |\n",
      "| mse_q2    | 0.174    |\n",
      "| mse_q3    | 0.262    |\n",
      "| samples   | 2.59e+05 |\n",
      "| step      | 4.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0613   |\n",
      "| loss      | 0.164    |\n",
      "| loss_q0   | 0.0924   |\n",
      "| loss_q1   | 0.135    |\n",
      "| loss_q2   | 0.172    |\n",
      "| loss_q3   | 0.254    |\n",
      "| mse       | 0.164    |\n",
      "| mse_q0    | 0.0924   |\n",
      "| mse_q1    | 0.135    |\n",
      "| mse_q2    | 0.172    |\n",
      "| mse_q3    | 0.254    |\n",
      "| samples   | 2.62e+05 |\n",
      "| step      | 4.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0618   |\n",
      "| loss      | 0.165    |\n",
      "| loss_q0   | 0.094    |\n",
      "| loss_q1   | 0.132    |\n",
      "| loss_q2   | 0.17     |\n",
      "| loss_q3   | 0.26     |\n",
      "| mse       | 0.165    |\n",
      "| mse_q0    | 0.094    |\n",
      "| mse_q1    | 0.132    |\n",
      "| mse_q2    | 0.17     |\n",
      "| mse_q3    | 0.26     |\n",
      "| samples   | 2.66e+05 |\n",
      "| step      | 4.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0618   |\n",
      "| loss      | 0.166    |\n",
      "| loss_q0   | 0.0924   |\n",
      "| loss_q1   | 0.129    |\n",
      "| loss_q2   | 0.17     |\n",
      "| loss_q3   | 0.264    |\n",
      "| mse       | 0.166    |\n",
      "| mse_q0    | 0.0924   |\n",
      "| mse_q1    | 0.129    |\n",
      "| mse_q2    | 0.17     |\n",
      "| mse_q3    | 0.264    |\n",
      "| samples   | 2.69e+05 |\n",
      "| step      | 4.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0594   |\n",
      "| loss      | 0.164    |\n",
      "| loss_q0   | 0.0916   |\n",
      "| loss_q1   | 0.133    |\n",
      "| loss_q2   | 0.169    |\n",
      "| loss_q3   | 0.258    |\n",
      "| mse       | 0.164    |\n",
      "| mse_q0    | 0.0916   |\n",
      "| mse_q1    | 0.133    |\n",
      "| mse_q2    | 0.169    |\n",
      "| mse_q3    | 0.258    |\n",
      "| samples   | 2.72e+05 |\n",
      "| step      | 4.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0624   |\n",
      "| loss      | 0.162    |\n",
      "| loss_q0   | 0.0895   |\n",
      "| loss_q1   | 0.13     |\n",
      "| loss_q2   | 0.168    |\n",
      "| loss_q3   | 0.263    |\n",
      "| mse       | 0.162    |\n",
      "| mse_q0    | 0.0895   |\n",
      "| mse_q1    | 0.13     |\n",
      "| mse_q2    | 0.168    |\n",
      "| mse_q3    | 0.263    |\n",
      "| samples   | 2.75e+05 |\n",
      "| step      | 4.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0584   |\n",
      "| loss      | 0.16     |\n",
      "| loss_q0   | 0.088    |\n",
      "| loss_q1   | 0.13     |\n",
      "| loss_q2   | 0.168    |\n",
      "| loss_q3   | 0.256    |\n",
      "| mse       | 0.16     |\n",
      "| mse_q0    | 0.088    |\n",
      "| mse_q1    | 0.13     |\n",
      "| mse_q2    | 0.168    |\n",
      "| mse_q3    | 0.256    |\n",
      "| samples   | 2.78e+05 |\n",
      "| step      | 4.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0586   |\n",
      "| loss      | 0.16     |\n",
      "| loss_q0   | 0.0893   |\n",
      "| loss_q1   | 0.129    |\n",
      "| loss_q2   | 0.165    |\n",
      "| loss_q3   | 0.257    |\n",
      "| mse       | 0.16     |\n",
      "| mse_q0    | 0.0893   |\n",
      "| mse_q1    | 0.129    |\n",
      "| mse_q2    | 0.165    |\n",
      "| mse_q3    | 0.257    |\n",
      "| samples   | 2.82e+05 |\n",
      "| step      | 4.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.059    |\n",
      "| loss      | 0.158    |\n",
      "| loss_q0   | 0.0898   |\n",
      "| loss_q1   | 0.131    |\n",
      "| loss_q2   | 0.166    |\n",
      "| loss_q3   | 0.25     |\n",
      "| mse       | 0.158    |\n",
      "| mse_q0    | 0.0898   |\n",
      "| mse_q1    | 0.131    |\n",
      "| mse_q2    | 0.166    |\n",
      "| mse_q3    | 0.25     |\n",
      "| samples   | 2.85e+05 |\n",
      "| step      | 4.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.059    |\n",
      "| loss      | 0.159    |\n",
      "| loss_q0   | 0.0907   |\n",
      "| loss_q1   | 0.128    |\n",
      "| loss_q2   | 0.162    |\n",
      "| loss_q3   | 0.258    |\n",
      "| mse       | 0.159    |\n",
      "| mse_q0    | 0.0907   |\n",
      "| mse_q1    | 0.128    |\n",
      "| mse_q2    | 0.162    |\n",
      "| mse_q3    | 0.258    |\n",
      "| samples   | 2.88e+05 |\n",
      "| step      | 4.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0609   |\n",
      "| loss      | 0.158    |\n",
      "| loss_q0   | 0.0901   |\n",
      "| loss_q1   | 0.126    |\n",
      "| loss_q2   | 0.166    |\n",
      "| loss_q3   | 0.253    |\n",
      "| mse       | 0.158    |\n",
      "| mse_q0    | 0.0901   |\n",
      "| mse_q1    | 0.126    |\n",
      "| mse_q2    | 0.166    |\n",
      "| mse_q3    | 0.253    |\n",
      "| samples   | 2.91e+05 |\n",
      "| step      | 4.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0612   |\n",
      "| loss      | 0.158    |\n",
      "| loss_q0   | 0.0867   |\n",
      "| loss_q1   | 0.129    |\n",
      "| loss_q2   | 0.168    |\n",
      "| loss_q3   | 0.25     |\n",
      "| mse       | 0.158    |\n",
      "| mse_q0    | 0.0867   |\n",
      "| mse_q1    | 0.129    |\n",
      "| mse_q2    | 0.168    |\n",
      "| mse_q3    | 0.25     |\n",
      "| samples   | 2.94e+05 |\n",
      "| step      | 4.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.059    |\n",
      "| loss      | 0.157    |\n",
      "| loss_q0   | 0.0864   |\n",
      "| loss_q1   | 0.125    |\n",
      "| loss_q2   | 0.164    |\n",
      "| loss_q3   | 0.255    |\n",
      "| mse       | 0.157    |\n",
      "| mse_q0    | 0.0864   |\n",
      "| mse_q1    | 0.125    |\n",
      "| mse_q2    | 0.164    |\n",
      "| mse_q3    | 0.255    |\n",
      "| samples   | 2.98e+05 |\n",
      "| step      | 4.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.062    |\n",
      "| loss      | 0.158    |\n",
      "| loss_q0   | 0.0895   |\n",
      "| loss_q1   | 0.127    |\n",
      "| loss_q2   | 0.16     |\n",
      "| loss_q3   | 0.257    |\n",
      "| mse       | 0.158    |\n",
      "| mse_q0    | 0.0895   |\n",
      "| mse_q1    | 0.127    |\n",
      "| mse_q2    | 0.16     |\n",
      "| mse_q3    | 0.257    |\n",
      "| samples   | 3.01e+05 |\n",
      "| step      | 4.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0567   |\n",
      "| loss      | 0.156    |\n",
      "| loss_q0   | 0.0866   |\n",
      "| loss_q1   | 0.127    |\n",
      "| loss_q2   | 0.163    |\n",
      "| loss_q3   | 0.249    |\n",
      "| mse       | 0.156    |\n",
      "| mse_q0    | 0.0866   |\n",
      "| mse_q1    | 0.127    |\n",
      "| mse_q2    | 0.163    |\n",
      "| mse_q3    | 0.249    |\n",
      "| samples   | 3.04e+05 |\n",
      "| step      | 4.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0593   |\n",
      "| loss      | 0.157    |\n",
      "| loss_q0   | 0.0862   |\n",
      "| loss_q1   | 0.125    |\n",
      "| loss_q2   | 0.161    |\n",
      "| loss_q3   | 0.257    |\n",
      "| mse       | 0.157    |\n",
      "| mse_q0    | 0.0862   |\n",
      "| mse_q1    | 0.125    |\n",
      "| mse_q2    | 0.161    |\n",
      "| mse_q3    | 0.257    |\n",
      "| samples   | 3.07e+05 |\n",
      "| step      | 4.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0606   |\n",
      "| loss      | 0.156    |\n",
      "| loss_q0   | 0.0865   |\n",
      "| loss_q1   | 0.123    |\n",
      "| loss_q2   | 0.162    |\n",
      "| loss_q3   | 0.254    |\n",
      "| mse       | 0.156    |\n",
      "| mse_q0    | 0.0865   |\n",
      "| mse_q1    | 0.123    |\n",
      "| mse_q2    | 0.162    |\n",
      "| mse_q3    | 0.254    |\n",
      "| samples   | 3.1e+05  |\n",
      "| step      | 4.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0581   |\n",
      "| loss      | 0.157    |\n",
      "| loss_q0   | 0.0849   |\n",
      "| loss_q1   | 0.124    |\n",
      "| loss_q2   | 0.163    |\n",
      "| loss_q3   | 0.259    |\n",
      "| mse       | 0.157    |\n",
      "| mse_q0    | 0.0849   |\n",
      "| mse_q1    | 0.124    |\n",
      "| mse_q2    | 0.163    |\n",
      "| mse_q3    | 0.259    |\n",
      "| samples   | 3.14e+05 |\n",
      "| step      | 4.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0585   |\n",
      "| loss      | 0.156    |\n",
      "| loss_q0   | 0.0851   |\n",
      "| loss_q1   | 0.123    |\n",
      "| loss_q2   | 0.164    |\n",
      "| loss_q3   | 0.252    |\n",
      "| mse       | 0.156    |\n",
      "| mse_q0    | 0.0851   |\n",
      "| mse_q1    | 0.123    |\n",
      "| mse_q2    | 0.164    |\n",
      "| mse_q3    | 0.252    |\n",
      "| samples   | 3.17e+05 |\n",
      "| step      | 4.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0585   |\n",
      "| loss      | 0.155    |\n",
      "| loss_q0   | 0.0851   |\n",
      "| loss_q1   | 0.126    |\n",
      "| loss_q2   | 0.161    |\n",
      "| loss_q3   | 0.247    |\n",
      "| mse       | 0.155    |\n",
      "| mse_q0    | 0.0851   |\n",
      "| mse_q1    | 0.126    |\n",
      "| mse_q2    | 0.161    |\n",
      "| mse_q3    | 0.247    |\n",
      "| samples   | 3.2e+05  |\n",
      "| step      | 5e+03    |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.059    |\n",
      "| loss      | 0.155    |\n",
      "| loss_q0   | 0.0855   |\n",
      "| loss_q1   | 0.124    |\n",
      "| loss_q2   | 0.158    |\n",
      "| loss_q3   | 0.252    |\n",
      "| mse       | 0.155    |\n",
      "| mse_q0    | 0.0855   |\n",
      "| mse_q1    | 0.124    |\n",
      "| mse_q2    | 0.158    |\n",
      "| mse_q3    | 0.252    |\n",
      "| samples   | 3.23e+05 |\n",
      "| step      | 5.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.155    |\n",
      "| loss_q0   | 0.0846   |\n",
      "| loss_q1   | 0.123    |\n",
      "| loss_q2   | 0.159    |\n",
      "| loss_q3   | 0.248    |\n",
      "| mse       | 0.155    |\n",
      "| mse_q0    | 0.0846   |\n",
      "| mse_q1    | 0.123    |\n",
      "| mse_q2    | 0.159    |\n",
      "| mse_q3    | 0.248    |\n",
      "| samples   | 3.26e+05 |\n",
      "| step      | 5.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0599   |\n",
      "| loss      | 0.155    |\n",
      "| loss_q0   | 0.0858   |\n",
      "| loss_q1   | 0.123    |\n",
      "| loss_q2   | 0.158    |\n",
      "| loss_q3   | 0.253    |\n",
      "| mse       | 0.155    |\n",
      "| mse_q0    | 0.0858   |\n",
      "| mse_q1    | 0.123    |\n",
      "| mse_q2    | 0.158    |\n",
      "| mse_q3    | 0.253    |\n",
      "| samples   | 3.3e+05  |\n",
      "| step      | 5.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0601   |\n",
      "| loss      | 0.155    |\n",
      "| loss_q0   | 0.0863   |\n",
      "| loss_q1   | 0.121    |\n",
      "| loss_q2   | 0.16     |\n",
      "| loss_q3   | 0.256    |\n",
      "| mse       | 0.155    |\n",
      "| mse_q0    | 0.0863   |\n",
      "| mse_q1    | 0.121    |\n",
      "| mse_q2    | 0.16     |\n",
      "| mse_q3    | 0.256    |\n",
      "| samples   | 3.33e+05 |\n",
      "| step      | 5.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0596   |\n",
      "| loss      | 0.153    |\n",
      "| loss_q0   | 0.0849   |\n",
      "| loss_q1   | 0.121    |\n",
      "| loss_q2   | 0.156    |\n",
      "| loss_q3   | 0.25     |\n",
      "| mse       | 0.153    |\n",
      "| mse_q0    | 0.0849   |\n",
      "| mse_q1    | 0.121    |\n",
      "| mse_q2    | 0.156    |\n",
      "| mse_q3    | 0.25     |\n",
      "| samples   | 3.36e+05 |\n",
      "| step      | 5.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0575   |\n",
      "| loss      | 0.153    |\n",
      "| loss_q0   | 0.0823   |\n",
      "| loss_q1   | 0.121    |\n",
      "| loss_q2   | 0.161    |\n",
      "| loss_q3   | 0.249    |\n",
      "| mse       | 0.153    |\n",
      "| mse_q0    | 0.0823   |\n",
      "| mse_q1    | 0.121    |\n",
      "| mse_q2    | 0.161    |\n",
      "| mse_q3    | 0.249    |\n",
      "| samples   | 3.39e+05 |\n",
      "| step      | 5.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0578   |\n",
      "| loss      | 0.152    |\n",
      "| loss_q0   | 0.0824   |\n",
      "| loss_q1   | 0.124    |\n",
      "| loss_q2   | 0.155    |\n",
      "| loss_q3   | 0.246    |\n",
      "| mse       | 0.152    |\n",
      "| mse_q0    | 0.0824   |\n",
      "| mse_q1    | 0.124    |\n",
      "| mse_q2    | 0.155    |\n",
      "| mse_q3    | 0.246    |\n",
      "| samples   | 3.42e+05 |\n",
      "| step      | 5.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0598   |\n",
      "| loss      | 0.153    |\n",
      "| loss_q0   | 0.086    |\n",
      "| loss_q1   | 0.123    |\n",
      "| loss_q2   | 0.158    |\n",
      "| loss_q3   | 0.247    |\n",
      "| mse       | 0.153    |\n",
      "| mse_q0    | 0.086    |\n",
      "| mse_q1    | 0.123    |\n",
      "| mse_q2    | 0.158    |\n",
      "| mse_q3    | 0.247    |\n",
      "| samples   | 3.46e+05 |\n",
      "| step      | 5.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0577   |\n",
      "| loss      | 0.151    |\n",
      "| loss_q0   | 0.0821   |\n",
      "| loss_q1   | 0.12     |\n",
      "| loss_q2   | 0.158    |\n",
      "| loss_q3   | 0.242    |\n",
      "| mse       | 0.151    |\n",
      "| mse_q0    | 0.0821   |\n",
      "| mse_q1    | 0.12     |\n",
      "| mse_q2    | 0.158    |\n",
      "| mse_q3    | 0.242    |\n",
      "| samples   | 3.49e+05 |\n",
      "| step      | 5.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0578   |\n",
      "| loss      | 0.151    |\n",
      "| loss_q0   | 0.0821   |\n",
      "| loss_q1   | 0.122    |\n",
      "| loss_q2   | 0.155    |\n",
      "| loss_q3   | 0.245    |\n",
      "| mse       | 0.151    |\n",
      "| mse_q0    | 0.0821   |\n",
      "| mse_q1    | 0.122    |\n",
      "| mse_q2    | 0.155    |\n",
      "| mse_q3    | 0.245    |\n",
      "| samples   | 3.52e+05 |\n",
      "| step      | 5.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0579   |\n",
      "| loss      | 0.153    |\n",
      "| loss_q0   | 0.0814   |\n",
      "| loss_q1   | 0.12     |\n",
      "| loss_q2   | 0.155    |\n",
      "| loss_q3   | 0.254    |\n",
      "| mse       | 0.153    |\n",
      "| mse_q0    | 0.0814   |\n",
      "| mse_q1    | 0.12     |\n",
      "| mse_q2    | 0.155    |\n",
      "| mse_q3    | 0.254    |\n",
      "| samples   | 3.55e+05 |\n",
      "| step      | 5.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.151    |\n",
      "| loss_q0   | 0.0826   |\n",
      "| loss_q1   | 0.119    |\n",
      "| loss_q2   | 0.155    |\n",
      "| loss_q3   | 0.243    |\n",
      "| mse       | 0.151    |\n",
      "| mse_q0    | 0.0826   |\n",
      "| mse_q1    | 0.119    |\n",
      "| mse_q2    | 0.155    |\n",
      "| mse_q3    | 0.243    |\n",
      "| samples   | 3.58e+05 |\n",
      "| step      | 5.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0565   |\n",
      "| loss      | 0.149    |\n",
      "| loss_q0   | 0.0818   |\n",
      "| loss_q1   | 0.119    |\n",
      "| loss_q2   | 0.157    |\n",
      "| loss_q3   | 0.242    |\n",
      "| mse       | 0.149    |\n",
      "| mse_q0    | 0.0818   |\n",
      "| mse_q1    | 0.119    |\n",
      "| mse_q2    | 0.157    |\n",
      "| mse_q3    | 0.242    |\n",
      "| samples   | 3.62e+05 |\n",
      "| step      | 5.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0592   |\n",
      "| loss      | 0.152    |\n",
      "| loss_q0   | 0.0822   |\n",
      "| loss_q1   | 0.121    |\n",
      "| loss_q2   | 0.157    |\n",
      "| loss_q3   | 0.242    |\n",
      "| mse       | 0.152    |\n",
      "| mse_q0    | 0.0822   |\n",
      "| mse_q1    | 0.121    |\n",
      "| mse_q2    | 0.157    |\n",
      "| mse_q3    | 0.242    |\n",
      "| samples   | 3.65e+05 |\n",
      "| step      | 5.7e+03  |\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to sample metric: Unknown Error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "| grad_norm | 0.0594   |\n",
      "| loss      | 0.152    |\n",
      "| loss_q0   | 0.0805   |\n",
      "| loss_q1   | 0.117    |\n",
      "| loss_q2   | 0.154    |\n",
      "| loss_q3   | 0.248    |\n",
      "| mse       | 0.152    |\n",
      "| mse_q0    | 0.0805   |\n",
      "| mse_q1    | 0.117    |\n",
      "| mse_q2    | 0.154    |\n",
      "| mse_q3    | 0.248    |\n",
      "| samples   | 3.68e+05 |\n",
      "| step      | 5.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0571   |\n",
      "| loss      | 0.15     |\n",
      "| loss_q0   | 0.0789   |\n",
      "| loss_q1   | 0.118    |\n",
      "| loss_q2   | 0.158    |\n",
      "| loss_q3   | 0.244    |\n",
      "| mse       | 0.15     |\n",
      "| mse_q0    | 0.0789   |\n",
      "| mse_q1    | 0.118    |\n",
      "| mse_q2    | 0.158    |\n",
      "| mse_q3    | 0.244    |\n",
      "| samples   | 3.71e+05 |\n",
      "| step      | 5.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.149    |\n",
      "| loss_q0   | 0.0826   |\n",
      "| loss_q1   | 0.119    |\n",
      "| loss_q2   | 0.152    |\n",
      "| loss_q3   | 0.241    |\n",
      "| mse       | 0.149    |\n",
      "| mse_q0    | 0.0826   |\n",
      "| mse_q1    | 0.119    |\n",
      "| mse_q2    | 0.152    |\n",
      "| mse_q3    | 0.241    |\n",
      "| samples   | 3.74e+05 |\n",
      "| step      | 5.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0574   |\n",
      "| loss      | 0.149    |\n",
      "| loss_q0   | 0.0803   |\n",
      "| loss_q1   | 0.118    |\n",
      "| loss_q2   | 0.15     |\n",
      "| loss_q3   | 0.248    |\n",
      "| mse       | 0.149    |\n",
      "| mse_q0    | 0.0803   |\n",
      "| mse_q1    | 0.118    |\n",
      "| mse_q2    | 0.15     |\n",
      "| mse_q3    | 0.248    |\n",
      "| samples   | 3.78e+05 |\n",
      "| step      | 5.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0574   |\n",
      "| loss      | 0.147    |\n",
      "| loss_q0   | 0.0785   |\n",
      "| loss_q1   | 0.116    |\n",
      "| loss_q2   | 0.15     |\n",
      "| loss_q3   | 0.244    |\n",
      "| mse       | 0.147    |\n",
      "| mse_q0    | 0.0785   |\n",
      "| mse_q1    | 0.116    |\n",
      "| mse_q2    | 0.15     |\n",
      "| mse_q3    | 0.244    |\n",
      "| samples   | 3.81e+05 |\n",
      "| step      | 5.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.151    |\n",
      "| loss_q0   | 0.0802   |\n",
      "| loss_q1   | 0.119    |\n",
      "| loss_q2   | 0.154    |\n",
      "| loss_q3   | 0.246    |\n",
      "| mse       | 0.151    |\n",
      "| mse_q0    | 0.0802   |\n",
      "| mse_q1    | 0.119    |\n",
      "| mse_q2    | 0.154    |\n",
      "| mse_q3    | 0.246    |\n",
      "| samples   | 3.84e+05 |\n",
      "| step      | 6e+03    |\n",
      "------------------------\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.151    |\n",
      "| eval_loss_q0 | 0.105    |\n",
      "| eval_loss_q1 | 0.111    |\n",
      "| eval_loss_q2 | 0.145    |\n",
      "| eval_loss_q3 | 0.257    |\n",
      "| eval_mse     | 0.151    |\n",
      "| eval_mse_q0  | 0.105    |\n",
      "| eval_mse_q1  | 0.111    |\n",
      "| eval_mse_q2  | 0.145    |\n",
      "| eval_mse_q3  | 0.257    |\n",
      "---------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0556   |\n",
      "| loss      | 0.148    |\n",
      "| loss_q0   | 0.0786   |\n",
      "| loss_q1   | 0.118    |\n",
      "| loss_q2   | 0.151    |\n",
      "| loss_q3   | 0.243    |\n",
      "| mse       | 0.148    |\n",
      "| mse_q0    | 0.0786   |\n",
      "| mse_q1    | 0.118    |\n",
      "| mse_q2    | 0.151    |\n",
      "| mse_q3    | 0.243    |\n",
      "| samples   | 3.87e+05 |\n",
      "| step      | 6.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0555   |\n",
      "| loss      | 0.148    |\n",
      "| loss_q0   | 0.0784   |\n",
      "| loss_q1   | 0.116    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.251    |\n",
      "| mse       | 0.148    |\n",
      "| mse_q0    | 0.0784   |\n",
      "| mse_q1    | 0.116    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.251    |\n",
      "| samples   | 3.9e+05  |\n",
      "| step      | 6.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.147    |\n",
      "| loss_q0   | 0.0776   |\n",
      "| loss_q1   | 0.118    |\n",
      "| loss_q2   | 0.151    |\n",
      "| loss_q3   | 0.238    |\n",
      "| mse       | 0.147    |\n",
      "| mse_q0    | 0.0776   |\n",
      "| mse_q1    | 0.118    |\n",
      "| mse_q2    | 0.151    |\n",
      "| mse_q3    | 0.238    |\n",
      "| samples   | 3.94e+05 |\n",
      "| step      | 6.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0575   |\n",
      "| loss      | 0.147    |\n",
      "| loss_q0   | 0.0765   |\n",
      "| loss_q1   | 0.116    |\n",
      "| loss_q2   | 0.151    |\n",
      "| loss_q3   | 0.243    |\n",
      "| mse       | 0.147    |\n",
      "| mse_q0    | 0.0765   |\n",
      "| mse_q1    | 0.116    |\n",
      "| mse_q2    | 0.151    |\n",
      "| mse_q3    | 0.243    |\n",
      "| samples   | 3.97e+05 |\n",
      "| step      | 6.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0591   |\n",
      "| loss      | 0.148    |\n",
      "| loss_q0   | 0.0799   |\n",
      "| loss_q1   | 0.115    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.246    |\n",
      "| mse       | 0.148    |\n",
      "| mse_q0    | 0.0799   |\n",
      "| mse_q1    | 0.115    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.246    |\n",
      "| samples   | 4e+05    |\n",
      "| step      | 6.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0612   |\n",
      "| loss      | 0.149    |\n",
      "| loss_q0   | 0.0793   |\n",
      "| loss_q1   | 0.114    |\n",
      "| loss_q2   | 0.149    |\n",
      "| loss_q3   | 0.252    |\n",
      "| mse       | 0.149    |\n",
      "| mse_q0    | 0.0793   |\n",
      "| mse_q1    | 0.114    |\n",
      "| mse_q2    | 0.149    |\n",
      "| mse_q3    | 0.252    |\n",
      "| samples   | 4.03e+05 |\n",
      "| step      | 6.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0576   |\n",
      "| loss      | 0.147    |\n",
      "| loss_q0   | 0.0793   |\n",
      "| loss_q1   | 0.114    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.244    |\n",
      "| mse       | 0.147    |\n",
      "| mse_q0    | 0.0793   |\n",
      "| mse_q1    | 0.114    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.244    |\n",
      "| samples   | 4.06e+05 |\n",
      "| step      | 6.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.058    |\n",
      "| loss      | 0.147    |\n",
      "| loss_q0   | 0.0781   |\n",
      "| loss_q1   | 0.115    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.245    |\n",
      "| mse       | 0.147    |\n",
      "| mse_q0    | 0.0781   |\n",
      "| mse_q1    | 0.115    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.245    |\n",
      "| samples   | 4.1e+05  |\n",
      "| step      | 6.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0585   |\n",
      "| loss      | 0.143    |\n",
      "| loss_q0   | 0.0763   |\n",
      "| loss_q1   | 0.112    |\n",
      "| loss_q2   | 0.145    |\n",
      "| loss_q3   | 0.243    |\n",
      "| mse       | 0.143    |\n",
      "| mse_q0    | 0.0763   |\n",
      "| mse_q1    | 0.112    |\n",
      "| mse_q2    | 0.145    |\n",
      "| mse_q3    | 0.243    |\n",
      "| samples   | 4.13e+05 |\n",
      "| step      | 6.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0586   |\n",
      "| loss      | 0.144    |\n",
      "| loss_q0   | 0.0763   |\n",
      "| loss_q1   | 0.111    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.24     |\n",
      "| mse       | 0.144    |\n",
      "| mse_q0    | 0.0763   |\n",
      "| mse_q1    | 0.111    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.24     |\n",
      "| samples   | 4.16e+05 |\n",
      "| step      | 6.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0566   |\n",
      "| loss      | 0.143    |\n",
      "| loss_q0   | 0.076    |\n",
      "| loss_q1   | 0.112    |\n",
      "| loss_q2   | 0.149    |\n",
      "| loss_q3   | 0.239    |\n",
      "| mse       | 0.143    |\n",
      "| mse_q0    | 0.076    |\n",
      "| mse_q1    | 0.112    |\n",
      "| mse_q2    | 0.149    |\n",
      "| mse_q3    | 0.239    |\n",
      "| samples   | 4.19e+05 |\n",
      "| step      | 6.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0557   |\n",
      "| loss      | 0.142    |\n",
      "| loss_q0   | 0.0777   |\n",
      "| loss_q1   | 0.111    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.237    |\n",
      "| mse       | 0.142    |\n",
      "| mse_q0    | 0.0777   |\n",
      "| mse_q1    | 0.111    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.237    |\n",
      "| samples   | 4.22e+05 |\n",
      "| step      | 6.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0601   |\n",
      "| loss      | 0.146    |\n",
      "| loss_q0   | 0.0777   |\n",
      "| loss_q1   | 0.115    |\n",
      "| loss_q2   | 0.15     |\n",
      "| loss_q3   | 0.243    |\n",
      "| mse       | 0.146    |\n",
      "| mse_q0    | 0.0777   |\n",
      "| mse_q1    | 0.115    |\n",
      "| mse_q2    | 0.15     |\n",
      "| mse_q3    | 0.243    |\n",
      "| samples   | 4.26e+05 |\n",
      "| step      | 6.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0569   |\n",
      "| loss      | 0.141    |\n",
      "| loss_q0   | 0.0751   |\n",
      "| loss_q1   | 0.112    |\n",
      "| loss_q2   | 0.145    |\n",
      "| loss_q3   | 0.234    |\n",
      "| mse       | 0.141    |\n",
      "| mse_q0    | 0.0751   |\n",
      "| mse_q1    | 0.112    |\n",
      "| mse_q2    | 0.145    |\n",
      "| mse_q3    | 0.234    |\n",
      "| samples   | 4.29e+05 |\n",
      "| step      | 6.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0566   |\n",
      "| loss      | 0.14     |\n",
      "| loss_q0   | 0.0749   |\n",
      "| loss_q1   | 0.11     |\n",
      "| loss_q2   | 0.144    |\n",
      "| loss_q3   | 0.233    |\n",
      "| mse       | 0.14     |\n",
      "| mse_q0    | 0.0749   |\n",
      "| mse_q1    | 0.11     |\n",
      "| mse_q2    | 0.144    |\n",
      "| mse_q3    | 0.233    |\n",
      "| samples   | 4.32e+05 |\n",
      "| step      | 6.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0565   |\n",
      "| loss      | 0.144    |\n",
      "| loss_q0   | 0.0785   |\n",
      "| loss_q1   | 0.114    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.241    |\n",
      "| mse       | 0.144    |\n",
      "| mse_q0    | 0.0785   |\n",
      "| mse_q1    | 0.114    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.241    |\n",
      "| samples   | 4.35e+05 |\n",
      "| step      | 6.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.055    |\n",
      "| loss      | 0.141    |\n",
      "| loss_q0   | 0.0769   |\n",
      "| loss_q1   | 0.111    |\n",
      "| loss_q2   | 0.147    |\n",
      "| loss_q3   | 0.237    |\n",
      "| mse       | 0.141    |\n",
      "| mse_q0    | 0.0769   |\n",
      "| mse_q1    | 0.111    |\n",
      "| mse_q2    | 0.147    |\n",
      "| mse_q3    | 0.237    |\n",
      "| samples   | 4.38e+05 |\n",
      "| step      | 6.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0563   |\n",
      "| loss      | 0.143    |\n",
      "| loss_q0   | 0.0747   |\n",
      "| loss_q1   | 0.112    |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.242    |\n",
      "| mse       | 0.143    |\n",
      "| mse_q0    | 0.0747   |\n",
      "| mse_q1    | 0.112    |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.242    |\n",
      "| samples   | 4.42e+05 |\n",
      "| step      | 6.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0574   |\n",
      "| loss      | 0.141    |\n",
      "| loss_q0   | 0.0749   |\n",
      "| loss_q1   | 0.11     |\n",
      "| loss_q2   | 0.144    |\n",
      "| loss_q3   | 0.237    |\n",
      "| mse       | 0.141    |\n",
      "| mse_q0    | 0.0749   |\n",
      "| mse_q1    | 0.11     |\n",
      "| mse_q2    | 0.144    |\n",
      "| mse_q3    | 0.237    |\n",
      "| samples   | 4.45e+05 |\n",
      "| step      | 6.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.055    |\n",
      "| loss      | 0.141    |\n",
      "| loss_q0   | 0.0742   |\n",
      "| loss_q1   | 0.109    |\n",
      "| loss_q2   | 0.145    |\n",
      "| loss_q3   | 0.24     |\n",
      "| mse       | 0.141    |\n",
      "| mse_q0    | 0.0742   |\n",
      "| mse_q1    | 0.109    |\n",
      "| mse_q2    | 0.145    |\n",
      "| mse_q3    | 0.24     |\n",
      "| samples   | 4.48e+05 |\n",
      "| step      | 7e+03    |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0576   |\n",
      "| loss      | 0.142    |\n",
      "| loss_q0   | 0.0736   |\n",
      "| loss_q1   | 0.113    |\n",
      "| loss_q2   | 0.146    |\n",
      "| loss_q3   | 0.237    |\n",
      "| mse       | 0.142    |\n",
      "| mse_q0    | 0.0736   |\n",
      "| mse_q1    | 0.113    |\n",
      "| mse_q2    | 0.146    |\n",
      "| mse_q3    | 0.237    |\n",
      "| samples   | 4.51e+05 |\n",
      "| step      | 7.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0588   |\n",
      "| loss      | 0.144    |\n",
      "| loss_q0   | 0.0767   |\n",
      "| loss_q1   | 0.11     |\n",
      "| loss_q2   | 0.143    |\n",
      "| loss_q3   | 0.242    |\n",
      "| mse       | 0.144    |\n",
      "| mse_q0    | 0.0767   |\n",
      "| mse_q1    | 0.11     |\n",
      "| mse_q2    | 0.143    |\n",
      "| mse_q3    | 0.242    |\n",
      "| samples   | 4.54e+05 |\n",
      "| step      | 7.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.143    |\n",
      "| loss_q0   | 0.0751   |\n",
      "| loss_q1   | 0.111    |\n",
      "| loss_q2   | 0.146    |\n",
      "| loss_q3   | 0.238    |\n",
      "| mse       | 0.143    |\n",
      "| mse_q0    | 0.0751   |\n",
      "| mse_q1    | 0.111    |\n",
      "| mse_q2    | 0.146    |\n",
      "| mse_q3    | 0.238    |\n",
      "| samples   | 4.58e+05 |\n",
      "| step      | 7.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0582   |\n",
      "| loss      | 0.142    |\n",
      "| loss_q0   | 0.0742   |\n",
      "| loss_q1   | 0.111    |\n",
      "| loss_q2   | 0.143    |\n",
      "| loss_q3   | 0.238    |\n",
      "| mse       | 0.142    |\n",
      "| mse_q0    | 0.0742   |\n",
      "| mse_q1    | 0.111    |\n",
      "| mse_q2    | 0.143    |\n",
      "| mse_q3    | 0.238    |\n",
      "| samples   | 4.61e+05 |\n",
      "| step      | 7.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0564   |\n",
      "| loss      | 0.14     |\n",
      "| loss_q0   | 0.0752   |\n",
      "| loss_q1   | 0.112    |\n",
      "| loss_q2   | 0.142    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.14     |\n",
      "| mse_q0    | 0.0752   |\n",
      "| mse_q1    | 0.112    |\n",
      "| mse_q2    | 0.142    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 4.64e+05 |\n",
      "| step      | 7.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.139    |\n",
      "| loss_q0   | 0.0735   |\n",
      "| loss_q1   | 0.109    |\n",
      "| loss_q2   | 0.145    |\n",
      "| loss_q3   | 0.231    |\n",
      "| mse       | 0.139    |\n",
      "| mse_q0    | 0.0735   |\n",
      "| mse_q1    | 0.109    |\n",
      "| mse_q2    | 0.145    |\n",
      "| mse_q3    | 0.231    |\n",
      "| samples   | 4.67e+05 |\n",
      "| step      | 7.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0557   |\n",
      "| loss      | 0.141    |\n",
      "| loss_q0   | 0.0733   |\n",
      "| loss_q1   | 0.11     |\n",
      "| loss_q2   | 0.148    |\n",
      "| loss_q3   | 0.237    |\n",
      "| mse       | 0.141    |\n",
      "| mse_q0    | 0.0733   |\n",
      "| mse_q1    | 0.11     |\n",
      "| mse_q2    | 0.148    |\n",
      "| mse_q3    | 0.237    |\n",
      "| samples   | 4.7e+05  |\n",
      "| step      | 7.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0562   |\n",
      "| loss      | 0.14     |\n",
      "| loss_q0   | 0.0727   |\n",
      "| loss_q1   | 0.11     |\n",
      "| loss_q2   | 0.138    |\n",
      "| loss_q3   | 0.237    |\n",
      "| mse       | 0.14     |\n",
      "| mse_q0    | 0.0727   |\n",
      "| mse_q1    | 0.11     |\n",
      "| mse_q2    | 0.138    |\n",
      "| mse_q3    | 0.237    |\n",
      "| samples   | 4.74e+05 |\n",
      "| step      | 7.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0545   |\n",
      "| loss      | 0.136    |\n",
      "| loss_q0   | 0.072    |\n",
      "| loss_q1   | 0.106    |\n",
      "| loss_q2   | 0.138    |\n",
      "| loss_q3   | 0.234    |\n",
      "| mse       | 0.136    |\n",
      "| mse_q0    | 0.072    |\n",
      "| mse_q1    | 0.106    |\n",
      "| mse_q2    | 0.138    |\n",
      "| mse_q3    | 0.234    |\n",
      "| samples   | 4.77e+05 |\n",
      "| step      | 7.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.14     |\n",
      "| loss_q0   | 0.0727   |\n",
      "| loss_q1   | 0.109    |\n",
      "| loss_q2   | 0.145    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.14     |\n",
      "| mse_q0    | 0.0727   |\n",
      "| mse_q1    | 0.109    |\n",
      "| mse_q2    | 0.145    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 4.8e+05  |\n",
      "| step      | 7.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0571   |\n",
      "| loss      | 0.139    |\n",
      "| loss_q0   | 0.0736   |\n",
      "| loss_q1   | 0.108    |\n",
      "| loss_q2   | 0.139    |\n",
      "| loss_q3   | 0.238    |\n",
      "| mse       | 0.139    |\n",
      "| mse_q0    | 0.0736   |\n",
      "| mse_q1    | 0.108    |\n",
      "| mse_q2    | 0.139    |\n",
      "| mse_q3    | 0.238    |\n",
      "| samples   | 4.83e+05 |\n",
      "| step      | 7.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.14     |\n",
      "| loss_q0   | 0.0739   |\n",
      "| loss_q1   | 0.11     |\n",
      "| loss_q2   | 0.142    |\n",
      "| loss_q3   | 0.237    |\n",
      "| mse       | 0.14     |\n",
      "| mse_q0    | 0.0739   |\n",
      "| mse_q1    | 0.11     |\n",
      "| mse_q2    | 0.142    |\n",
      "| mse_q3    | 0.237    |\n",
      "| samples   | 4.86e+05 |\n",
      "| step      | 7.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.138    |\n",
      "| loss_q0   | 0.0731   |\n",
      "| loss_q1   | 0.109    |\n",
      "| loss_q2   | 0.142    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.138    |\n",
      "| mse_q0    | 0.0731   |\n",
      "| mse_q1    | 0.109    |\n",
      "| mse_q2    | 0.142    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 4.9e+05  |\n",
      "| step      | 7.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0591   |\n",
      "| loss      | 0.138    |\n",
      "| loss_q0   | 0.0732   |\n",
      "| loss_q1   | 0.108    |\n",
      "| loss_q2   | 0.143    |\n",
      "| loss_q3   | 0.229    |\n",
      "| mse       | 0.138    |\n",
      "| mse_q0    | 0.0732   |\n",
      "| mse_q1    | 0.108    |\n",
      "| mse_q2    | 0.143    |\n",
      "| mse_q3    | 0.229    |\n",
      "| samples   | 4.93e+05 |\n",
      "| step      | 7.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0577   |\n",
      "| loss      | 0.14     |\n",
      "| loss_q0   | 0.075    |\n",
      "| loss_q1   | 0.106    |\n",
      "| loss_q2   | 0.139    |\n",
      "| loss_q3   | 0.234    |\n",
      "| mse       | 0.14     |\n",
      "| mse_q0    | 0.075    |\n",
      "| mse_q1    | 0.106    |\n",
      "| mse_q2    | 0.139    |\n",
      "| mse_q3    | 0.234    |\n",
      "| samples   | 4.96e+05 |\n",
      "| step      | 7.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.137    |\n",
      "| loss_q0   | 0.0712   |\n",
      "| loss_q1   | 0.108    |\n",
      "| loss_q2   | 0.138    |\n",
      "| loss_q3   | 0.236    |\n",
      "| mse       | 0.137    |\n",
      "| mse_q0    | 0.0712   |\n",
      "| mse_q1    | 0.108    |\n",
      "| mse_q2    | 0.138    |\n",
      "| mse_q3    | 0.236    |\n",
      "| samples   | 4.99e+05 |\n",
      "| step      | 7.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0564   |\n",
      "| loss      | 0.138    |\n",
      "| loss_q0   | 0.0723   |\n",
      "| loss_q1   | 0.105    |\n",
      "| loss_q2   | 0.143    |\n",
      "| loss_q3   | 0.232    |\n",
      "| mse       | 0.138    |\n",
      "| mse_q0    | 0.0723   |\n",
      "| mse_q1    | 0.105    |\n",
      "| mse_q2    | 0.143    |\n",
      "| mse_q3    | 0.232    |\n",
      "| samples   | 5.02e+05 |\n",
      "| step      | 7.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.138    |\n",
      "| loss_q0   | 0.0731   |\n",
      "| loss_q1   | 0.106    |\n",
      "| loss_q2   | 0.141    |\n",
      "| loss_q3   | 0.229    |\n",
      "| mse       | 0.138    |\n",
      "| mse_q0    | 0.0731   |\n",
      "| mse_q1    | 0.106    |\n",
      "| mse_q2    | 0.141    |\n",
      "| mse_q3    | 0.229    |\n",
      "| samples   | 5.06e+05 |\n",
      "| step      | 7.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.135    |\n",
      "| loss_q0   | 0.0722   |\n",
      "| loss_q1   | 0.106    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.135    |\n",
      "| mse_q0    | 0.0722   |\n",
      "| mse_q1    | 0.106    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 5.09e+05 |\n",
      "| step      | 7.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0555   |\n",
      "| loss      | 0.137    |\n",
      "| loss_q0   | 0.0706   |\n",
      "| loss_q1   | 0.107    |\n",
      "| loss_q2   | 0.142    |\n",
      "| loss_q3   | 0.232    |\n",
      "| mse       | 0.137    |\n",
      "| mse_q0    | 0.0706   |\n",
      "| mse_q1    | 0.107    |\n",
      "| mse_q2    | 0.142    |\n",
      "| mse_q3    | 0.232    |\n",
      "| samples   | 5.12e+05 |\n",
      "| step      | 8e+03    |\n",
      "------------------------\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.133    |\n",
      "| eval_loss_q0 | 0.0646   |\n",
      "| eval_loss_q1 | 0.0914   |\n",
      "| eval_loss_q2 | 0.131    |\n",
      "| eval_loss_q3 | 0.256    |\n",
      "| eval_mse     | 0.133    |\n",
      "| eval_mse_q0  | 0.0646   |\n",
      "| eval_mse_q1  | 0.0914   |\n",
      "| eval_mse_q2  | 0.131    |\n",
      "| eval_mse_q3  | 0.256    |\n",
      "---------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0576   |\n",
      "| loss      | 0.14     |\n",
      "| loss_q0   | 0.0717   |\n",
      "| loss_q1   | 0.108    |\n",
      "| loss_q2   | 0.142    |\n",
      "| loss_q3   | 0.233    |\n",
      "| mse       | 0.14     |\n",
      "| mse_q0    | 0.0717   |\n",
      "| mse_q1    | 0.108    |\n",
      "| mse_q2    | 0.142    |\n",
      "| mse_q3    | 0.233    |\n",
      "| samples   | 5.15e+05 |\n",
      "| step      | 8.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0574   |\n",
      "| loss      | 0.138    |\n",
      "| loss_q0   | 0.0711   |\n",
      "| loss_q1   | 0.104    |\n",
      "| loss_q2   | 0.139    |\n",
      "| loss_q3   | 0.233    |\n",
      "| mse       | 0.138    |\n",
      "| mse_q0    | 0.0711   |\n",
      "| mse_q1    | 0.104    |\n",
      "| mse_q2    | 0.139    |\n",
      "| mse_q3    | 0.233    |\n",
      "| samples   | 5.18e+05 |\n",
      "| step      | 8.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.136    |\n",
      "| loss_q0   | 0.0703   |\n",
      "| loss_q1   | 0.107    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.232    |\n",
      "| mse       | 0.136    |\n",
      "| mse_q0    | 0.0703   |\n",
      "| mse_q1    | 0.107    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.232    |\n",
      "| samples   | 5.22e+05 |\n",
      "| step      | 8.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0556   |\n",
      "| loss      | 0.137    |\n",
      "| loss_q0   | 0.0711   |\n",
      "| loss_q1   | 0.105    |\n",
      "| loss_q2   | 0.139    |\n",
      "| loss_q3   | 0.231    |\n",
      "| mse       | 0.137    |\n",
      "| mse_q0    | 0.0711   |\n",
      "| mse_q1    | 0.105    |\n",
      "| mse_q2    | 0.139    |\n",
      "| mse_q3    | 0.231    |\n",
      "| samples   | 5.25e+05 |\n",
      "| step      | 8.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0567   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0689   |\n",
      "| loss_q1   | 0.104    |\n",
      "| loss_q2   | 0.136    |\n",
      "| loss_q3   | 0.233    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0689   |\n",
      "| mse_q1    | 0.104    |\n",
      "| mse_q2    | 0.136    |\n",
      "| mse_q3    | 0.233    |\n",
      "| samples   | 5.28e+05 |\n",
      "| step      | 8.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0552   |\n",
      "| loss      | 0.135    |\n",
      "| loss_q0   | 0.0708   |\n",
      "| loss_q1   | 0.105    |\n",
      "| loss_q2   | 0.139    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.135    |\n",
      "| mse_q0    | 0.0708   |\n",
      "| mse_q1    | 0.105    |\n",
      "| mse_q2    | 0.139    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 5.31e+05 |\n",
      "| step      | 8.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0568   |\n",
      "| loss      | 0.135    |\n",
      "| loss_q0   | 0.073    |\n",
      "| loss_q1   | 0.105    |\n",
      "| loss_q2   | 0.137    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.135    |\n",
      "| mse_q0    | 0.073    |\n",
      "| mse_q1    | 0.105    |\n",
      "| mse_q2    | 0.137    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 5.34e+05 |\n",
      "| step      | 8.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0576   |\n",
      "| loss      | 0.136    |\n",
      "| loss_q0   | 0.0711   |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.139    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.136    |\n",
      "| mse_q0    | 0.0711   |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.139    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 5.38e+05 |\n",
      "| step      | 8.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0704   |\n",
      "| loss_q1   | 0.103    |\n",
      "| loss_q2   | 0.134    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0704   |\n",
      "| mse_q1    | 0.103    |\n",
      "| mse_q2    | 0.134    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 5.41e+05 |\n",
      "| step      | 8.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0557   |\n",
      "| loss      | 0.137    |\n",
      "| loss_q0   | 0.071    |\n",
      "| loss_q1   | 0.104    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.137    |\n",
      "| mse_q0    | 0.071    |\n",
      "| mse_q1    | 0.104    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 5.44e+05 |\n",
      "| step      | 8.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0561   |\n",
      "| loss      | 0.135    |\n",
      "| loss_q0   | 0.0696   |\n",
      "| loss_q1   | 0.105    |\n",
      "| loss_q2   | 0.138    |\n",
      "| loss_q3   | 0.233    |\n",
      "| mse       | 0.135    |\n",
      "| mse_q0    | 0.0696   |\n",
      "| mse_q1    | 0.105    |\n",
      "| mse_q2    | 0.138    |\n",
      "| mse_q3    | 0.233    |\n",
      "| samples   | 5.47e+05 |\n",
      "| step      | 8.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.055    |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0677   |\n",
      "| loss_q1   | 0.105    |\n",
      "| loss_q2   | 0.137    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0677   |\n",
      "| mse_q1    | 0.105    |\n",
      "| mse_q2    | 0.137    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 5.5e+05  |\n",
      "| step      | 8.6e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0551   |\n",
      "| loss      | 0.135    |\n",
      "| loss_q0   | 0.0698   |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.133    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.135    |\n",
      "| mse_q0    | 0.0698   |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.133    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 5.54e+05 |\n",
      "| step      | 8.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.134    |\n",
      "| loss_q0   | 0.0696   |\n",
      "| loss_q1   | 0.1      |\n",
      "| loss_q2   | 0.136    |\n",
      "| loss_q3   | 0.228    |\n",
      "| mse       | 0.134    |\n",
      "| mse_q0    | 0.0696   |\n",
      "| mse_q1    | 0.1      |\n",
      "| mse_q2    | 0.136    |\n",
      "| mse_q3    | 0.228    |\n",
      "| samples   | 5.57e+05 |\n",
      "| step      | 8.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0598   |\n",
      "| loss      | 0.135    |\n",
      "| loss_q0   | 0.0702   |\n",
      "| loss_q1   | 0.106    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.135    |\n",
      "| mse_q0    | 0.0702   |\n",
      "| mse_q1    | 0.106    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 5.6e+05  |\n",
      "| step      | 8.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0575   |\n",
      "| loss      | 0.135    |\n",
      "| loss_q0   | 0.0692   |\n",
      "| loss_q1   | 0.103    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.229    |\n",
      "| mse       | 0.135    |\n",
      "| mse_q0    | 0.0692   |\n",
      "| mse_q1    | 0.103    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.229    |\n",
      "| samples   | 5.63e+05 |\n",
      "| step      | 8.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.132    |\n",
      "| loss_q0   | 0.0687   |\n",
      "| loss_q1   | 0.103    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.223    |\n",
      "| mse       | 0.132    |\n",
      "| mse_q0    | 0.0687   |\n",
      "| mse_q1    | 0.103    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.223    |\n",
      "| samples   | 5.66e+05 |\n",
      "| step      | 8.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0547   |\n",
      "| loss      | 0.132    |\n",
      "| loss_q0   | 0.0693   |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.131    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.132    |\n",
      "| mse_q0    | 0.0693   |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.131    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 5.7e+05  |\n",
      "| step      | 8.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0688   |\n",
      "| loss_q1   | 0.106    |\n",
      "| loss_q2   | 0.134    |\n",
      "| loss_q3   | 0.222    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0688   |\n",
      "| mse_q1    | 0.106    |\n",
      "| mse_q2    | 0.134    |\n",
      "| mse_q3    | 0.222    |\n",
      "| samples   | 5.73e+05 |\n",
      "| step      | 8.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0571   |\n",
      "| loss      | 0.134    |\n",
      "| loss_q0   | 0.0683   |\n",
      "| loss_q1   | 0.0999   |\n",
      "| loss_q2   | 0.137    |\n",
      "| loss_q3   | 0.225    |\n",
      "| mse       | 0.134    |\n",
      "| mse_q0    | 0.0683   |\n",
      "| mse_q1    | 0.0999   |\n",
      "| mse_q2    | 0.137    |\n",
      "| mse_q3    | 0.225    |\n",
      "| samples   | 5.76e+05 |\n",
      "| step      | 9e+03    |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0576   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0689   |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.133    |\n",
      "| loss_q3   | 0.228    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0689   |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.133    |\n",
      "| mse_q3    | 0.228    |\n",
      "| samples   | 5.79e+05 |\n",
      "| step      | 9.05e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0554   |\n",
      "| loss      | 0.134    |\n",
      "| loss_q0   | 0.067    |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.234    |\n",
      "| mse       | 0.134    |\n",
      "| mse_q0    | 0.067    |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.234    |\n",
      "| samples   | 5.82e+05 |\n",
      "| step      | 9.1e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.055    |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0694   |\n",
      "| loss_q1   | 0.0999   |\n",
      "| loss_q2   | 0.136    |\n",
      "| loss_q3   | 0.224    |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0694   |\n",
      "| mse_q1    | 0.0999   |\n",
      "| mse_q2    | 0.136    |\n",
      "| mse_q3    | 0.224    |\n",
      "| samples   | 5.86e+05 |\n",
      "| step      | 9.15e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0691   |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.132    |\n",
      "| loss_q3   | 0.231    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0691   |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.132    |\n",
      "| mse_q3    | 0.231    |\n",
      "| samples   | 5.89e+05 |\n",
      "| step      | 9.2e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0664   |\n",
      "| loss_q1   | 0.103    |\n",
      "| loss_q2   | 0.133    |\n",
      "| loss_q3   | 0.223    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0664   |\n",
      "| mse_q1    | 0.103    |\n",
      "| mse_q2    | 0.133    |\n",
      "| mse_q3    | 0.223    |\n",
      "| samples   | 5.92e+05 |\n",
      "| step      | 9.25e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0554   |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0679   |\n",
      "| loss_q1   | 0.0995   |\n",
      "| loss_q2   | 0.13     |\n",
      "| loss_q3   | 0.222    |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0679   |\n",
      "| mse_q1    | 0.0995   |\n",
      "| mse_q2    | 0.13     |\n",
      "| mse_q3    | 0.222    |\n",
      "| samples   | 5.95e+05 |\n",
      "| step      | 9.3e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0553   |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0696   |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.132    |\n",
      "| loss_q3   | 0.228    |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0696   |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.132    |\n",
      "| mse_q3    | 0.228    |\n",
      "| samples   | 5.98e+05 |\n",
      "| step      | 9.35e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0568   |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0695   |\n",
      "| loss_q1   | 0.0969   |\n",
      "| loss_q2   | 0.13     |\n",
      "| loss_q3   | 0.225    |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0695   |\n",
      "| mse_q1    | 0.0969   |\n",
      "| mse_q2    | 0.13     |\n",
      "| mse_q3    | 0.225    |\n",
      "| samples   | 6.02e+05 |\n",
      "| step      | 9.4e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0568   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0687   |\n",
      "| loss_q1   | 0.1      |\n",
      "| loss_q2   | 0.133    |\n",
      "| loss_q3   | 0.235    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0687   |\n",
      "| mse_q1    | 0.1      |\n",
      "| mse_q2    | 0.133    |\n",
      "| mse_q3    | 0.235    |\n",
      "| samples   | 6.05e+05 |\n",
      "| step      | 9.45e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.132    |\n",
      "| loss_q0   | 0.0673   |\n",
      "| loss_q1   | 0.1      |\n",
      "| loss_q2   | 0.133    |\n",
      "| loss_q3   | 0.228    |\n",
      "| mse       | 0.132    |\n",
      "| mse_q0    | 0.0673   |\n",
      "| mse_q1    | 0.1      |\n",
      "| mse_q2    | 0.133    |\n",
      "| mse_q3    | 0.228    |\n",
      "| samples   | 6.08e+05 |\n",
      "| step      | 9.5e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.134    |\n",
      "| loss_q0   | 0.067    |\n",
      "| loss_q1   | 0.102    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.231    |\n",
      "| mse       | 0.134    |\n",
      "| mse_q0    | 0.067    |\n",
      "| mse_q1    | 0.102    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.231    |\n",
      "| samples   | 6.11e+05 |\n",
      "| step      | 9.55e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0579   |\n",
      "| loss      | 0.132    |\n",
      "| loss_q0   | 0.0686   |\n",
      "| loss_q1   | 0.0971   |\n",
      "| loss_q2   | 0.134    |\n",
      "| loss_q3   | 0.228    |\n",
      "| mse       | 0.132    |\n",
      "| mse_q0    | 0.0686   |\n",
      "| mse_q1    | 0.0971   |\n",
      "| mse_q2    | 0.134    |\n",
      "| mse_q3    | 0.228    |\n",
      "| samples   | 6.14e+05 |\n",
      "| step      | 9.6e+03  |\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to sample metric: The operating system has blocked the request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "| grad_norm | 0.0557   |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0665   |\n",
      "| loss_q1   | 0.101    |\n",
      "| loss_q2   | 0.131    |\n",
      "| loss_q3   | 0.224    |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0665   |\n",
      "| mse_q1    | 0.101    |\n",
      "| mse_q2    | 0.131    |\n",
      "| mse_q3    | 0.224    |\n",
      "| samples   | 6.18e+05 |\n",
      "| step      | 9.65e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0569   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0663   |\n",
      "| loss_q1   | 0.0992   |\n",
      "| loss_q2   | 0.133    |\n",
      "| loss_q3   | 0.228    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0663   |\n",
      "| mse_q1    | 0.0992   |\n",
      "| mse_q2    | 0.133    |\n",
      "| mse_q3    | 0.228    |\n",
      "| samples   | 6.21e+05 |\n",
      "| step      | 9.7e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0553   |\n",
      "| loss      | 0.128    |\n",
      "| loss_q0   | 0.0679   |\n",
      "| loss_q1   | 0.0976   |\n",
      "| loss_q2   | 0.129    |\n",
      "| loss_q3   | 0.223    |\n",
      "| mse       | 0.128    |\n",
      "| mse_q0    | 0.0679   |\n",
      "| mse_q1    | 0.0976   |\n",
      "| mse_q2    | 0.129    |\n",
      "| mse_q3    | 0.223    |\n",
      "| samples   | 6.24e+05 |\n",
      "| step      | 9.75e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0559   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0652   |\n",
      "| loss_q1   | 0.101    |\n",
      "| loss_q2   | 0.133    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0652   |\n",
      "| mse_q1    | 0.101    |\n",
      "| mse_q2    | 0.133    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 6.27e+05 |\n",
      "| step      | 9.8e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0555   |\n",
      "| loss      | 0.129    |\n",
      "| loss_q0   | 0.066    |\n",
      "| loss_q1   | 0.0979   |\n",
      "| loss_q2   | 0.128    |\n",
      "| loss_q3   | 0.224    |\n",
      "| mse       | 0.129    |\n",
      "| mse_q0    | 0.066    |\n",
      "| mse_q1    | 0.0979   |\n",
      "| mse_q2    | 0.128    |\n",
      "| mse_q3    | 0.224    |\n",
      "| samples   | 6.3e+05  |\n",
      "| step      | 9.85e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0667   |\n",
      "| loss_q1   | 0.0989   |\n",
      "| loss_q2   | 0.127    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0667   |\n",
      "| mse_q1    | 0.0989   |\n",
      "| mse_q2    | 0.127    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 6.34e+05 |\n",
      "| step      | 9.9e+03  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0562   |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0668   |\n",
      "| loss_q1   | 0.0985   |\n",
      "| loss_q2   | 0.129    |\n",
      "| loss_q3   | 0.225    |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0668   |\n",
      "| mse_q1    | 0.0985   |\n",
      "| mse_q2    | 0.129    |\n",
      "| mse_q3    | 0.225    |\n",
      "| samples   | 6.37e+05 |\n",
      "| step      | 9.95e+03 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0573   |\n",
      "| loss      | 0.133    |\n",
      "| loss_q0   | 0.0663   |\n",
      "| loss_q1   | 0.101    |\n",
      "| loss_q2   | 0.135    |\n",
      "| loss_q3   | 0.229    |\n",
      "| mse       | 0.133    |\n",
      "| mse_q0    | 0.0663   |\n",
      "| mse_q1    | 0.101    |\n",
      "| mse_q2    | 0.135    |\n",
      "| mse_q3    | 0.229    |\n",
      "| samples   | 6.4e+05  |\n",
      "| step      | 1e+04    |\n",
      "------------------------\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.136    |\n",
      "| eval_loss_q0 | 0.0765   |\n",
      "| eval_loss_q1 | 0.0841   |\n",
      "| eval_loss_q2 | 0.134    |\n",
      "| eval_loss_q3 | 0.256    |\n",
      "| eval_mse     | 0.136    |\n",
      "| eval_mse_q0  | 0.0765   |\n",
      "| eval_mse_q1  | 0.0841   |\n",
      "| eval_mse_q2  | 0.134    |\n",
      "| eval_mse_q3  | 0.256    |\n",
      "---------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0566   |\n",
      "| loss      | 0.13     |\n",
      "| loss_q0   | 0.0677   |\n",
      "| loss_q1   | 0.098    |\n",
      "| loss_q2   | 0.129    |\n",
      "| loss_q3   | 0.225    |\n",
      "| mse       | 0.13     |\n",
      "| mse_q0    | 0.0677   |\n",
      "| mse_q1    | 0.098    |\n",
      "| mse_q2    | 0.129    |\n",
      "| mse_q3    | 0.225    |\n",
      "| samples   | 6.43e+05 |\n",
      "| step      | 1.00e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0579   |\n",
      "| loss      | 0.132    |\n",
      "| loss_q0   | 0.0658   |\n",
      "| loss_q1   | 0.0985   |\n",
      "| loss_q2   | 0.13     |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.132    |\n",
      "| mse_q0    | 0.0658   |\n",
      "| mse_q1    | 0.0985   |\n",
      "| mse_q2    | 0.13     |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 6.46e+05 |\n",
      "| step      | 1.01e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0564   |\n",
      "| loss      | 0.132    |\n",
      "| loss_q0   | 0.067    |\n",
      "| loss_q1   | 0.0978   |\n",
      "| loss_q2   | 0.131    |\n",
      "| loss_q3   | 0.225    |\n",
      "| mse       | 0.132    |\n",
      "| mse_q0    | 0.067    |\n",
      "| mse_q1    | 0.0978   |\n",
      "| mse_q2    | 0.131    |\n",
      "| mse_q3    | 0.225    |\n",
      "| samples   | 6.5e+05  |\n",
      "| step      | 1.02e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0566   |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0665   |\n",
      "| loss_q1   | 0.0978   |\n",
      "| loss_q2   | 0.128    |\n",
      "| loss_q3   | 0.23     |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0665   |\n",
      "| mse_q1    | 0.0978   |\n",
      "| mse_q2    | 0.128    |\n",
      "| mse_q3    | 0.23     |\n",
      "| samples   | 6.53e+05 |\n",
      "| step      | 1.02e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.128    |\n",
      "| loss_q0   | 0.0657   |\n",
      "| loss_q1   | 0.0969   |\n",
      "| loss_q2   | 0.129    |\n",
      "| loss_q3   | 0.222    |\n",
      "| mse       | 0.128    |\n",
      "| mse_q0    | 0.0657   |\n",
      "| mse_q1    | 0.0969   |\n",
      "| mse_q2    | 0.129    |\n",
      "| mse_q3    | 0.222    |\n",
      "| samples   | 6.56e+05 |\n",
      "| step      | 1.02e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0559   |\n",
      "| loss      | 0.128    |\n",
      "| loss_q0   | 0.0653   |\n",
      "| loss_q1   | 0.0971   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.225    |\n",
      "| mse       | 0.128    |\n",
      "| mse_q0    | 0.0653   |\n",
      "| mse_q1    | 0.0971   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.225    |\n",
      "| samples   | 6.59e+05 |\n",
      "| step      | 1.03e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0568   |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.0665   |\n",
      "| loss_q1   | 0.0967   |\n",
      "| loss_q2   | 0.127    |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.0665   |\n",
      "| mse_q1    | 0.0967   |\n",
      "| mse_q2    | 0.127    |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 6.62e+05 |\n",
      "| step      | 1.04e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.13     |\n",
      "| loss_q0   | 0.0663   |\n",
      "| loss_q1   | 0.0974   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.224    |\n",
      "| mse       | 0.13     |\n",
      "| mse_q0    | 0.0663   |\n",
      "| mse_q1    | 0.0974   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.224    |\n",
      "| samples   | 6.66e+05 |\n",
      "| step      | 1.04e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0545   |\n",
      "| loss      | 0.125    |\n",
      "| loss_q0   | 0.0647   |\n",
      "| loss_q1   | 0.0965   |\n",
      "| loss_q2   | 0.128    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.125    |\n",
      "| mse_q0    | 0.0647   |\n",
      "| mse_q1    | 0.0965   |\n",
      "| mse_q2    | 0.128    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 6.69e+05 |\n",
      "| step      | 1.04e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0562   |\n",
      "| loss      | 0.131    |\n",
      "| loss_q0   | 0.0653   |\n",
      "| loss_q1   | 0.0973   |\n",
      "| loss_q2   | 0.125    |\n",
      "| loss_q3   | 0.226    |\n",
      "| mse       | 0.131    |\n",
      "| mse_q0    | 0.0653   |\n",
      "| mse_q1    | 0.0973   |\n",
      "| mse_q2    | 0.125    |\n",
      "| mse_q3    | 0.226    |\n",
      "| samples   | 6.72e+05 |\n",
      "| step      | 1.05e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0547   |\n",
      "| loss      | 0.129    |\n",
      "| loss_q0   | 0.0651   |\n",
      "| loss_q1   | 0.0994   |\n",
      "| loss_q2   | 0.13     |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.129    |\n",
      "| mse_q0    | 0.0651   |\n",
      "| mse_q1    | 0.0994   |\n",
      "| mse_q2    | 0.13     |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 6.75e+05 |\n",
      "| step      | 1.06e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0557   |\n",
      "| loss      | 0.128    |\n",
      "| loss_q0   | 0.0658   |\n",
      "| loss_q1   | 0.0954   |\n",
      "| loss_q2   | 0.127    |\n",
      "| loss_q3   | 0.22     |\n",
      "| mse       | 0.128    |\n",
      "| mse_q0    | 0.0658   |\n",
      "| mse_q1    | 0.0954   |\n",
      "| mse_q2    | 0.127    |\n",
      "| mse_q3    | 0.22     |\n",
      "| samples   | 6.78e+05 |\n",
      "| step      | 1.06e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0583   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0643   |\n",
      "| loss_q1   | 0.0959   |\n",
      "| loss_q2   | 0.123    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0643   |\n",
      "| mse_q1    | 0.0959   |\n",
      "| mse_q2    | 0.123    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 6.82e+05 |\n",
      "| step      | 1.06e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0542   |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.0655   |\n",
      "| loss_q1   | 0.0956   |\n",
      "| loss_q2   | 0.128    |\n",
      "| loss_q3   | 0.22     |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.0655   |\n",
      "| mse_q1    | 0.0956   |\n",
      "| mse_q2    | 0.128    |\n",
      "| mse_q3    | 0.22     |\n",
      "| samples   | 6.85e+05 |\n",
      "| step      | 1.07e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0553   |\n",
      "| loss      | 0.128    |\n",
      "| loss_q0   | 0.0641   |\n",
      "| loss_q1   | 0.095    |\n",
      "| loss_q2   | 0.125    |\n",
      "| loss_q3   | 0.221    |\n",
      "| mse       | 0.128    |\n",
      "| mse_q0    | 0.0641   |\n",
      "| mse_q1    | 0.095    |\n",
      "| mse_q2    | 0.125    |\n",
      "| mse_q3    | 0.221    |\n",
      "| samples   | 6.88e+05 |\n",
      "| step      | 1.08e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0566   |\n",
      "| loss      | 0.128    |\n",
      "| loss_q0   | 0.0654   |\n",
      "| loss_q1   | 0.0973   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.221    |\n",
      "| mse       | 0.128    |\n",
      "| mse_q0    | 0.0654   |\n",
      "| mse_q1    | 0.0973   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.221    |\n",
      "| samples   | 6.91e+05 |\n",
      "| step      | 1.08e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0539   |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.0636   |\n",
      "| loss_q1   | 0.0962   |\n",
      "| loss_q2   | 0.128    |\n",
      "| loss_q3   | 0.218    |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.0636   |\n",
      "| mse_q1    | 0.0962   |\n",
      "| mse_q2    | 0.128    |\n",
      "| mse_q3    | 0.218    |\n",
      "| samples   | 6.94e+05 |\n",
      "| step      | 1.08e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0551   |\n",
      "| loss      | 0.127    |\n",
      "| loss_q0   | 0.0632   |\n",
      "| loss_q1   | 0.0962   |\n",
      "| loss_q2   | 0.128    |\n",
      "| loss_q3   | 0.224    |\n",
      "| mse       | 0.127    |\n",
      "| mse_q0    | 0.0632   |\n",
      "| mse_q1    | 0.0962   |\n",
      "| mse_q2    | 0.128    |\n",
      "| mse_q3    | 0.224    |\n",
      "| samples   | 6.98e+05 |\n",
      "| step      | 1.09e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0553   |\n",
      "| loss      | 0.128    |\n",
      "| loss_q0   | 0.0643   |\n",
      "| loss_q1   | 0.0979   |\n",
      "| loss_q2   | 0.129    |\n",
      "| loss_q3   | 0.22     |\n",
      "| mse       | 0.128    |\n",
      "| mse_q0    | 0.0643   |\n",
      "| mse_q1    | 0.0979   |\n",
      "| mse_q2    | 0.129    |\n",
      "| mse_q3    | 0.22     |\n",
      "| samples   | 7.01e+05 |\n",
      "| step      | 1.1e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.127    |\n",
      "| loss_q0   | 0.0639   |\n",
      "| loss_q1   | 0.0951   |\n",
      "| loss_q2   | 0.129    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.127    |\n",
      "| mse_q0    | 0.0639   |\n",
      "| mse_q1    | 0.0951   |\n",
      "| mse_q2    | 0.129    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 7.04e+05 |\n",
      "| step      | 1.1e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0561   |\n",
      "| loss      | 0.127    |\n",
      "| loss_q0   | 0.0633   |\n",
      "| loss_q1   | 0.0928   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.223    |\n",
      "| mse       | 0.127    |\n",
      "| mse_q0    | 0.0633   |\n",
      "| mse_q1    | 0.0928   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.223    |\n",
      "| samples   | 7.07e+05 |\n",
      "| step      | 1.10e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0522   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0627   |\n",
      "| loss_q1   | 0.093    |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.214    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0627   |\n",
      "| mse_q1    | 0.093    |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.214    |\n",
      "| samples   | 7.1e+05  |\n",
      "| step      | 1.11e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0568   |\n",
      "| loss      | 0.125    |\n",
      "| loss_q0   | 0.0639   |\n",
      "| loss_q1   | 0.0976   |\n",
      "| loss_q2   | 0.129    |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.125    |\n",
      "| mse_q0    | 0.0639   |\n",
      "| mse_q1    | 0.0976   |\n",
      "| mse_q2    | 0.129    |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 7.14e+05 |\n",
      "| step      | 1.12e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0529   |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.0645   |\n",
      "| loss_q1   | 0.0929   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.0645   |\n",
      "| mse_q1    | 0.0929   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 7.17e+05 |\n",
      "| step      | 1.12e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.063    |\n",
      "| loss_q1   | 0.0925   |\n",
      "| loss_q2   | 0.125    |\n",
      "| loss_q3   | 0.222    |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.063    |\n",
      "| mse_q1    | 0.0925   |\n",
      "| mse_q2    | 0.125    |\n",
      "| mse_q3    | 0.222    |\n",
      "| samples   | 7.2e+05  |\n",
      "| step      | 1.12e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.055    |\n",
      "| loss      | 0.127    |\n",
      "| loss_q0   | 0.0645   |\n",
      "| loss_q1   | 0.0959   |\n",
      "| loss_q2   | 0.125    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.127    |\n",
      "| mse_q0    | 0.0645   |\n",
      "| mse_q1    | 0.0959   |\n",
      "| mse_q2    | 0.125    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 7.23e+05 |\n",
      "| step      | 1.13e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0554   |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.063    |\n",
      "| loss_q1   | 0.0953   |\n",
      "| loss_q2   | 0.128    |\n",
      "| loss_q3   | 0.214    |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.063    |\n",
      "| mse_q1    | 0.0953   |\n",
      "| mse_q2    | 0.128    |\n",
      "| mse_q3    | 0.214    |\n",
      "| samples   | 7.26e+05 |\n",
      "| step      | 1.14e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0559   |\n",
      "| loss      | 0.125    |\n",
      "| loss_q0   | 0.0641   |\n",
      "| loss_q1   | 0.0929   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.125    |\n",
      "| mse_q0    | 0.0641   |\n",
      "| mse_q1    | 0.0929   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 7.3e+05  |\n",
      "| step      | 1.14e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0543   |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.0624   |\n",
      "| loss_q1   | 0.0966   |\n",
      "| loss_q2   | 0.125    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.0624   |\n",
      "| mse_q1    | 0.0966   |\n",
      "| mse_q2    | 0.125    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 7.33e+05 |\n",
      "| step      | 1.14e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0548   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0624   |\n",
      "| loss_q1   | 0.0919   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0624   |\n",
      "| mse_q1    | 0.0919   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 7.36e+05 |\n",
      "| step      | 1.15e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.053    |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.0626   |\n",
      "| loss_q1   | 0.0943   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.218    |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.0626   |\n",
      "| mse_q1    | 0.0943   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.218    |\n",
      "| samples   | 7.39e+05 |\n",
      "| step      | 1.16e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0556   |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.0638   |\n",
      "| loss_q1   | 0.0951   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.218    |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.0638   |\n",
      "| mse_q1    | 0.0951   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.218    |\n",
      "| samples   | 7.42e+05 |\n",
      "| step      | 1.16e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0546   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0627   |\n",
      "| loss_q1   | 0.0938   |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0627   |\n",
      "| mse_q1    | 0.0938   |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 7.46e+05 |\n",
      "| step      | 1.16e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0558   |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.0641   |\n",
      "| loss_q1   | 0.0955   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.0641   |\n",
      "| mse_q1    | 0.0955   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 7.49e+05 |\n",
      "| step      | 1.17e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0567   |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.0635   |\n",
      "| loss_q1   | 0.0936   |\n",
      "| loss_q2   | 0.127    |\n",
      "| loss_q3   | 0.22     |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.0635   |\n",
      "| mse_q1    | 0.0936   |\n",
      "| mse_q2    | 0.127    |\n",
      "| mse_q3    | 0.22     |\n",
      "| samples   | 7.52e+05 |\n",
      "| step      | 1.18e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0553   |\n",
      "| loss      | 0.125    |\n",
      "| loss_q0   | 0.0622   |\n",
      "| loss_q1   | 0.0945   |\n",
      "| loss_q2   | 0.126    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.125    |\n",
      "| mse_q0    | 0.0622   |\n",
      "| mse_q1    | 0.0945   |\n",
      "| mse_q2    | 0.126    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 7.55e+05 |\n",
      "| step      | 1.18e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0539   |\n",
      "| loss      | 0.125    |\n",
      "| loss_q0   | 0.0623   |\n",
      "| loss_q1   | 0.0954   |\n",
      "| loss_q2   | 0.125    |\n",
      "| loss_q3   | 0.211    |\n",
      "| mse       | 0.125    |\n",
      "| mse_q0    | 0.0623   |\n",
      "| mse_q1    | 0.0954   |\n",
      "| mse_q2    | 0.125    |\n",
      "| mse_q3    | 0.211    |\n",
      "| samples   | 7.58e+05 |\n",
      "| step      | 1.18e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.055    |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.0632   |\n",
      "| loss_q1   | 0.0913   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.22     |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.0632   |\n",
      "| mse_q1    | 0.0913   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.22     |\n",
      "| samples   | 7.62e+05 |\n",
      "| step      | 1.19e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0547   |\n",
      "| loss      | 0.125    |\n",
      "| loss_q0   | 0.063    |\n",
      "| loss_q1   | 0.0926   |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.218    |\n",
      "| mse       | 0.125    |\n",
      "| mse_q0    | 0.063    |\n",
      "| mse_q1    | 0.0926   |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.218    |\n",
      "| samples   | 7.65e+05 |\n",
      "| step      | 1.2e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.053    |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0604   |\n",
      "| loss_q1   | 0.0947   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0604   |\n",
      "| mse_q1    | 0.0947   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 7.68e+05 |\n",
      "| step      | 1.2e+04  |\n",
      "------------------------\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.129    |\n",
      "| eval_loss_q0 | 0.0584   |\n",
      "| eval_loss_q1 | 0.0959   |\n",
      "| eval_loss_q2 | 0.131    |\n",
      "| eval_loss_q3 | 0.201    |\n",
      "| eval_mse     | 0.129    |\n",
      "| eval_mse_q0  | 0.0584   |\n",
      "| eval_mse_q1  | 0.0959   |\n",
      "| eval_mse_q2  | 0.131    |\n",
      "| eval_mse_q3  | 0.201    |\n",
      "---------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.054    |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0628   |\n",
      "| loss_q1   | 0.0902   |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0628   |\n",
      "| mse_q1    | 0.0902   |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 7.71e+05 |\n",
      "| step      | 1.20e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0552   |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.0607   |\n",
      "| loss_q1   | 0.0907   |\n",
      "| loss_q2   | 0.121    |\n",
      "| loss_q3   | 0.222    |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.0607   |\n",
      "| mse_q1    | 0.0907   |\n",
      "| mse_q2    | 0.121    |\n",
      "| mse_q3    | 0.222    |\n",
      "| samples   | 7.74e+05 |\n",
      "| step      | 1.21e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0627   |\n",
      "| loss_q1   | 0.0929   |\n",
      "| loss_q2   | 0.123    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0627   |\n",
      "| mse_q1    | 0.0929   |\n",
      "| mse_q2    | 0.123    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 7.78e+05 |\n",
      "| step      | 1.22e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0545   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0608   |\n",
      "| loss_q1   | 0.0935   |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.214    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0608   |\n",
      "| mse_q1    | 0.0935   |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.214    |\n",
      "| samples   | 7.81e+05 |\n",
      "| step      | 1.22e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0605   |\n",
      "| loss_q1   | 0.0925   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0605   |\n",
      "| mse_q1    | 0.0925   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 7.84e+05 |\n",
      "| step      | 1.22e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0563   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0612   |\n",
      "| loss_q1   | 0.0934   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.218    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0612   |\n",
      "| mse_q1    | 0.0934   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.218    |\n",
      "| samples   | 7.87e+05 |\n",
      "| step      | 1.23e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0609   |\n",
      "| loss_q1   | 0.092    |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0609   |\n",
      "| mse_q1    | 0.092    |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 7.9e+05  |\n",
      "| step      | 1.24e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.053    |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0619   |\n",
      "| loss_q1   | 0.0921   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.217    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0619   |\n",
      "| mse_q1    | 0.0921   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.217    |\n",
      "| samples   | 7.94e+05 |\n",
      "| step      | 1.24e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0555   |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.0609   |\n",
      "| loss_q1   | 0.0912   |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.0609   |\n",
      "| mse_q1    | 0.0912   |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 7.97e+05 |\n",
      "| step      | 1.24e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0534   |\n",
      "| loss      | 0.12     |\n",
      "| loss_q0   | 0.0607   |\n",
      "| loss_q1   | 0.0921   |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.212    |\n",
      "| mse       | 0.12     |\n",
      "| mse_q0    | 0.0606   |\n",
      "| mse_q1    | 0.0921   |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.212    |\n",
      "| samples   | 8e+05    |\n",
      "| step      | 1.25e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.126    |\n",
      "| loss_q0   | 0.0622   |\n",
      "| loss_q1   | 0.0928   |\n",
      "| loss_q2   | 0.121    |\n",
      "| loss_q3   | 0.219    |\n",
      "| mse       | 0.126    |\n",
      "| mse_q0    | 0.0622   |\n",
      "| mse_q1    | 0.0928   |\n",
      "| mse_q2    | 0.121    |\n",
      "| mse_q3    | 0.219    |\n",
      "| samples   | 8.03e+05 |\n",
      "| step      | 1.26e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0537   |\n",
      "| loss      | 0.121    |\n",
      "| loss_q0   | 0.0601   |\n",
      "| loss_q1   | 0.0922   |\n",
      "| loss_q2   | 0.123    |\n",
      "| loss_q3   | 0.211    |\n",
      "| mse       | 0.121    |\n",
      "| mse_q0    | 0.0601   |\n",
      "| mse_q1    | 0.0922   |\n",
      "| mse_q2    | 0.123    |\n",
      "| mse_q3    | 0.211    |\n",
      "| samples   | 8.06e+05 |\n",
      "| step      | 1.26e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0542   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0609   |\n",
      "| loss_q1   | 0.0904   |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.214    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0609   |\n",
      "| mse_q1    | 0.0904   |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.214    |\n",
      "| samples   | 8.1e+05  |\n",
      "| step      | 1.26e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0543   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0615   |\n",
      "| loss_q1   | 0.0921   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0615   |\n",
      "| mse_q1    | 0.0921   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 8.13e+05 |\n",
      "| step      | 1.27e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.053    |\n",
      "| loss      | 0.121    |\n",
      "| loss_q0   | 0.06     |\n",
      "| loss_q1   | 0.0905   |\n",
      "| loss_q2   | 0.121    |\n",
      "| loss_q3   | 0.216    |\n",
      "| mse       | 0.121    |\n",
      "| mse_q0    | 0.06     |\n",
      "| mse_q1    | 0.0905   |\n",
      "| mse_q2    | 0.121    |\n",
      "| mse_q3    | 0.216    |\n",
      "| samples   | 8.16e+05 |\n",
      "| step      | 1.28e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0534   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0607   |\n",
      "| loss_q1   | 0.0912   |\n",
      "| loss_q2   | 0.123    |\n",
      "| loss_q3   | 0.22     |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0607   |\n",
      "| mse_q1    | 0.0912   |\n",
      "| mse_q2    | 0.123    |\n",
      "| mse_q3    | 0.22     |\n",
      "| samples   | 8.19e+05 |\n",
      "| step      | 1.28e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0553   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0617   |\n",
      "| loss_q1   | 0.0887   |\n",
      "| loss_q2   | 0.12     |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0617   |\n",
      "| mse_q1    | 0.0887   |\n",
      "| mse_q2    | 0.12     |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 8.22e+05 |\n",
      "| step      | 1.28e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.056    |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.061    |\n",
      "| loss_q1   | 0.0907   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.061    |\n",
      "| mse_q1    | 0.0907   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 8.26e+05 |\n",
      "| step      | 1.29e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0551   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0595   |\n",
      "| loss_q1   | 0.0912   |\n",
      "| loss_q2   | 0.121    |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0595   |\n",
      "| mse_q1    | 0.0912   |\n",
      "| mse_q2    | 0.121    |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 8.29e+05 |\n",
      "| step      | 1.3e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0581   |\n",
      "| loss      | 0.123    |\n",
      "| loss_q0   | 0.0609   |\n",
      "| loss_q1   | 0.0905   |\n",
      "| loss_q2   | 0.121    |\n",
      "| loss_q3   | 0.218    |\n",
      "| mse       | 0.123    |\n",
      "| mse_q0    | 0.0609   |\n",
      "| mse_q1    | 0.0905   |\n",
      "| mse_q2    | 0.121    |\n",
      "| mse_q3    | 0.218    |\n",
      "| samples   | 8.32e+05 |\n",
      "| step      | 1.3e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.055    |\n",
      "| loss      | 0.124    |\n",
      "| loss_q0   | 0.0615   |\n",
      "| loss_q1   | 0.0911   |\n",
      "| loss_q2   | 0.124    |\n",
      "| loss_q3   | 0.211    |\n",
      "| mse       | 0.124    |\n",
      "| mse_q0    | 0.0615   |\n",
      "| mse_q1    | 0.0911   |\n",
      "| mse_q2    | 0.124    |\n",
      "| mse_q3    | 0.211    |\n",
      "| samples   | 8.35e+05 |\n",
      "| step      | 1.30e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0544   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0617   |\n",
      "| loss_q1   | 0.0922   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.217    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0617   |\n",
      "| mse_q1    | 0.0922   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.217    |\n",
      "| samples   | 8.38e+05 |\n",
      "| step      | 1.31e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0547   |\n",
      "| loss      | 0.121    |\n",
      "| loss_q0   | 0.0603   |\n",
      "| loss_q1   | 0.089    |\n",
      "| loss_q2   | 0.12     |\n",
      "| loss_q3   | 0.21     |\n",
      "| mse       | 0.121    |\n",
      "| mse_q0    | 0.0603   |\n",
      "| mse_q1    | 0.089    |\n",
      "| mse_q2    | 0.12     |\n",
      "| mse_q3    | 0.21     |\n",
      "| samples   | 8.42e+05 |\n",
      "| step      | 1.32e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0555   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0605   |\n",
      "| loss_q1   | 0.088    |\n",
      "| loss_q2   | 0.117    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0605   |\n",
      "| mse_q1    | 0.088    |\n",
      "| mse_q2    | 0.117    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 8.45e+05 |\n",
      "| step      | 1.32e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0535   |\n",
      "| loss      | 0.12     |\n",
      "| loss_q0   | 0.0608   |\n",
      "| loss_q1   | 0.0879   |\n",
      "| loss_q2   | 0.12     |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.12     |\n",
      "| mse_q0    | 0.0608   |\n",
      "| mse_q1    | 0.0879   |\n",
      "| mse_q2    | 0.12     |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 8.48e+05 |\n",
      "| step      | 1.32e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0537   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0608   |\n",
      "| loss_q1   | 0.0894   |\n",
      "| loss_q2   | 0.121    |\n",
      "| loss_q3   | 0.208    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0608   |\n",
      "| mse_q1    | 0.0894   |\n",
      "| mse_q2    | 0.121    |\n",
      "| mse_q3    | 0.208    |\n",
      "| samples   | 8.51e+05 |\n",
      "| step      | 1.33e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0542   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.061    |\n",
      "| loss_q1   | 0.0899   |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.061    |\n",
      "| mse_q1    | 0.0899   |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 8.54e+05 |\n",
      "| step      | 1.34e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0532   |\n",
      "| loss      | 0.121    |\n",
      "| loss_q0   | 0.0616   |\n",
      "| loss_q1   | 0.0887   |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.214    |\n",
      "| mse       | 0.121    |\n",
      "| mse_q0    | 0.0616   |\n",
      "| mse_q1    | 0.0887   |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.214    |\n",
      "| samples   | 8.58e+05 |\n",
      "| step      | 1.34e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0533   |\n",
      "| loss      | 0.12     |\n",
      "| loss_q0   | 0.0604   |\n",
      "| loss_q1   | 0.0891   |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.214    |\n",
      "| mse       | 0.12     |\n",
      "| mse_q0    | 0.0604   |\n",
      "| mse_q1    | 0.0891   |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.214    |\n",
      "| samples   | 8.61e+05 |\n",
      "| step      | 1.34e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0538   |\n",
      "| loss      | 0.12     |\n",
      "| loss_q0   | 0.0595   |\n",
      "| loss_q1   | 0.0895   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.215    |\n",
      "| mse       | 0.12     |\n",
      "| mse_q0    | 0.0595   |\n",
      "| mse_q1    | 0.0895   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.215    |\n",
      "| samples   | 8.64e+05 |\n",
      "| step      | 1.35e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0553   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0595   |\n",
      "| loss_q1   | 0.0898   |\n",
      "| loss_q2   | 0.116    |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0595   |\n",
      "| mse_q1    | 0.0898   |\n",
      "| mse_q2    | 0.116    |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 8.67e+05 |\n",
      "| step      | 1.36e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0534   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.059    |\n",
      "| loss_q1   | 0.0916   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.206    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.059    |\n",
      "| mse_q1    | 0.0916   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.206    |\n",
      "| samples   | 8.7e+05  |\n",
      "| step      | 1.36e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0538   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0591   |\n",
      "| loss_q1   | 0.0885   |\n",
      "| loss_q2   | 0.122    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0591   |\n",
      "| mse_q1    | 0.0885   |\n",
      "| mse_q2    | 0.122    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 8.74e+05 |\n",
      "| step      | 1.36e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0542   |\n",
      "| loss      | 0.12     |\n",
      "| loss_q0   | 0.0595   |\n",
      "| loss_q1   | 0.0916   |\n",
      "| loss_q2   | 0.117    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.12     |\n",
      "| mse_q0    | 0.0595   |\n",
      "| mse_q1    | 0.0916   |\n",
      "| mse_q2    | 0.117    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 8.77e+05 |\n",
      "| step      | 1.37e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0546   |\n",
      "| loss      | 0.121    |\n",
      "| loss_q0   | 0.0595   |\n",
      "| loss_q1   | 0.0901   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.217    |\n",
      "| mse       | 0.121    |\n",
      "| mse_q0    | 0.0595   |\n",
      "| mse_q1    | 0.0901   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.217    |\n",
      "| samples   | 8.8e+05  |\n",
      "| step      | 1.38e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0526   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.06     |\n",
      "| loss_q1   | 0.0895   |\n",
      "| loss_q2   | 0.121    |\n",
      "| loss_q3   | 0.206    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.06     |\n",
      "| mse_q1    | 0.0895   |\n",
      "| mse_q2    | 0.121    |\n",
      "| mse_q3    | 0.206    |\n",
      "| samples   | 8.83e+05 |\n",
      "| step      | 1.38e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0518   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0597   |\n",
      "| loss_q1   | 0.0899   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.205    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0597   |\n",
      "| mse_q1    | 0.0899   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.205    |\n",
      "| samples   | 8.86e+05 |\n",
      "| step      | 1.38e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0544   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0592   |\n",
      "| loss_q1   | 0.0893   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.207    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0592   |\n",
      "| mse_q1    | 0.0893   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.207    |\n",
      "| samples   | 8.9e+05  |\n",
      "| step      | 1.39e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0549   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0603   |\n",
      "| loss_q1   | 0.0889   |\n",
      "| loss_q2   | 0.116    |\n",
      "| loss_q3   | 0.208    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0603   |\n",
      "| mse_q1    | 0.0889   |\n",
      "| mse_q2    | 0.116    |\n",
      "| mse_q3    | 0.208    |\n",
      "| samples   | 8.93e+05 |\n",
      "| step      | 1.4e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0529   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0605   |\n",
      "| loss_q1   | 0.0882   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.212    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0605   |\n",
      "| mse_q1    | 0.0882   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.212    |\n",
      "| samples   | 8.96e+05 |\n",
      "| step      | 1.4e+04  |\n",
      "------------------------\n",
      "eval on validation set\n",
      "---------------------------\n",
      "| eval_loss    | 0.117    |\n",
      "| eval_loss_q0 | 0.058    |\n",
      "| eval_loss_q1 | 0.0866   |\n",
      "| eval_loss_q2 | 0.0966   |\n",
      "| eval_loss_q3 | 0.191    |\n",
      "| eval_mse     | 0.117    |\n",
      "| eval_mse_q0  | 0.058    |\n",
      "| eval_mse_q1  | 0.0866   |\n",
      "| eval_mse_q2  | 0.0966   |\n",
      "| eval_mse_q3  | 0.191    |\n",
      "---------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0529   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0595   |\n",
      "| loss_q1   | 0.0897   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.212    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0595   |\n",
      "| mse_q1    | 0.0897   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.212    |\n",
      "| samples   | 8.99e+05 |\n",
      "| step      | 1.40e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0534   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.058    |\n",
      "| loss_q1   | 0.0876   |\n",
      "| loss_q2   | 0.117    |\n",
      "| loss_q3   | 0.207    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.058    |\n",
      "| mse_q1    | 0.0876   |\n",
      "| mse_q2    | 0.117    |\n",
      "| mse_q3    | 0.207    |\n",
      "| samples   | 9.02e+05 |\n",
      "| step      | 1.41e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0528   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0589   |\n",
      "| loss_q1   | 0.0896   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0589   |\n",
      "| mse_q1    | 0.0896   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 9.06e+05 |\n",
      "| step      | 1.42e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0555   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0607   |\n",
      "| loss_q1   | 0.0889   |\n",
      "| loss_q2   | 0.116    |\n",
      "| loss_q3   | 0.214    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0607   |\n",
      "| mse_q1    | 0.0889   |\n",
      "| mse_q2    | 0.116    |\n",
      "| mse_q3    | 0.214    |\n",
      "| samples   | 9.09e+05 |\n",
      "| step      | 1.42e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0521   |\n",
      "| loss      | 0.115    |\n",
      "| loss_q0   | 0.0579   |\n",
      "| loss_q1   | 0.0866   |\n",
      "| loss_q2   | 0.117    |\n",
      "| loss_q3   | 0.202    |\n",
      "| mse       | 0.115    |\n",
      "| mse_q0    | 0.0579   |\n",
      "| mse_q1    | 0.0866   |\n",
      "| mse_q2    | 0.117    |\n",
      "| mse_q3    | 0.202    |\n",
      "| samples   | 9.12e+05 |\n",
      "| step      | 1.42e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0545   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0588   |\n",
      "| loss_q1   | 0.088    |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.208    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0588   |\n",
      "| mse_q1    | 0.088    |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.208    |\n",
      "| samples   | 9.15e+05 |\n",
      "| step      | 1.43e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0563   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.058    |\n",
      "| loss_q1   | 0.0861   |\n",
      "| loss_q2   | 0.116    |\n",
      "| loss_q3   | 0.21     |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.058    |\n",
      "| mse_q1    | 0.0861   |\n",
      "| mse_q2    | 0.116    |\n",
      "| mse_q3    | 0.21     |\n",
      "| samples   | 9.18e+05 |\n",
      "| step      | 1.44e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0574   |\n",
      "| loss      | 0.117    |\n",
      "| loss_q0   | 0.0588   |\n",
      "| loss_q1   | 0.086    |\n",
      "| loss_q2   | 0.114    |\n",
      "| loss_q3   | 0.21     |\n",
      "| mse       | 0.117    |\n",
      "| mse_q0    | 0.0588   |\n",
      "| mse_q1    | 0.086    |\n",
      "| mse_q2    | 0.114    |\n",
      "| mse_q3    | 0.21     |\n",
      "| samples   | 9.22e+05 |\n",
      "| step      | 1.44e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0535   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.059    |\n",
      "| loss_q1   | 0.0863   |\n",
      "| loss_q2   | 0.12     |\n",
      "| loss_q3   | 0.206    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.059    |\n",
      "| mse_q1    | 0.0863   |\n",
      "| mse_q2    | 0.12     |\n",
      "| mse_q3    | 0.206    |\n",
      "| samples   | 9.25e+05 |\n",
      "| step      | 1.44e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0529   |\n",
      "| loss      | 0.117    |\n",
      "| loss_q0   | 0.0585   |\n",
      "| loss_q1   | 0.0879   |\n",
      "| loss_q2   | 0.117    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.117    |\n",
      "| mse_q0    | 0.0585   |\n",
      "| mse_q1    | 0.0879   |\n",
      "| mse_q2    | 0.117    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 9.28e+05 |\n",
      "| step      | 1.45e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0533   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0578   |\n",
      "| loss_q1   | 0.0869   |\n",
      "| loss_q2   | 0.115    |\n",
      "| loss_q3   | 0.211    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0578   |\n",
      "| mse_q1    | 0.0869   |\n",
      "| mse_q2    | 0.115    |\n",
      "| mse_q3    | 0.211    |\n",
      "| samples   | 9.31e+05 |\n",
      "| step      | 1.46e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0533   |\n",
      "| loss      | 0.117    |\n",
      "| loss_q0   | 0.0587   |\n",
      "| loss_q1   | 0.0875   |\n",
      "| loss_q2   | 0.119    |\n",
      "| loss_q3   | 0.206    |\n",
      "| mse       | 0.117    |\n",
      "| mse_q0    | 0.0587   |\n",
      "| mse_q1    | 0.0875   |\n",
      "| mse_q2    | 0.119    |\n",
      "| mse_q3    | 0.206    |\n",
      "| samples   | 9.34e+05 |\n",
      "| step      | 1.46e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0579   |\n",
      "| loss      | 0.122    |\n",
      "| loss_q0   | 0.0577   |\n",
      "| loss_q1   | 0.0862   |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.22     |\n",
      "| mse       | 0.122    |\n",
      "| mse_q0    | 0.0577   |\n",
      "| mse_q1    | 0.0862   |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.22     |\n",
      "| samples   | 9.38e+05 |\n",
      "| step      | 1.46e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0556   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0568   |\n",
      "| loss_q1   | 0.0862   |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.206    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0568   |\n",
      "| mse_q1    | 0.0862   |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.206    |\n",
      "| samples   | 9.41e+05 |\n",
      "| step      | 1.47e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0536   |\n",
      "| loss      | 0.114    |\n",
      "| loss_q0   | 0.0572   |\n",
      "| loss_q1   | 0.0857   |\n",
      "| loss_q2   | 0.113    |\n",
      "| loss_q3   | 0.204    |\n",
      "| mse       | 0.114    |\n",
      "| mse_q0    | 0.0572   |\n",
      "| mse_q1    | 0.0857   |\n",
      "| mse_q2    | 0.113    |\n",
      "| mse_q3    | 0.204    |\n",
      "| samples   | 9.44e+05 |\n",
      "| step      | 1.48e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.053    |\n",
      "| loss      | 0.116    |\n",
      "| loss_q0   | 0.0586   |\n",
      "| loss_q1   | 0.0856   |\n",
      "| loss_q2   | 0.113    |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.116    |\n",
      "| mse_q0    | 0.0586   |\n",
      "| mse_q1    | 0.0856   |\n",
      "| mse_q2    | 0.113    |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 9.47e+05 |\n",
      "| step      | 1.48e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0557   |\n",
      "| loss      | 0.12     |\n",
      "| loss_q0   | 0.0576   |\n",
      "| loss_q1   | 0.0881   |\n",
      "| loss_q2   | 0.118    |\n",
      "| loss_q3   | 0.208    |\n",
      "| mse       | 0.12     |\n",
      "| mse_q0    | 0.0576   |\n",
      "| mse_q1    | 0.0881   |\n",
      "| mse_q2    | 0.118    |\n",
      "| mse_q3    | 0.208    |\n",
      "| samples   | 9.5e+05  |\n",
      "| step      | 1.48e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0557   |\n",
      "| loss      | 0.116    |\n",
      "| loss_q0   | 0.0588   |\n",
      "| loss_q1   | 0.086    |\n",
      "| loss_q2   | 0.113    |\n",
      "| loss_q3   | 0.205    |\n",
      "| mse       | 0.116    |\n",
      "| mse_q0    | 0.0588   |\n",
      "| mse_q1    | 0.086    |\n",
      "| mse_q2    | 0.113    |\n",
      "| mse_q3    | 0.205    |\n",
      "| samples   | 9.54e+05 |\n",
      "| step      | 1.49e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0534   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0585   |\n",
      "| loss_q1   | 0.0868   |\n",
      "| loss_q2   | 0.12     |\n",
      "| loss_q3   | 0.206    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0585   |\n",
      "| mse_q1    | 0.0868   |\n",
      "| mse_q2    | 0.12     |\n",
      "| mse_q3    | 0.206    |\n",
      "| samples   | 9.57e+05 |\n",
      "| step      | 1.5e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0551   |\n",
      "| loss      | 0.12     |\n",
      "| loss_q0   | 0.0584   |\n",
      "| loss_q1   | 0.0867   |\n",
      "| loss_q2   | 0.116    |\n",
      "| loss_q3   | 0.212    |\n",
      "| mse       | 0.12     |\n",
      "| mse_q0    | 0.0584   |\n",
      "| mse_q1    | 0.0867   |\n",
      "| mse_q2    | 0.116    |\n",
      "| mse_q3    | 0.212    |\n",
      "| samples   | 9.6e+05  |\n",
      "| step      | 1.5e+04  |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0556   |\n",
      "| loss      | 0.117    |\n",
      "| loss_q0   | 0.0583   |\n",
      "| loss_q1   | 0.0879   |\n",
      "| loss_q2   | 0.115    |\n",
      "| loss_q3   | 0.204    |\n",
      "| mse       | 0.117    |\n",
      "| mse_q0    | 0.0583   |\n",
      "| mse_q1    | 0.0879   |\n",
      "| mse_q2    | 0.115    |\n",
      "| mse_q3    | 0.204    |\n",
      "| samples   | 9.63e+05 |\n",
      "| step      | 1.50e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0522   |\n",
      "| loss      | 0.116    |\n",
      "| loss_q0   | 0.0578   |\n",
      "| loss_q1   | 0.0869   |\n",
      "| loss_q2   | 0.115    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.116    |\n",
      "| mse_q0    | 0.0578   |\n",
      "| mse_q1    | 0.0869   |\n",
      "| mse_q2    | 0.115    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 9.66e+05 |\n",
      "| step      | 1.51e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0542   |\n",
      "| loss      | 0.117    |\n",
      "| loss_q0   | 0.0574   |\n",
      "| loss_q1   | 0.086    |\n",
      "| loss_q2   | 0.114    |\n",
      "| loss_q3   | 0.206    |\n",
      "| mse       | 0.117    |\n",
      "| mse_q0    | 0.0574   |\n",
      "| mse_q1    | 0.086    |\n",
      "| mse_q2    | 0.114    |\n",
      "| mse_q3    | 0.206    |\n",
      "| samples   | 9.7e+05  |\n",
      "| step      | 1.52e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0521   |\n",
      "| loss      | 0.117    |\n",
      "| loss_q0   | 0.058    |\n",
      "| loss_q1   | 0.0868   |\n",
      "| loss_q2   | 0.115    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.117    |\n",
      "| mse_q0    | 0.058    |\n",
      "| mse_q1    | 0.0868   |\n",
      "| mse_q2    | 0.115    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 9.73e+05 |\n",
      "| step      | 1.52e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0552   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0575   |\n",
      "| loss_q1   | 0.0869   |\n",
      "| loss_q2   | 0.117    |\n",
      "| loss_q3   | 0.213    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0575   |\n",
      "| mse_q1    | 0.0869   |\n",
      "| mse_q2    | 0.117    |\n",
      "| mse_q3    | 0.213    |\n",
      "| samples   | 9.76e+05 |\n",
      "| step      | 1.52e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0545   |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.059    |\n",
      "| loss_q1   | 0.0871   |\n",
      "| loss_q2   | 0.114    |\n",
      "| loss_q3   | 0.207    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.059    |\n",
      "| mse_q1    | 0.0871   |\n",
      "| mse_q2    | 0.114    |\n",
      "| mse_q3    | 0.207    |\n",
      "| samples   | 9.79e+05 |\n",
      "| step      | 1.53e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0532   |\n",
      "| loss      | 0.116    |\n",
      "| loss_q0   | 0.059    |\n",
      "| loss_q1   | 0.0856   |\n",
      "| loss_q2   | 0.111    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.116    |\n",
      "| mse_q0    | 0.059    |\n",
      "| mse_q1    | 0.0856   |\n",
      "| mse_q2    | 0.111    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 9.82e+05 |\n",
      "| step      | 1.54e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0534   |\n",
      "| loss      | 0.115    |\n",
      "| loss_q0   | 0.0579   |\n",
      "| loss_q1   | 0.0858   |\n",
      "| loss_q2   | 0.115    |\n",
      "| loss_q3   | 0.204    |\n",
      "| mse       | 0.115    |\n",
      "| mse_q0    | 0.0579   |\n",
      "| mse_q1    | 0.0858   |\n",
      "| mse_q2    | 0.115    |\n",
      "| mse_q3    | 0.204    |\n",
      "| samples   | 9.86e+05 |\n",
      "| step      | 1.54e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0537   |\n",
      "| loss      | 0.119    |\n",
      "| loss_q0   | 0.0581   |\n",
      "| loss_q1   | 0.0863   |\n",
      "| loss_q2   | 0.116    |\n",
      "| loss_q3   | 0.208    |\n",
      "| mse       | 0.119    |\n",
      "| mse_q0    | 0.0581   |\n",
      "| mse_q1    | 0.0863   |\n",
      "| mse_q2    | 0.116    |\n",
      "| mse_q3    | 0.208    |\n",
      "| samples   | 9.89e+05 |\n",
      "| step      | 1.54e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.054    |\n",
      "| loss      | 0.118    |\n",
      "| loss_q0   | 0.0583   |\n",
      "| loss_q1   | 0.0863   |\n",
      "| loss_q2   | 0.114    |\n",
      "| loss_q3   | 0.209    |\n",
      "| mse       | 0.118    |\n",
      "| mse_q0    | 0.0583   |\n",
      "| mse_q1    | 0.0863   |\n",
      "| mse_q2    | 0.114    |\n",
      "| mse_q3    | 0.209    |\n",
      "| samples   | 9.92e+05 |\n",
      "| step      | 1.55e+04 |\n",
      "------------------------\n",
      "------------------------\n",
      "| grad_norm | 0.0531   |\n",
      "| loss      | 0.114    |\n",
      "| loss_q0   | 0.0571   |\n",
      "| loss_q1   | 0.084    |\n",
      "| loss_q2   | 0.113    |\n",
      "| loss_q3   | 0.207    |\n",
      "| mse       | 0.114    |\n",
      "| mse_q0    | 0.0571   |\n",
      "| mse_q1    | 0.084    |\n",
      "| mse_q2    | 0.113    |\n",
      "| mse_q3    | 0.207    |\n",
      "| samples   | 9.95e+05 |\n",
      "| step      | 1.56e+04 |\n",
      "------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1578/1920632460.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mschedule_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschedule_sampler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0;34m**\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m ).run_loop()\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/train_util.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         while (\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_anneal_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/train_util.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, batch, cond)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DIFFUSION_TRAINING_TEST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Save the last checkpoint if it wasn't already saved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_interval\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/train_util.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, batch, cond)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddp_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mmicrobatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                 \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro_cond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/respace.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     ):  # pylint: disable=signature-differs\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# print('called training_losses')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0mCompute\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses_e2e\u001b[0;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[1;32m   1508\u001b[0m                 \u001b[0mx_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m                 \u001b[0mget_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0mx_start_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_start_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_start_log_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_start_log_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/respace.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# return temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# print(new_ts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_global_requires_backward_grad_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_joined_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_ddp_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;31m# sync params according to location (before/after forward) user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36m_run_ddp_forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m             )\n\u001b[1;32m    999\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inside_ddp_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inside_ddp_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/studium/Diffusion-LM/improved-diffusion/improved_diffusion/transformer_model2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, timesteps, y, src_ids, src_mask)\u001b[0m\n\u001b[1;32m    909\u001b[0m                                                                 ).last_hidden_state\n\u001b[1;32m    910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0minput_trans_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_down_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_trans_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m                 )\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         )\n\u001b[1;32m    480\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         )\n\u001b[1;32m    412\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from improved_diffusion import dist_util, logger\n",
    "from improved_diffusion.image_datasets import load_data\n",
    "from improved_diffusion.text_datasets import load_data_text\n",
    "from improved_diffusion.resample import create_named_schedule_sampler\n",
    "from improved_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    args_to_dict,\n",
    "    add_dict_to_argparser,\n",
    ")\n",
    "from improved_diffusion.train_util import TrainLoop\n",
    "from transformers import set_seed\n",
    "from functools import partial\n",
    "from improved_diffusion.test_util import get_weights, compute_logp\n",
    "from improved_diffusion.rounding import load_models, load_tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "import torch.distributed as dist\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class Parameters(dict):\n",
    "    '''Most of the code in the Diffusion-LM Paper just passes the args through all the functions\n",
    "    I wanted to use dicts for the params, but only noticed then that they don't support getting their\n",
    "    values like attributes.'''\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def __getattribute__(self, __name: 'str'):\n",
    "        if hasattr(super(), __name):\n",
    "            return super().__getattribute__(__name) \n",
    "        elif super().get(__name):\n",
    "            return super().get(__name)\n",
    "        else:\n",
    "            raise AttributeError(f\"'parameters' object has no attribute '{__name}'\")\n",
    "\n",
    "\n",
    "COVOST_PATH = \"./covost/dataset\"\n",
    "DIFFUSION_MODELS_PATH = \"./improved-diffusion/diffusion_models/\"\n",
    "TEXT_DEFAULTS: 'dict[str, str | int | float]' = dict(\n",
    "    modality='text',\n",
    "    dataset_name='covost',\n",
    "    experiment='translation',\n",
    "    noise_schedule='cosine',\n",
    "    loss_type='Lsimple',\n",
    "    dropout=0.1,\n",
    "    weight_decay=0.0,\n",
    "    image_size=8,\n",
    "    hidden_size=128,\n",
    "    in_channel=16, ## Embedding Dimension\n",
    "    lr_anneal_steps=400000, ## Training steps\n",
    "    num_res_blocks=2, ## Not sure\n",
    "    lr=1e-04, ## Learning rate?\n",
    "    bsz=64, ## Batch Size\n",
    "    diff_steps=4000, ## Steps of diffusion\n",
    "    model_arch='conv-unet',\n",
    "    emb_scale_factor=1.0, \n",
    "    noise_level=0.0, \n",
    "    cache_mode='no', \n",
    "    use_bert_tokenizer='no',\n",
    "    padding_mode='block',\n",
    "    preprocessing_num_workers=1,\n",
    "    #config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml',\n",
    "    #model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None',\n",
    "    #experiment='gpt2_pre_compress',\n",
    "\n",
    ")\n",
    "'''These are the defaults from Diffusion-LMs run_train.py'''\n",
    "\n",
    "DIFFUSION_DEFAULTS: \"dict[str, str | float]\" = {\n",
    "    'seed': 101,\n",
    "    'data_dir': \"\",\n",
    "    'schedule_sampler': \"uniform\",\n",
    "    'lr':1e-4,\n",
    "    'weight_decay':0.0,\n",
    "    'lr_anneal_steps':0,\n",
    "    'batch_size':1,\n",
    "    'microbatch':-1, # -1 disables microbatches\n",
    "    'ema_rate':\"0.9999\", # comma-seperated list of EMA values\n",
    "    'log_interval':50,\n",
    "    'save_interval':50000,\n",
    "    'resume_checkpoint':\"\",\n",
    "    'use_fp16':False,\n",
    "    'fp16_scale_growth':1e-3,\n",
    "    'gradient_clipping':-1.0,\n",
    "    'eval_interval':2000,\n",
    "    'checkpoint_path':\"diff_models\"\n",
    "}\n",
    "'''These are the defaults from improved-diffusions train.py'''\n",
    "\n",
    "MODEL_AND_DIFFUSION_DEFAULTS = model_and_diffusion_defaults()\n",
    "ARGS: 'dict[str, str | int | float]' = {\n",
    "    'diffusion_steps': 2000,\n",
    "    'model_arch': 'transformer',\n",
    "    'lr': 0.0001,\n",
    "    'lr_anneal_steps': 50000,\n",
    "    'seed': 102,\n",
    "    'noise_schedule': 'sqrt',\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256, # Same as in_channel\n",
    "    'modality': 'text',\n",
    "    'experiment': 'translation',\n",
    "    'submit': 'no',\n",
    "    'padding_mode': 'pad',\n",
    "    'predict_xstart': True,\n",
    "    'training_mode': 'e2e',\n",
    "    'notes': 'xstart_e2e',\n",
    "    'batch_size': 64,\n",
    "    'vocab_size': 30000\n",
    "}\n",
    "\n",
    "'''Adjust these for your run.'''\n",
    "\n",
    "PARAMS = Parameters(**{\n",
    "    **DIFFUSION_DEFAULTS,\n",
    "    **MODEL_AND_DIFFUSION_DEFAULTS,\n",
    "    **TEXT_DEFAULTS,\n",
    "    **ARGS\n",
    "})\n",
    "'''This collects all the parameters, as in the Diffusion-Lm repo'''\n",
    "\n",
    "### From run_train.py\n",
    "if PARAMS['loss_type'] == 'Lsimple':\n",
    "    PARAMS.update(use_kl= False, learn_sigma= False)\n",
    "elif PARAMS['loss_type'] == 'Lhybrid':\n",
    "    PARAMS.update(use_kl= False, learn_sigma= True)\n",
    "elif PARAMS['loss_type'] == 'Lvlb':\n",
    "    PARAMS.update(use_kl= True, learn_sigma= True)\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "def model_path(\n",
    "    modality,\n",
    "    padding_mode,\n",
    "    experiment,\n",
    "    in_channel,\n",
    "    model_arch,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    diff_steps,\n",
    "    noise_schedule,\n",
    "    loss_type,\n",
    "    hidden_size,\n",
    "    num_res_blocks,\n",
    "    dropout,\n",
    "    seed,\n",
    "    notes=None,\n",
    "    **_\n",
    "    ):\n",
    "    MODEL_NAME = f\"diff\" \\\n",
    "        f\"_{modality}\" \\\n",
    "        f\"_{padding_mode}\" \\\n",
    "        f\"_{experiment}{in_channel}\" \\\n",
    "        f\"_{model_arch}\" \\\n",
    "        f\"_lr{lr}\" \\\n",
    "        f\"_{weight_decay}\" \\\n",
    "        f\"_{diff_steps}\" \\\n",
    "        f\"_{noise_schedule}\" \\\n",
    "        f\"_{loss_type}\" \\\n",
    "        f\"_h{hidden_size}\" \\\n",
    "        f\"_s{num_res_blocks}\" \\\n",
    "        f\"_d{dropout}\" \\\n",
    "        f\"_sd{seed}\" \\\n",
    "        f\"{f'_{notes}' if notes else ''}\"\n",
    "\n",
    "    return os.path.join(DIFFUSION_MODELS_PATH, MODEL_NAME)\n",
    "\n",
    "MODEL_PATH = model_path(**PARAMS)\n",
    "\n",
    "PARAMS['checkpoint_path'] = MODEL_PATH\n",
    "\n",
    "### Environment variables for the training script\n",
    "os.environ['OPENAI_LOGDIR']=MODEL_PATH\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "set_seed(PARAMS['seed']) \n",
    "dist_util.setup_dist() # DEBUG **\n",
    "logger.configure()\n",
    "\n",
    "print(PARAMS)\n",
    "\n",
    "logger.log(\"creating model and diffusion...\")\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "    **{\n",
    "        key: PARAMS[key] for key in MODEL_AND_DIFFUSION_DEFAULTS.keys()\n",
    "    }\n",
    ")\n",
    "model.to(dist_util.dev()) #  DEBUG **\n",
    "# model.cuda() #  DEBUG **\n",
    "\n",
    "pytorch_total_params = sum(parameter.numel() for parameter in model.parameters())\n",
    "logger.log(f'the parameter count is {pytorch_total_params}')\n",
    "\n",
    "schedule_sampler = create_named_schedule_sampler(PARAMS['schedule_sampler'], diffusion)\n",
    "\n",
    "logger.log(f'saving the hyperparameters to {PARAMS[\"checkpoint_path\"]}/training_args.json')\n",
    "with open(f'{PARAMS[\"checkpoint_path\"]}/training_args.json', 'w') as hyperparams_file:\n",
    "    json.dump(PARAMS, hyperparams_file, indent=2)\n",
    "\n",
    "logger.log(\"creating data loader...\")\n",
    "print('load data', '*'*50)\n",
    "\n",
    "tokenizer: 'Tokenizer' = Tokenizer.from_file(f'{COVOST_PATH}/tokenizer.json')\n",
    "\n",
    "embedding_model = torch.nn.Embedding(tokenizer.get_vocab_size(), PARAMS['in_channel'])\n",
    "\n",
    "torch.save(embedding_model.weight, f'{MODEL_PATH}/embedding_weights_initial.pt')\n",
    "\n",
    "def create_data_loaders():\n",
    "    '''Creating a function for this so that unnecessary data can be freed'''\n",
    "    from improved_diffusion.text_datasets import TextDataset_NoCache\n",
    "    from torch.utils.data import DataLoader\n",
    "    import itertools\n",
    "    from diffusion_translation.datasets import TextDataset_FileBacked\n",
    "\n",
    "    split_renamings = {\n",
    "        'train':'data',\n",
    "        'dev': 'eval_data'\n",
    "    }\n",
    "\n",
    "    # return {\n",
    "    #     split_renamings[split]: (\n",
    "    #         databatch for databatch in DataLoader(\n",
    "    #             TextDataset_FileBacked(\n",
    "    #                 tokenizer= tokenizer,\n",
    "    #                 file= f'{COVOST_PATH}/covost_v2.de_en.{split}.txt',\n",
    "    #                 embedding_model = embedding_model,\n",
    "    #                 **PARAMS\n",
    "    #             ),\n",
    "    #             batch_size=PARAMS['batch_size'],  # 64,\n",
    "    #             drop_last=True,\n",
    "    #             shuffle=False,\n",
    "    #             num_workers=1,\n",
    "    #         )\n",
    "    #     ) for split in ['train', 'dev']\n",
    "    # }\n",
    "\n",
    "    # As in Diffusion-LM\n",
    "    max_seq_len = PARAMS.image_size ** 2\n",
    "\n",
    "    from datasets import DatasetDict, Dataset\n",
    "    from improved_diffusion.text_datasets import _collate_batch_helper\n",
    "\n",
    "    data = DatasetDict()\n",
    "\n",
    "    for split in [\"test\", \"train\", \"dev\"]:\n",
    "        pad_token_id = int(tokenizer.token_to_id('[PAD]'))\n",
    "        reader = open(f'{COVOST_PATH}/covost_v2.de_en.{split}.txt')\n",
    "\n",
    "        ## Skip Header\n",
    "        next(reader)\n",
    "\n",
    "        encoded = (tokenizer.encode(line).ids for line in reader)\n",
    "\n",
    "        ## Some lines might be too long. Can't split them up for translation.\n",
    "        def filtered():\n",
    "            num_dataset_valid_lines = 0\n",
    "            num_dataset_filtered_out_lines = 0\n",
    "            for encoding in encoded:\n",
    "                if len(encoding) < max_seq_len:\n",
    "                    num_dataset_valid_lines+=1\n",
    "                    yield encoding\n",
    "                else:\n",
    "                    num_dataset_filtered_out_lines+=1\n",
    "            num_lines = num_dataset_filtered_out_lines + num_dataset_valid_lines\n",
    "            print(f'Finished filtering the dataset lines: {num_dataset_filtered_out_lines} out of {num_lines} were too long. ({num_dataset_filtered_out_lines/num_lines*100}\\%)')\n",
    "\n",
    "        encoded_dataset = Dataset.from_dict({\n",
    "            'input_ids':[ encoding  for encoding in filtered()]\n",
    "        })\n",
    "\n",
    "        def pad_function(group_lst):\n",
    "            group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], pad_token_id, max_seq_len)\n",
    "            return group_lst\n",
    "\n",
    "        padded_dataset = encoded_dataset.map(\n",
    "            pad_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            desc=f'padding',\n",
    "        )\n",
    "\n",
    "        data[split]=padded_dataset\n",
    "\n",
    "        reader.close()\n",
    "\n",
    "    def data_generator(split, data):\n",
    "        dataset = TextDataset_NoCache(\n",
    "            data,\n",
    "            PARAMS.image_size,\n",
    "            PARAMS,\n",
    "            model_arch=PARAMS['model_arch'],\n",
    "            model_emb=embedding_model.cpu(),\n",
    "            split=split\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=PARAMS['batch_size'],  # 64,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            yield from dataloader\n",
    "\n",
    "    return {\n",
    "        # split_renamings[split]: (\n",
    "        #     databatch for databatch in DataLoader(\n",
    "        #         TextDataset_NoCache(\n",
    "        #             data,\n",
    "        #             PARAMS.image_size,\n",
    "        #             PARAMS,\n",
    "        #             model_arch=PARAMS['model_arch'],\n",
    "        #             model_emb=embedding_model,\n",
    "        #             split=split\n",
    "        #         ),\n",
    "        #         batch_size=PARAMS['batch_size'],  # 20,\n",
    "        #         drop_last=True,\n",
    "        #         shuffle=False,\n",
    "        #         num_workers=1,\n",
    "        #     )\n",
    "        #) for split in ['train', 'dev']\n",
    "        split_renamings[split]: data_generator(split, data) for split in ['train', 'dev']\n",
    "    }\n",
    "\n",
    "data_loaders = create_data_loaders()\n",
    "\n",
    "embedding_model_cuda = embedding_model.cuda()\n",
    "\n",
    "def set_mapping_func(args, diffusion):\n",
    "    print(f'Embedding model: {embedding_model}\\n Requires Grad: {embedding_model.weight.requires_grad}')\n",
    "    mapping_func = partial(compute_logp, args, embedding_model_cuda)\n",
    "    diffusion.mapping_func = mapping_func\n",
    "\n",
    "set_mapping_func(PARAMS, diffusion)\n",
    "\n",
    "def training_params(\n",
    "    batch_size, \n",
    "    microbatch, \n",
    "    lr, \n",
    "    ema_rate,\n",
    "    log_interval,\n",
    "    save_interval,\n",
    "    resume_checkpoint,\n",
    "    use_fp16,\n",
    "    fp16_scale_growth,\n",
    "    weight_decay,\n",
    "    lr_anneal_steps,\n",
    "    checkpoint_path,\n",
    "    gradient_clipping,\n",
    "    eval_interval, **_):\n",
    "    '''Extracts just the training parameters'''\n",
    "    return dict(\n",
    "        batch_size=batch_size, \n",
    "        microbatch=microbatch, \n",
    "        lr=lr, \n",
    "        ema_rate=ema_rate,\n",
    "        log_interval=log_interval,\n",
    "        save_interval=save_interval,\n",
    "        resume_checkpoint=resume_checkpoint,\n",
    "        use_fp16=use_fp16,\n",
    "        fp16_scale_growth=fp16_scale_growth,\n",
    "        weight_decay=weight_decay,\n",
    "        lr_anneal_steps=lr_anneal_steps,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        eval_interval=eval_interval\n",
    "    )\n",
    "\n",
    "wandb.init(\n",
    "    project=os.getenv(\"WANDB_PROJECT\", \"diffusion_lm\"),\n",
    "    name=PARAMS['checkpoint_path'],\n",
    ")\n",
    "wandb.config.update(PARAMS, allow_val_change=True)\n",
    "\n",
    "logger.log(\"training...\")\n",
    "TrainLoop(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    schedule_sampler=schedule_sampler,\n",
    "    **training_params(**PARAMS),\n",
    "    **data_loaders\n",
    ").run_loop()\n",
    "\n",
    "## Saving the Embedding Model\n",
    "torch.save(embedding_model.weight, f'{MODEL_PATH}/embedding_weights.pt')\n",
    "torch.save(embedding_model_cuda.weight, f'{MODEL_PATH}/embedding_weights_cuda.pt')\n",
    "\n",
    "\n",
    "# for data_loader in data_loaders:\n",
    "#     data_loaders[data_loader].close()\n",
    "\n",
    "wandb.finish(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script for resuming training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3132311fb184848bccee0ea58ca8d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>█▂▁▁▁▁▁▁</td></tr><tr><td>eval_loss_q0</td><td>█▂▁▁▁▁▁▁</td></tr><tr><td>eval_loss_q1</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>eval_loss_q2</td><td>█▂▂▂▁▁▁▁</td></tr><tr><td>eval_loss_q3</td><td>█▂▁▂▂▂▁▁</td></tr><tr><td>eval_mse</td><td>█▂▁▁▁▁▁▁</td></tr><tr><td>eval_mse_q0</td><td>█▂▁▁▁▁▁▁</td></tr><tr><td>eval_mse_q1</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>eval_mse_q2</td><td>█▂▂▂▁▁▁▁</td></tr><tr><td>eval_mse_q3</td><td>█▂▁▂▂▂▁▁</td></tr><tr><td>grad_norm</td><td>▂▁█▆▅▄▄▄▄▃▃▂▂▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▁▁▂▁▁▂▂▂▁▁▁</td></tr><tr><td>loss</td><td>██▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q0</td><td>██▆▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q1</td><td>██▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q2</td><td>██▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_q3</td><td>██▆▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>mse</td><td>██▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q0</td><td>██▆▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q1</td><td>██▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q2</td><td>██▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mse_q3</td><td>██▆▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>samples</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>0.11745</td></tr><tr><td>eval_loss_q0</td><td>0.05801</td></tr><tr><td>eval_loss_q1</td><td>0.08657</td></tr><tr><td>eval_loss_q2</td><td>0.09655</td></tr><tr><td>eval_loss_q3</td><td>0.19131</td></tr><tr><td>eval_mse</td><td>0.11745</td></tr><tr><td>eval_mse_q0</td><td>0.05801</td></tr><tr><td>eval_mse_q1</td><td>0.08657</td></tr><tr><td>eval_mse_q2</td><td>0.09655</td></tr><tr><td>eval_mse_q3</td><td>0.19131</td></tr><tr><td>grad_norm</td><td>0.05311</td></tr><tr><td>loss</td><td>0.11404</td></tr><tr><td>loss_q0</td><td>0.05708</td></tr><tr><td>loss_q1</td><td>0.08403</td></tr><tr><td>loss_q2</td><td>0.11344</td></tr><tr><td>loss_q3</td><td>0.20725</td></tr><tr><td>mse</td><td>0.11404</td></tr><tr><td>mse_q0</td><td>0.05708</td></tr><tr><td>mse_q1</td><td>0.08403</td></tr><tr><td>mse_q2</td><td>0.11344</td></tr><tr><td>mse_q3</td><td>0.20725</td></tr><tr><td>samples</td><td>995264</td></tr><tr><td>step</td><td>15550</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">./improved-diffusion/diffusion_models/diff_text_pad_translation256_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e</strong>: <a href=\"https://wandb.ai/yunus-demirag/diffusion_lm/runs/3os32mzj\" target=\"_blank\">https://wandb.ai/yunus-demirag/diffusion_lm/runs/3os32mzj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221208_154710-3os32mzj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class parameters(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def __getattribute__(self, __name: 'str'):\n",
    "        if hasattr(super(), __name):\n",
    "            return super().__getattribute__(__name) \n",
    "        elif super().get(__name):\n",
    "            return super().get(__name)\n",
    "        else:\n",
    "            raise AttributeError(f\"'parameters' object has no attribute '{__name}'\")\n",
    "\n",
    "dict1 = parameters(\n",
    "    one=1,\n",
    "    two=2,\n",
    "    three=3\n",
    ")\n",
    "\n",
    "dict1.one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load('./improved-diffusion/diffusion_models/diff_text_pad_translation128_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e-simple/embedding_weights_initial.pt')\n",
    "model2 = torch.load('./improved-diffusion/diffusion_models/diff_text_pad_translation128_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e-simple/embedding_weights_cuda.pt')\n",
    "\n",
    "print(model1==model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(data_loaders['data'])\n",
    "sample[0].shape\n",
    "sample=sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "def exp(x):\n",
    "    _x = x\n",
    "    while True:\n",
    "        _x += _x\n",
    "        yield _x\n",
    "\n",
    "gen = (x for x in exp(1))\n",
    "\n",
    "print([next(gen) for _ in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Infilling of Diffusion-LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2022-12-07-17-09-05-733693\n",
      "creating model, based on transformer\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "LossType.E2E_Simple_MSE False\n",
      "training mode is  e2e-simple\n",
      "training mode is  e2e-simple\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerNetModel2(\n",
       "  (word_embedding): Embedding(30000, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=30000, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from diffusion_translation.parameters import Parameters\n",
    "from improved_diffusion.script_util import model_and_diffusion_defaults, create_model_and_diffusion\n",
    "from improved_diffusion import logger\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "TEXT_DEFAULTS: 'dict[str, str | int | float]' = dict(\n",
    "    modality='text',\n",
    "    dataset_name='covost',\n",
    "    experiment='translation',\n",
    "    noise_schedule='cosine',\n",
    "    loss_type='Lsimple',\n",
    "    dropout=0.1,\n",
    "    weight_decay=0.0,\n",
    "    image_size=8,\n",
    "    hidden_size=128,\n",
    "    in_channel=16, ## Embedding Dimension\n",
    "    lr_anneal_steps=400000, ## Training steps\n",
    "    num_res_blocks=2, ## Not sure\n",
    "    lr=1e-04, ## Learning rate?\n",
    "    bsz=64, ## Batch Size\n",
    "    diff_steps=4000, ## Steps of diffusion\n",
    "    model_arch='conv-unet',\n",
    "    emb_scale_factor=1.0, \n",
    "    noise_level=0.0, \n",
    "    cache_mode='no', \n",
    "    use_bert_tokenizer='no',\n",
    "    padding_mode='block',\n",
    "    preprocessing_num_workers=1,\n",
    "    #config='diffusion_lm/synthetic_data/configs/emnlp2020/experiments/difflm_seed0_m3_k128_trainc20000.yaml',\n",
    "    #model_name_or_path='predictability/diff_models/compress_e=5_b=60_m=gpt2_wikitext-103-raw-v1_None',\n",
    "    #experiment='gpt2_pre_compress',\n",
    "\n",
    ")\n",
    "'''These are the defaults from Diffusion-LMs run_train.py'''\n",
    "\n",
    "DIFFUSION_DEFAULTS: \"dict[str, str | float]\" = {\n",
    "    'seed': 101,\n",
    "    'data_dir': \"\",\n",
    "    'schedule_sampler': \"uniform\",\n",
    "    'lr':1e-4,\n",
    "    'weight_decay':0.0,\n",
    "    'lr_anneal_steps':0,\n",
    "    'batch_size':1,\n",
    "    'microbatch':-1, # -1 disables microbatches\n",
    "    'ema_rate':\"0.9999\", # comma-seperated list of EMA values\n",
    "    'log_interval':50,\n",
    "    'save_interval':50000,\n",
    "    'resume_checkpoint':\"\",\n",
    "    'use_fp16':False,\n",
    "    'fp16_scale_growth':1e-3,\n",
    "    'gradient_clipping':-1.0,\n",
    "    'eval_interval':2000,\n",
    "    'checkpoint_path':\"diff_models\"\n",
    "}\n",
    "'''These are the defaults from improved-diffusions train.py'''\n",
    "\n",
    "MODEL_AND_DIFFUSION_DEFAULTS = model_and_diffusion_defaults()\n",
    "\n",
    "ARGS = {\n",
    "    'model_path': './improved-diffusion/diffusion_models/diff_text_pad_translation128_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e-simple',\n",
    "    'model_file':'model002000.pt',\n",
    "    'tokenizer': './covost/dataset/tokenizer.json',\n",
    "    'num_samples':50,\n",
    "    'seqlen':64,\n",
    "    'batch_size':64,\n",
    "    'in_channel':128,\n",
    "    'clip_denoised': False,\n",
    "    'eta': 1.,\n",
    "    'top_p':-1.0,\n",
    "    'output_directory':'improved-diffusion/diffusion_models/diff_text_pad_translation128_transformer_lr0.0001_0.0_4000_sqrt_Lsimple_h128_s2_d0.1_sd102_xstart_e2e-simple'\n",
    "}\n",
    "\n",
    "# load configurations.\n",
    "config_path = f'{ARGS[\"model_path\"]}/training_args.json'\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "with open(config_path, 'rb', ) as f:\n",
    "    TRAINING_ARGS = json.load(f)\n",
    "\n",
    "PARAMS = Parameters(**{\n",
    "    **DIFFUSION_DEFAULTS,\n",
    "    **MODEL_AND_DIFFUSION_DEFAULTS,\n",
    "    **TEXT_DEFAULTS,\n",
    "    **ARGS,\n",
    "    **TRAINING_ARGS\n",
    "})\n",
    "\n",
    "logger.configure()\n",
    "\n",
    "cuda = torch.full([1],1).cuda().device\n",
    "\n",
    "tokenizer = Tokenizer.from_file(PARAMS.tokenizer)\n",
    "\n",
    "padding_token = tokenizer.token_to_id('[PAD]')\n",
    "\n",
    "num_samples = PARAMS.num_samples\n",
    "\n",
    "embedding_weights = torch.load(f'{PARAMS.model_path}/embedding_weights_cuda.pt')\n",
    "\n",
    "embedding_model = torch.nn.Embedding.from_pretrained(embedding_weights)\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "    **{key: PARAMS[key] for key in MODEL_AND_DIFFUSION_DEFAULTS.keys()}\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(f'{PARAMS.model_path}/{PARAMS.model_file}'))\n",
    "\n",
    "model.to(cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7428b342ed491b9fb614879b6cead5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3961, -1.6038, -0.0275,  ..., -0.0809, -0.3764, -1.3592],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         ...,\n",
       "         [ 0.9512,  0.0573,  1.1934,  ..., -0.5230, -0.7619,  0.0284],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982]],\n",
       "\n",
       "        [[ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [-1.1557, -1.2191,  0.9237,  ..., -2.3334, -0.4334, -0.6740],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         ...,\n",
       "         [ 0.9512,  0.0573,  1.1934,  ..., -0.5230, -0.7619,  0.0284],\n",
       "         [ 0.9512,  0.0573,  1.1934,  ..., -0.5230, -0.7619,  0.0284],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982]],\n",
       "\n",
       "        [[ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [-1.1557, -1.2191,  0.9237,  ..., -2.3334, -0.4334, -0.6740],\n",
       "         [-0.1866,  0.0254, -0.4966,  ...,  0.2443, -0.8408,  2.0965],\n",
       "         ...,\n",
       "         [ 0.9512,  0.0573,  1.1934,  ..., -0.5230, -0.7619,  0.0284],\n",
       "         [-0.1820,  0.2705,  0.4792,  ...,  0.1253,  0.3813, -0.3299],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.3961, -1.6038, -0.0275,  ..., -0.0809, -0.3764, -1.3592],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         ...,\n",
       "         [-0.1820,  0.2705,  0.4792,  ...,  0.1253,  0.3813, -0.3299],\n",
       "         [ 1.4134, -0.8184,  1.4811,  ...,  1.2948, -0.8297,  1.2789],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982]],\n",
       "\n",
       "        [[ 2.0078, -0.7526,  0.5457,  ..., -0.1995, -0.0550,  0.6694],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         ...,\n",
       "         [-0.1820,  0.2705,  0.4792,  ...,  0.1253,  0.3813, -0.3299],\n",
       "         [ 0.9512,  0.0573,  1.1934,  ..., -0.5230, -0.7619,  0.0284],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982]],\n",
       "\n",
       "        [[ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [ 0.6653, -0.9652,  1.2173,  ..., -0.2525,  1.3132,  1.2846],\n",
       "         [ 0.6653, -0.9652,  1.2173,  ..., -0.2525,  1.3132,  1.2846],\n",
       "         ...,\n",
       "         [-0.1820,  0.2705,  0.4792,  ...,  0.1253,  0.3813, -0.3299],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982],\n",
       "         [ 0.5318,  0.0996,  0.6039,  ...,  0.5425, -0.5783, -0.4982]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from improved_diffusion.test_util import denoised_fn_round\n",
    "from functools import partial\n",
    "\n",
    "model_kwargs = {}\n",
    "\n",
    "sample_shape = (PARAMS.batch_size, PARAMS.seqlen, PARAMS.in_channel, )\n",
    "\n",
    "sample = diffusion.p_sample_loop(\n",
    "    model,\n",
    "    sample_shape,\n",
    "    denoised_fn=partial(denoised_fn_round, PARAMS, embedding_model.cuda()),\n",
    "    clip_denoised=PARAMS['clip_denoised'],\n",
    "    model_kwargs=model_kwargs,\n",
    "    top_p = PARAMS.top_p,\n",
    "    device=cuda,\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Embedding\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def k_nearest_neighbors(text_embedding_list: Tensor, embedding_model: Embedding, tokenizer: Tokenizer):\n",
    "    decoded_outputs = []\n",
    "    embedding_weights = embedding_model.weight\n",
    "\n",
    "    def get_k_nearest_neighbors(embedding_weights: Tensor, text_embedding: Tensor, dist='cos'):\n",
    "        text_embedding = text_embedding.to(device=embedding_weights.device)\n",
    "        if dist == 'cos':\n",
    "            adjacency = embedding_weights @ text_embedding.transpose(1, 0).to(embedding_weights.device)\n",
    "        elif dist == 'l2':\n",
    "            adjacency = embedding_weights.unsqueeze(1).expand(-1, text_embedding.size(0), -1) - text_embedding.unsqueeze(0).expand(embedding_weights.size(0), -1, -1)\n",
    "            adjacency = -torch.norm(adjacency, dim=-1)\n",
    "        topk_out = torch.topk(adjacency, k=6, dim=0)\n",
    "        return topk_out.values, topk_out.indices\n",
    "\n",
    "    for text_embedding in text_embedding_list:\n",
    "        if len(text_embedding.shape) > 2:\n",
    "            text_embedding = text_embedding.view(-1, text_embedding.size(-1))\n",
    "        values, indices = get_k_nearest_neighbors(embedding_weights=embedding_weights, text_embedding=text_embedding, dist='l2')\n",
    "        decoded_output = tokenizer.decode(indices[0].tolist())\n",
    "        decoded_outputs.append(decoded_output)\n",
    "    \n",
    "    return decoded_outputs\n",
    "\n",
    "decoded_outputs = k_nearest_neighbors(text_embedding_list=sample, embedding_model=embedding_model, tokenizer=tokenizer)\n",
    "\n",
    "decoded_outputs_in_lines = (decoded_output + '\\n' for decoded_output in decoded_outputs)\n",
    "\n",
    "with open(f'{PARAMS.output_directory}/samples.txt', 'w') as output_file:\n",
    "    output_file.writelines(decoded_outputs_in_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 128) full shape\n",
      "(50, 64, 128) arr shape\n",
      "decoding for e2e\n",
      "torch.Size([64, 64, 128])\n",
      "torch.float32\n",
      "<bound method TransformerNetModel2.get_logits of TransformerNetModel2(\n",
      "  (word_embedding): Embedding(30000, 128)\n",
      "  (lm_head): Linear(in_features=128, out_features=30000, bias=True)\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
      "  )\n",
      "  (input_up_proj): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=768, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (input_transformers): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_down_proj): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
      "  )\n",
      ")>\n",
      "tensor([[24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [29759],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [13719],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25152],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25519],\n",
      "        [10179],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24089],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 5788],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [29759],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6745],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 5555],\n",
      "        [13433],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [20169],\n",
      "        [ 5788],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [ 1800],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [16205],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 3372],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [28589],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 1800],\n",
      "        [25297],\n",
      "        [20146],\n",
      "        [25297],\n",
      "        [ 7050],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[16205],\n",
      "        [16205],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [13433],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 8529],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[19846],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [18568],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8801],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [10179],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [  997],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [28533],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [26733],\n",
      "        [16205],\n",
      "        [28533],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 3817],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 1800],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [28533],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25950],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [13719],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [27855],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [20515],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [ 8529],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [15668],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [  650],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [11034],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [27271],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16068],\n",
      "        [ 4180],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[16205],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [24918],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [27639],\n",
      "        [16205],\n",
      "        [10181],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [28533],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [28533],\n",
      "        [28533],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [13719],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [28533],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [25297],\n",
      "        [ 8529],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [28533],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [ 6745],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [28589],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 1485],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydemirag/anaconda3/envs/Diffusion-LM/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [ 5788],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [ 3817],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [ 4180],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[16205],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [ 9866],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8585],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10543],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [12050],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [13719],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [29759],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10543],\n",
      "        [16205],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [ 8529],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [ 2381],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [ 3372],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [27271],\n",
      "        [22481],\n",
      "        [26733],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [13433],\n",
      "        [  320],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [20169],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [26313],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [18212],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [18212],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[24918],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [18212],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16117],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [ 4180],\n",
      "        [16205],\n",
      "        [ 6778],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [28533],\n",
      "        [24089],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 8454],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [20515],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [21769],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [15428],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [21769],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[ 8529],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [24918],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [ 5788],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13719],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [20515],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13719],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [23101]], device='cuda:0')\n",
      "tensor([[ 5788],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [13433],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [23101],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [13719],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [23809],\n",
      "        [18212],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [18212],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[ 7939],\n",
      "        [25297],\n",
      "        [27855],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [ 1800],\n",
      "        [20515],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 8801],\n",
      "        [16205],\n",
      "        [28533],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [27639],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25152],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [20146],\n",
      "        [22481],\n",
      "        [ 2381],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [16205],\n",
      "        [20515],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [23809],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [ 8801],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 4180],\n",
      "        [10181],\n",
      "        [ 2381],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [ 8585],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 9739],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[13433],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [ 1800],\n",
      "        [25152],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [28533],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [16205],\n",
      "        [ 2381],\n",
      "        [ 1485],\n",
      "        [24918],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24391],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6745],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [28160],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13719],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [27271],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[24918],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25519],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 1485],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [23809],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10780],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [27532],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8801],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 5555],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 6778],\n",
      "        [27271],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [29759],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[28533],\n",
      "        [22512],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 4180],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [24918],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [28533],\n",
      "        [28533],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [ 3964],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [ 6745],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [27271],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[15006],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [23809],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [16205],\n",
      "        [24180],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [ 9866],\n",
      "        [16205],\n",
      "        [ 2381],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8529],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [27639],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6745],\n",
      "        [ 9866],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [27639],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [ 9312],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [27271],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [27271],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[13433],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [ 6778],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [ 5555],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [13433],\n",
      "        [ 5555],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [14434],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [28533],\n",
      "        [24918],\n",
      "        [13433],\n",
      "        [26733],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [ 5788],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [10780],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [13719],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [18212],\n",
      "        [24918],\n",
      "        [ 2381],\n",
      "        [16205],\n",
      "        [29759],\n",
      "        [ 9866],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [22481],\n",
      "        [13719],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [ 9866],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [29759],\n",
      "        [ 9866],\n",
      "        [ 9312],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [18568],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [29759],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [ 1800],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [20515],\n",
      "        [25297],\n",
      "        [16068],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [ 9312],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 8529],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25519],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25519],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [ 8529],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [21769],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25519],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16068],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24089],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 2381],\n",
      "        [22481],\n",
      "        [ 8801],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [11034],\n",
      "        [ 9312],\n",
      "        [  997],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [ 8529],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13668],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [13719],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [18212],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [24918]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [25519],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [20169],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 4180],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [13719],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6616],\n",
      "        [23101],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [ 4180],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[20146],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [ 3817],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [18212],\n",
      "        [24918],\n",
      "        [26482],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [13433],\n",
      "        [ 9312],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [28589],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16068],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [28533],\n",
      "        [ 6745],\n",
      "        [11034],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [21798],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [ 8801],\n",
      "        [ 5788],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[29759],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [ 6745],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [22280],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [24089],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [24918],\n",
      "        [ 2381],\n",
      "        [16205],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [ 9866],\n",
      "        [23101],\n",
      "        [26733],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 4180],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [10179],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [ 1800],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [20146],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [ 9866],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [ 6778],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8529],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [13433],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 7939],\n",
      "        [24918],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [22481],\n",
      "        [ 1800],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [29759],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [28283],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [ 8801],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8529],\n",
      "        [ 3964],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 4180],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 4180],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [13433],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [ 8801],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [17352],\n",
      "        [10179],\n",
      "        [ 4652],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 8529],\n",
      "        [13433],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [ 8801],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8801],\n",
      "        [ 9312],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [ 1800],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [18212],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[ 9866],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [ 6745],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10780],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [ 9312],\n",
      "        [29759],\n",
      "        [ 2381],\n",
      "        [ 4180],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [ 6778],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [18212],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 1485],\n",
      "        [22512],\n",
      "        [ 8529],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [27271],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [24918],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [20515],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [10179],\n",
      "        [15031],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [ 8801],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8801],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [ 5788],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [28533],\n",
      "        [23101],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6778],\n",
      "        [16205],\n",
      "        [ 2381],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [29759],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [23809],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6616],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [10179],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [28589],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 9390],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 5788],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 8529],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 4180],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [20146],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [27271],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[23101],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [23809],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [13719],\n",
      "        [27855],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [18212],\n",
      "        [10179],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [21769],\n",
      "        [25297],\n",
      "        [28533],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481]], device='cuda:0')\n",
      "tensor([[24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 6745],\n",
      "        [ 1800],\n",
      "        [25297],\n",
      "        [ 1485],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 2381],\n",
      "        [29759],\n",
      "        [ 3964],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [24391],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [21769],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 2381],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [26733],\n",
      "        [22481],\n",
      "        [29759],\n",
      "        [22481],\n",
      "        [16205],\n",
      "        [16205],\n",
      "        [13433],\n",
      "        [26733],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [27271],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25950],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25519],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [26733],\n",
      "        [22512],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [10179],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25519],\n",
      "        [13433],\n",
      "        [16205],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [13719],\n",
      "        [13719],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [ 4180],\n",
      "        [22481],\n",
      "        [27639],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10780],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [ 9866],\n",
      "        [ 9866],\n",
      "        [22481],\n",
      "        [25297]], device='cuda:0')\n",
      "tensor([[25297],\n",
      "        [26733],\n",
      "        [26733],\n",
      "        [29759],\n",
      "        [25297],\n",
      "        [24918],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [ 5555],\n",
      "        [13433],\n",
      "        [25297],\n",
      "        [29759],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [16205],\n",
      "        [25297],\n",
      "        [24089],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [27271],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [ 1800],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [10179],\n",
      "        [ 9312],\n",
      "        [25297],\n",
      "        [23101],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [10179],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [13433],\n",
      "        [24918],\n",
      "        [16205],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [25297],\n",
      "        [22481],\n",
      "        [ 9866],\n",
      "        [25297],\n",
      "        [25297]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "arr = np.concatenate([sample.cpu().numpy()], axis=0)\n",
    "print(arr.shape, 'full shape')\n",
    "arr = arr[: PARAMS.num_samples]\n",
    "print(arr.shape, 'arr shape')\n",
    "\n",
    "decoded_outputs = []\n",
    "print('decoding for e2e', )\n",
    "print(sample.shape)\n",
    "print(sample.dtype)\n",
    "print(model.get_logits)\n",
    "x_t = torch.tensor(sample).cuda().to(torch.float32)\n",
    "if PARAMS.model_arch == 'conv-unet':\n",
    "    reshaped_x_t = x_t.view(x_t.size(0), -1, x_t.size(-1))\n",
    "else:\n",
    "    reshaped_x_t = x_t\n",
    "logits = model.get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
    "cands = torch.topk(logits, k=1, dim=-1)\n",
    "indices = cands.indices\n",
    "for seq in cands.indices:\n",
    "    print(seq)\n",
    "    numpy_sequence = seq.cpu().numpy()\n",
    "    tokens = tokenizer.decode(numpy_sequence.squeeze(-1))\n",
    "    decoded_outputs.append(tokens)\n",
    "\n",
    "decoded_outputs_in_lines = (decoded_output + '\\n' for decoded_output in decoded_outputs)\n",
    "\n",
    "with open(f'{PARAMS.output_directory}/samples_e2e_decoded.txt', 'w') as output_file:\n",
    "    output_file.writelines(decoded_outputs_in_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('Diffusion-LM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 18 2022, 18:57:03) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb02dc56016c5c7b1e2062c7b40f50aeaad4c023c9f15df42684d8182a57eebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
